{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saber Box.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI8gKtzJ_7J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import exists, join, basename, splitext\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "COLAB = True\n",
        "%tensorflow_version 1.x \n",
        "print(\"Note: using Google CoLab\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR2dQlPrqZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_filename = '119.mp4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IERHKVkEPos",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Initial Parameters\n",
        "# Establishes Initial Parameters\n",
        "# For creating  a new .h5 detection model\n",
        "train_model = False\n",
        "# Provides additional comment feedback while running\n",
        "verbose = True\n",
        "# Crops each from to the tracking boxes\n",
        "overlay_masking_boxes = False\n",
        "# Adjusts the Reperesentative Dots based on camera motion implied by scoring box movement.\n",
        "camera_motion_compensate = False\n",
        "# Smooths the Bellguard positions\n",
        "smooth_video_clip = False\n",
        "# Assumes two lights if Bellguards are close to each other. Reduces dependency on Box detection.\n",
        "assume_lights = True\n",
        "# Ignores Lights from the Scorebox. Mitigates poor scorebox tracking.\n",
        "ignore_box_lights = True\n",
        "# Tests and Removes Duplicate Frames from the video\n",
        "remove_duplicate_frames = True\n",
        "# Downloads out and representative_out videos\n",
        "download_videos = True\n",
        "# Analyzes the Action\n",
        "analyze_action = True\n",
        "# Allows for Simple Usage\n",
        "simplified = True\n",
        "\n",
        "# Parameters:\n",
        "min_torso_confidence = 0.80\n",
        "bellguard_confidence = 0.65\n",
        "# Provides for a higher confidence of bellguard detection\n",
        "bellguard_confidence_high = 0.75\n",
        "# Allows for a different required confidence for initial detection than tracking\n",
        "bellguard_tracking_det_offset = 0.15\n",
        "wrist_conf_min = 2\n",
        "wrist_conf_high = 6\n",
        "wrist_conf_very_high = 9\n",
        "knee_conf_min = 3\n",
        "# The Threshold for determining duplicate frames\n",
        "duplicate_threshold_factor = 0.75\n",
        "# The Threshold for determining camera motion\n",
        "camera_motion_threshold_factor = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZEgNVlj7ljV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Git Downloads\n",
        "if not exists('keypoint.py'):\n",
        "  !wget https://raw.githubusercontent.com/facebookresearch/maskrcnn-benchmark/e0a525a0139baf7086117b7ed3fd318a4878d71c/maskrcnn_benchmark/structures/keypoint.py\n",
        "from keypoint import PersonKeypoints\n",
        "\n",
        "# Mask_RCNN include setup.py\n",
        "!git clone https://github.com/matterport/Mask_RCNN.git\n",
        "%cd /content/Mask_RCNN/\n",
        "!python setup.py install\n",
        "\n",
        "# Downloads mask-rcnn-coco weights to the working directory, Mask_RCNN\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alv-tSgo5hMQ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "import pandas as pd\n",
        "from xml.etree import ElementTree\n",
        "from PIL import Image\n",
        "# from mrcnn.utils import Dataset\n",
        "from matplotlib.patches import Rectangle\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "import sys\n",
        "import statistics\n",
        "import PIL\n",
        "import torchvision\n",
        "import torch\n",
        "torch.set_grad_enabled(False)\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "import os\n",
        "from os import listdir\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "from scipy import signal\n",
        "from skimage import data, img_as_float\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from tensorflow.keras.models import load_model\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "from mrcnn.utils import compute_ap\n",
        "from mrcnn.model import load_image_gt\n",
        "from mrcnn.model import mold_image\n",
        "from mrcnn.visualize import display_instances\n",
        "from mrcnn.utils import extract_bboxes\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "from mrcnn.utils import Dataset\n",
        "\n",
        "import imgaug  # https://github.com/aleju/imgaug (pip3 install imageaug)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "#For Human Pose Analysis\n",
        "plt.rcParams[\"axes.grid\"] = False\n",
        "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
        "model = model.eval().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru6n5JlJ668f",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Vis_Keypoints\n",
        "def vis_keypoints(img, kps, draw, kp_thresh=2, alpha=0.7):\n",
        "    #Returns Fencer_Data from Human Pose\n",
        "    \"\"\"Visualizes keypoints (adapted from vis_one_image).\n",
        "    kps has shape (4, #keypoints) where 4 rows are (x, y, logit, prob).\n",
        "    \"\"\"\n",
        "    dataset_keypoints = PersonKeypoints.NAMES\n",
        "    kp_lines = PersonKeypoints.CONNECTIONS\n",
        "\n",
        "    #The keypoints of interest are [left wrist, right wrist, left knee, right knee, left shoulder, right shoulder] with [x,y,confidence] for each point\n",
        "    fencer_data = []\n",
        "    fencer_kp = [9,10,13,14,5,6]\n",
        "\n",
        "    for keypoint in fencer_kp:    \n",
        "      fencer_data.append([int(kps[0][keypoint]),int(kps[1][keypoint]),int(kps[2][keypoint])])\n",
        "\n",
        "    if draw == True:\n",
        "      # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n",
        "      cmap = plt.get_cmap('rainbow')\n",
        "      colors = [cmap(i) for i in np.linspace(0, 1, len(kp_lines) + 2)]\n",
        "      colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n",
        "\n",
        "      # Perform the drawing on a copy of the image, to allow for blending.\n",
        "      kp_mask = np.copy(img)\n",
        "\n",
        "      # Draw mid shoulder / mid hip first for better visualization.\n",
        "      mid_shoulder = (\n",
        "          kps[:2, dataset_keypoints.index('right_shoulder')] +\n",
        "          kps[:2, dataset_keypoints.index('left_shoulder')]) / 2.0\n",
        "      sc_mid_shoulder = np.minimum(\n",
        "          kps[2, dataset_keypoints.index('right_shoulder')],\n",
        "          kps[2, dataset_keypoints.index('left_shoulder')])\n",
        "      mid_hip = (\n",
        "          kps[:2, dataset_keypoints.index('right_hip')] +\n",
        "          kps[:2, dataset_keypoints.index('left_hip')]) / 2.0\n",
        "      sc_mid_hip = np.minimum(\n",
        "          kps[2, dataset_keypoints.index('right_hip')],\n",
        "          kps[2, dataset_keypoints.index('left_hip')])\n",
        "      nose_idx = dataset_keypoints.index('nose')\n",
        "      if sc_mid_shoulder > kp_thresh and kps[2, nose_idx] > kp_thresh:\n",
        "          cv2.line(\n",
        "              kp_mask, tuple(mid_shoulder), tuple(kps[:2, nose_idx]),\n",
        "              color=colors[len(kp_lines)], thickness=2, lineType=cv2.LINE_AA)\n",
        "      if sc_mid_shoulder > kp_thresh and sc_mid_hip > kp_thresh:\n",
        "          cv2.line(\n",
        "              kp_mask, tuple(mid_shoulder), tuple(mid_hip),\n",
        "              color=colors[len(kp_lines) + 1], thickness=2, lineType=cv2.LINE_AA)\n",
        "\n",
        "      # Draw the keypoints.\n",
        "      for l in range(len(kp_lines)):\n",
        "          i1 = kp_lines[l][0]\n",
        "          i2 = kp_lines[l][1]\n",
        "          p1 = kps[0, i1], kps[1, i1]\n",
        "          p2 = kps[0, i2], kps[1, i2]\n",
        "          if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n",
        "              cv2.line(\n",
        "                  kp_mask, p1, p2,\n",
        "                  color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n",
        "          if kps[2, i1] > kp_thresh:\n",
        "              cv2.circle(\n",
        "                  kp_mask, p1,\n",
        "                  radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n",
        "          if kps[2, i2] > kp_thresh:\n",
        "              cv2.circle(\n",
        "                  kp_mask, p2,\n",
        "                  radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n",
        "\n",
        "    # Blend the keypoints.\n",
        "    return [cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0), fencer_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyRq47FB5Z_y",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Overlay_Keypoints\n",
        "def overlay_keypoints(image, kps, scores, draw):\n",
        "  kps = torch.cat((kps[:, :, 0:2], scores[:, :, None]), dim=2).cpu().numpy()\n",
        "  fencer_data = []\n",
        "  for region in kps:\n",
        "    [image, fencer_data_temp] = vis_keypoints(image, region.transpose((1, 0)), draw)\n",
        "    fencer_data.append(fencer_data_temp)\n",
        "  return (image, fencer_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUJWuA_v7M8A",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Fencer_Data_Compact\n",
        "def fencer_data_compact(fencer_data):\n",
        "  # Abbreviates Fencer Data to Relevant Data\n",
        "  # Condenses fencer_data to (Left, Right) (Weapon Hand, Front Knee, Shoulder Center)\n",
        "\n",
        "  fencer_data_compact = []\n",
        "  fencer_wrist = []\n",
        "  fencer_knee = []\n",
        "  shoulder_center = []\n",
        "  keypoints = [fencer_wrist,fencer_knee,shoulder_center]\n",
        "\n",
        "  #Sorts the 4 datapoints with respect to x and returns the center two points\n",
        "  fencer_wrist.append(fencer_data[0][0])\n",
        "  fencer_wrist.append(fencer_data[0][1])\n",
        "  fencer_wrist.append(fencer_data[1][0])\n",
        "  fencer_wrist.append(fencer_data[1][1])\n",
        "  fencer_wrist = sorted(fencer_wrist, key = lambda x: x[0])\n",
        "  fencer_data_compact.append(fencer_wrist[1:3])\n",
        "\n",
        "  fencer_knee.append(fencer_data[0][2])\n",
        "  fencer_knee.append(fencer_data[0][3])\n",
        "  fencer_knee.append(fencer_data[1][2])\n",
        "  fencer_knee.append(fencer_data[1][3])\n",
        "  fencer_knee = sorted(fencer_knee, key = lambda x: x[0])\n",
        "  fencer_data_compact.append(fencer_knee[1:3])\n",
        "\n",
        "  for i in range(2):\n",
        "    shoulder_temp = []\n",
        "    for j in range(len(fencer_data[i][4])):\n",
        "      shoulder_temp.append(int((fencer_data[i][4][j] + fencer_data[i][5][j])/2))    \n",
        "    shoulder_center.append(shoulder_temp)\n",
        "  shoulder_center[0], shoulder_center[1] = shoulder_center[1], shoulder_center[0]\n",
        "  fencer_data_compact.append(shoulder_center)\n",
        "\n",
        "  return (fencer_data_compact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vIYzqkq7lyM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Human_Pose_Analysis\n",
        "def human_pose_analysis(frame):\n",
        "  # image = PIL.Image.open(file_name)\n",
        "  image = frame\n",
        "  image_tensor = torchvision.transforms.functional.to_tensor(image).cuda()\n",
        "  output = model([image_tensor])[0]\n",
        "\n",
        "  result_image = np.array(image.copy())\n",
        "\n",
        "  #Uses only six keypoints\n",
        "  [result_image, fencer_data] = overlay_keypoints(result_image, output['keypoints'][:6], output['keypoints_scores'][:6], True)\n",
        "\n",
        "  keypoints = [output['keypoints'][:2], output['keypoints_scores'][:2]]\n",
        "\n",
        "  return (fencer_data, keypoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYfwhh3UFtCT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Fencer_Data_Verification\n",
        "def fencer_data_verification(Left_Torso_Position, left_torso_size_average, Right_Torso_Position, right_torso_size_average, fencer_data, frame):\n",
        "  #Tests that the fencer_pose_data is near the torso of the fencer\n",
        "  #Left and Right Torso positions are single x,y values in this function\n",
        "  #Format fencer_data to (Left, Right) (Weapon Hand, Front Knee, Shoulder Center)\n",
        "    #[[wristLx, wristLy, wristLconf][wristRx, wristRy, wristRconf]],[[kneeLx, kneeLy, kneeLconf][kneeRx, kneeRy, kneeRconf]],[[shldrLx, shldrLy, shldrLconf][shldrRx, shldrRy, shldrRconf]]\n",
        "  #Format torso_size_average [width, height]\n",
        "\n",
        "  fencer_data_pose_left = []\n",
        "  fencer_data_pose_right = []\n",
        "\n",
        "  for i in range(len(fencer_data)):\n",
        "    #Creates the Left Fencer_Data\n",
        "    lx_min = min(fencer_data[i][4][0],fencer_data[i][5][0]) - left_torso_size_average[1]/4\n",
        "    lx_max = max(fencer_data[i][4][0],fencer_data[i][5][0]) + left_torso_size_average[1]/4\n",
        "    ly_min = min(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    ly_max = max(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    torso_l_x = Left_Torso_Position[0]\n",
        "    torso_l_y_bottom = Left_Torso_Position[1] + left_torso_size_average[1]/2\n",
        "    torso_l_y_top = Left_Torso_Position[1] - left_torso_size_average[1]/2\n",
        "    #Checks if the torso is between the shoulders, that the shoulders and within the torso_box and that the left pose is empty\n",
        "\n",
        "    display(f'The left fencer data verification bounding box is:')\n",
        "    display(f'{lx_min} to {lx_max} in the x direction and {torso_l_y_bottom} to {torso_l_y_top}')\n",
        "    display(f'The center points are: {torso_l_x} for x and {ly_min},{ly_max} for y.')\n",
        "\n",
        "    if torso_l_x > lx_min and torso_l_x < lx_max and torso_l_y_top < ly_min and torso_l_y_bottom > ly_max and fencer_data_pose_left == []:\n",
        "      #Checks for which wrist and knee is forward\n",
        "      if fencer_data[i][0][0] > fencer_data[i][1][0]:\n",
        "        fencer_data_pose_left.append(fencer_data[i][0])\n",
        "      else:\n",
        "        fencer_data_pose_left.append(fencer_data[i][1])\n",
        "      if fencer_data[i][2][0] > fencer_data[i][3][0]:\n",
        "        fencer_data_pose_left.append(fencer_data[i][2])\n",
        "      else:\n",
        "        fencer_data_pose_left.append(fencer_data[i][3])\n",
        "      fencer_data_shldr_temp = []\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][0] + fencer_data[i][5][0])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][1] + fencer_data[i][5][1])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][2] + fencer_data[i][5][2])/2))\n",
        "      fencer_data_pose_left.append(fencer_data_shldr_temp)\n",
        "      \n",
        "  for i in range(len(fencer_data)):\n",
        "    #Creates the Right Fencer_Data\n",
        "    rx_min = min(fencer_data[i][4][0],fencer_data[i][5][0]) - left_torso_size_average[1]/4\n",
        "    rx_max = max(fencer_data[i][4][0],fencer_data[i][5][0]) + left_torso_size_average[1]/4\n",
        "    ry_min = min(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    ry_max = max(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    torso_r_x = Right_Torso_Position[0]\n",
        "    torso_r_y_bottom = Right_Torso_Position[1] + right_torso_size_average[1]/2\n",
        "    torso_r_y_top = Right_Torso_Position[1] - right_torso_size_average[1]/2\n",
        "    if torso_r_x > rx_min and torso_r_x < rx_max and torso_r_y_top < ry_min and torso_r_y_bottom > ry_max and fencer_data_pose_right == []:\n",
        "      #Checks for which wrist is forward\n",
        "      if fencer_data[i][0][0] > fencer_data[i][1][0]:\n",
        "        fencer_data_pose_right.append(fencer_data[i][1])\n",
        "      else:\n",
        "        fencer_data_pose_right.append(fencer_data[i][0])\n",
        "      #Checks for which knee is forward\n",
        "      if fencer_data[i][2][0] > fencer_data[i][3][0]:\n",
        "        fencer_data_pose_right.append(fencer_data[i][3])\n",
        "      else:\n",
        "        fencer_data_pose_right.append(fencer_data[i][2])\n",
        "      fencer_data_shldr_temp = []\n",
        "      # Averages the Shoulder data\n",
        "      fencer_data_shldr_temp = []\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][0] + fencer_data[i][5][0])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][1] + fencer_data[i][5][1])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][2] + fencer_data[i][5][2])/2))\n",
        "      fencer_data_pose_right.append(fencer_data_shldr_temp)\n",
        "\n",
        "  # Condenses the fencer data to only relevant data\n",
        "  fencer_data = [fencer_data_pose_left, fencer_data_pose_right]\n",
        "\n",
        "  # If no pose is found, then it is set to zeros\n",
        "  if fencer_data[0] == []:\n",
        "    fencer_data[0] = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "  if fencer_data[1] == []:\n",
        "    fencer_data[1] = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The compact fencer data from verification frame {frame - 1} is:')\n",
        "    display(fencer_data)\n",
        "\n",
        "  return (fencer_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttScHtsXdr7U",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Hour Minute Second String\n",
        "def hms_string(sec_elapsed):\n",
        "  # Nicely formatted time string\n",
        "  h = int(sec_elapsed / (60 * 60))\n",
        "  m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "  s = sec_elapsed % 60\n",
        "  return f\"{h}:{m:>02}:{s:>05.2f}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk1r3x84LJiT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Position_Linear_Approximation\n",
        "def position_linear_approximation(position, previous_certainty):\n",
        "  # Certainty is the number of times previous to current position that a point was not certain.\n",
        "  last_known_position = ((previous_certainty+2)*(-1))\n",
        "\n",
        "  # Finds the positional distance between two known boxes\n",
        "  x_delta = int((position[-1][0] - position[last_known_position][0])/(last_known_position+1))\n",
        "  y_delta = int((position[-1][1] - position[last_known_position][1])/(last_known_position+1))\n",
        "  delta = [x_delta, y_delta]\n",
        "\n",
        "  # Adjusts the previous positions, up to the previous certainty, based on a linear approximation\n",
        "  for j in range(2):\n",
        "    for i in range(previous_certainty+1):\n",
        "      position[i - (previous_certainty+1)][j] = position[i - (previous_certainty+2)][j] - delta[j]\n",
        "\n",
        "  return (position)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfH-PgbIdu4F",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Scoring_Box_Lights\n",
        "def scoring_box_lights(img, Scoring_Box_Position, scoring_box_size_average, default_color, frame, score_box_empty):\n",
        "\n",
        "  # A high max distance is less sensitive and a lower max distance is more sensitive\n",
        "  max_distance_total = 200\n",
        "  max_distance_specific_color = 100\n",
        "\n",
        "  # Defines the region of the top_left position of a 5x3 grid of the score_box, [xmin,ymin,xmax,ymax]\n",
        "  # Extends the Light Search Position outside of the detected box\n",
        "  xmin = Scoring_Box_Position[0] - int(scoring_box_size_average[0]/2) - int(scoring_box_size_average[0]/8)\n",
        "  xmax = Scoring_Box_Position[0] - int(scoring_box_size_average[0]/2) + int(scoring_box_size_average[0]/4)\n",
        "  ymin = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2)\n",
        "  ymax = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2) + int(scoring_box_size_average[1]/3)\n",
        "  left_light_position = [xmin, xmax, ymin, ymax]\n",
        "\n",
        "  # Defines the region of the top_right position of a 5x3 grid of the score_box, [xmin,ymin,xmax,ymax]\n",
        "  xmin = Scoring_Box_Position[0] + int(scoring_box_size_average[0]/2) - int(scoring_box_size_average[0]/4)\n",
        "  xmax = Scoring_Box_Position[0] + int(scoring_box_size_average[0]/2) + int(scoring_box_size_average[0]/8)\n",
        "  ymin = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2)\n",
        "  ymax = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2) + int(scoring_box_size_average[1]/3)\n",
        "  right_light_position = [xmin, xmax, ymin, ymax]\n",
        "\n",
        "  if default_color != []:\n",
        "    distance_temp, distance_specific_color_temp = [], []\n",
        "\n",
        "    width = left_light_position[1]-left_light_position[0]\n",
        "    height = left_light_position[3]-left_light_position[2]\n",
        "\n",
        "    #i is the x value of the image for the Left Side/Red\n",
        "    for i in range(width):\n",
        "      #j is y value of the image\n",
        "      for j in range(height):\n",
        "        #color channel of the image [B,G,R]\n",
        "        #image, img, is of format [y,x]\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = (img[pixel_position_y, pixel_position_x, 0] - default_color[0])\n",
        "        g = (img[pixel_position_y, pixel_position_x, 1] - default_color[1])\n",
        "        r = (img[pixel_position_y, pixel_position_x, 2] - default_color[2])\n",
        "        distance_temp.append(int((b**2 + g**2 + r**2)**(0.5)))\n",
        "        distance_specific_color_temp.append(abs(r))\n",
        "\n",
        "    #Sorts the distances and keeps the top quarter then finds the average\n",
        "    distance_temp.sort()\n",
        "    distance_temp = distance_temp[(int(len(distance_temp)/4)*-1):]\n",
        "    distance = int(sum(distance_temp)/len(distance_temp))\n",
        "    distance_specific_color_temp.sort()\n",
        "    distance_specific_color_temp = distance_specific_color_temp[(int(len(distance_specific_color_temp)/4)*-1):]\n",
        "    distance_specific_color = int(sum(distance_specific_color_temp)/len(distance_specific_color_temp))\n",
        "\n",
        "    #0 is no color change from the default color)\n",
        "    if distance > max_distance_total and distance_specific_color > max_distance_specific_color and score_box_empty == False:\n",
        "      left_light_comparison = 1\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      left_light_comparison = 0\n",
        "\n",
        "    #Resets b,g,r for the Right Side\n",
        "    distance_temp, distance_specific_color_temp= [], []\n",
        "    width = right_light_position[1]-right_light_position[0]\n",
        "    height = right_light_position[3]-right_light_position[2]\n",
        "\n",
        "    #i is the x value of the image\n",
        "    for i in range(width):\n",
        "      #j is y value of the image\n",
        "      for j in range(height):\n",
        "        #kcolor channel of the image [B,G,R]\n",
        "\n",
        "        # pixel_position = right_light_position[2] + j,right_light_position[0] + i\n",
        "        pixel_position_y = right_light_position[2] + j\n",
        "        pixel_position_x = right_light_position[0] + i\n",
        "        b = (img[pixel_position_y, pixel_position_x, 0] - default_color[0])\n",
        "        g = (img[pixel_position_y, pixel_position_x, 1] - default_color[1])\n",
        "        r = (img[pixel_position_y, pixel_position_x, 2] - default_color[2])\n",
        "        distance_temp.append(int((b**2 + g**2 + r**2)**(0.5)))\n",
        "        distance_specific_color_temp.append(abs(g))\n",
        "\n",
        "    #Sorts the distances and keeps the top sixth then finds the average\n",
        "    distance_temp.sort()\n",
        "    distance_temp = distance_temp[(int(len(distance_temp)/6)*-1):]\n",
        "    distance = int(sum(distance_temp)/len(distance_temp))\n",
        "    distance_specific_color_temp.sort()\n",
        "    distance_specific_color_temp = distance_specific_color_temp[(int(len(distance_specific_color_temp)/4)*-1):]\n",
        "    distance_specific_color = int(sum(distance_specific_color_temp)/len(distance_specific_color_temp))\n",
        "\n",
        "    #0 is no color change from the default color)\n",
        "    if (distance > max_distance_total and distance_specific_color > max_distance_specific_color):\n",
        "      right_light_comparison = 1\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      right_light_comparison = 0\n",
        "\n",
        "  #Finds the Defualt Color\n",
        "  else:\n",
        "    b, g, r = 0, 0, 0\n",
        "    # Cycles through the Left and Right Light Positions to determine a default color for the frame\n",
        "    width = left_light_position[1]-left_light_position[0]\n",
        "    height = left_light_position[3]-left_light_position[2]\n",
        "    for i in range(width):\n",
        "      for j in range(height):\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = b + img[pixel_position_y, pixel_position_x, 0]\n",
        "        g = g + img[pixel_position_y, pixel_position_x, 1]\n",
        "        r = r + img[pixel_position_y, pixel_position_x, 2]\n",
        "        default_color_left_temp = [int(b/(width*height)),int(g/(width*height)),int(r/(width*height))]\n",
        "    width = right_light_position[1]-right_light_position[0]\n",
        "    height = right_light_position[3]-right_light_position[2]\n",
        "    for i in range(width):\n",
        "      for j in range(height):\n",
        "        # pixel_position = right_light_position[2] + j,right_light_position[0] + i\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = b + img[pixel_position_y, pixel_position_x, 0]\n",
        "        g = g + img[pixel_position_y, pixel_position_x, 1]\n",
        "        r = r + img[pixel_position_y, pixel_position_x, 2]\n",
        "        default_color_right_temp = [int(b/(width*height)),int(g/(width*height)),int(r/(width*height))]\n",
        "    #Combines the Left and Right Default Colors for B,G,R\n",
        "    for i in range(3):\n",
        "      default_color.append((default_color_left_temp[i] + default_color_right_temp[i])/2)\n",
        "\n",
        "    # Assumes that the lights are off during the engarde phase.\n",
        "    left_light_comparison = 0\n",
        "    right_light_comparison = 0\n",
        "\n",
        "  return (left_light_comparison, right_light_comparison, default_color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz4y3kZgPX58",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Motion_Difference_Tracking\n",
        "def motion_difference_tracking(frame, side, Bounding_Box, width, height, kernel_scaling, erosion_iterations, dilation_iterations):\n",
        "\n",
        "  # Ensures Bounding_Box is not negative\n",
        "  for i in range(len(Bounding_Box)):\n",
        "    if Bounding_Box[i] < 0:\n",
        "      Bounding_Box[i] = 0\n",
        "\n",
        "  display(f'The original difference tracking bounding box at frame {frame - 1} is:')\n",
        "  display(Bounding_Box)\n",
        "\n",
        "  Position_y_Orig = int((Bounding_Box[3]+Bounding_Box[2])/2)\n",
        "\n",
        "  # Uses the original frames to avoid Region of Interest Boxes\n",
        "  save_path = r'/content/Mask_RCNN/videos/original/'\n",
        "  image_num = frame\n",
        "  image_name2 = str(image_num-1) + '.jpg'\n",
        "  image_name1 = str(image_num-2) + '.jpg'\n",
        "  file_name1 = os.path.join(save_path, image_name1)\n",
        "  file_name2 = os.path.join(save_path, image_name2)\n",
        "\n",
        "  # Reads the images\n",
        "  image1 = cv2.imread(file_name1)\n",
        "  image2 = cv2.imread(file_name2)\n",
        "\n",
        "  # Convert to Grayscale\n",
        "  image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "  image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
        "  image_diff = cv2.absdiff(image1_gray,image2_gray)\n",
        "\n",
        "  # Creates a Cropped Image\n",
        "  crop_img = image_diff[Bounding_Box[2]:Bounding_Box[3], Bounding_Box[0]:Bounding_Box[1]]\n",
        "\n",
        "  # Kernel is affected by Kernel Scaling which gets finer if it initially fails\n",
        "  kernel_number = int(width/(100*kernel_scaling))\n",
        "  \n",
        "  # Ensures that the kernel is odd\n",
        "  if kernel_number%2 == 0:\n",
        "    kernel_number = kernel_number + 1\n",
        "  kernel = np.ones((kernel_number,kernel_number),np.uint8)\n",
        "  \n",
        "  try:\n",
        "    # Errodes\n",
        "    erosion = cv2.erode(crop_img,kernel,iterations = erosion_iterations)\n",
        "\n",
        "    # Dilates\n",
        "    dilation = cv2.dilate(erosion,kernel,iterations = dilation_iterations)\n",
        "\n",
        "    # Blurs Image\n",
        "    blur = cv2.GaussianBlur(dilation,kernel.shape,0)\n",
        "\n",
        "    # Threshold\n",
        "    ret,thresh = cv2.threshold(blur,0,90,cv2.THRESH_BINARY)\n",
        "\n",
        "    # Find contours\n",
        "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "    \n",
        "    c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "    if side == 'Left':\n",
        "      # Obtain outer left coordinate of the contour\n",
        "      left = tuple(c[c[:, :, 0].argmin()][0])\n",
        "      position = [right[0] + Bounding_Box[0], Position_y_Orig]\n",
        "    elif side == 'Right':\n",
        "      right = tuple(c[c[:, :, 0].argmax()][0])\n",
        "      position = [left[0] + Bounding_Box[0], Position_y_Orig]\n",
        "    else:\n",
        "      display(f'Side is not given')\n",
        "  # Error occurs if the entire image is erroded\n",
        "  except:\n",
        "    display(f'There is no data from difference imaging on the {side} side.')\n",
        "    position = 'None'\n",
        "\n",
        "  return(position)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_koxlAByNn7W",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Saturation_Test\n",
        "def saturation_test(box, frame):\n",
        "  # Test is a True/False return\n",
        "  # Takes an image and tests it for the expected saturation\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "  file_name = str(frame) + '.jpg'\n",
        "  name = os.path.join(path, file_name)\n",
        "  img = cv2.imread(name)\n",
        "  # Converts from BGR to HSV\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "  \n",
        "  # Tests Bellguard\n",
        "  if box[2] == 1:\n",
        "    blue_range = [50, 150]\n",
        "    green_range = [50, 150]\n",
        "    red_range = [50, 160]\n",
        "    max_delta = 25\n",
        "    # saturation_range = [0, 20]\n",
        "    saturation_range = [0, 70]\n",
        "    object_tested = 'Bellguard'\n",
        "  # Tests Torso\n",
        "  elif box[2] == 3:\n",
        "    blue_range = [60, 150]\n",
        "    green_range = [60, 150]\n",
        "    red_range = [60, 160]\n",
        "    max_delta = 30\n",
        "    saturation_range = [0, 20]\n",
        "    object_tested = 'Torso'\n",
        "  else:\n",
        "    display(f'The object to test does not have a color/saturation profile.')\n",
        "\n",
        "  width = (box[0][3]-box[0][1])\n",
        "  height = (box[0][2]-box[0][0])\n",
        "\n",
        "  s_temp = []\n",
        "\n",
        "  #i is the x value of the image\n",
        "  for i in range(width):\n",
        "    #j is y value of the image\n",
        "    for j in range(height):\n",
        "      s = img[box[0][0] + j, box[0][1] + i, 1]\n",
        "      s_temp.append(s)\n",
        "\n",
        "    #Sorts the distances and keeps the top quarter then finds the average\n",
        "    s_temp.sort()\n",
        "    #Truncates to the least saturated/most gray values\n",
        "    s_temp = s_temp[:(int(len(s_temp)/2)*-1)]\n",
        "    s_temp = s_temp[:(int(len(s_temp)*3/4)*-1)]\n",
        "    #Averages the saturation values\n",
        "    s_average = int(sum(s_temp)/len(s_temp))\n",
        "\n",
        "  if s_average < saturation_range[1]:\n",
        "    test_result = True\n",
        "  else:\n",
        "    test_result = False\n",
        "\n",
        "  display(f'The test result for the {object_tested} saturation is {test_result} with a saturation of {s_average}.')\n",
        "\n",
        "  return (test_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNPGfzrydJ9M",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Box_Size_Finder\n",
        "def box_size_finder(bbox, capture_width, capture_height, object_to_size):\n",
        "\n",
        "  Box_Size = [[],[]]\n",
        "  sum_of_boxes = [[],[]]\n",
        "  frame_multiplier = 1\n",
        "\n",
        "  if object_to_size == 'score_box':\n",
        "    x_min = int(capture_width/4)\n",
        "    x_max = int(capture_width*3/4)\n",
        "    bbox_category = 2\n",
        "  elif object_to_size == 'left':\n",
        "    x_min = 0\n",
        "    x_max = int(capture_width/2)\n",
        "    bbox_category = 3\n",
        "  elif object_to_size == 'right':\n",
        "    x_min = int(capture_width/2)\n",
        "    x_max = int(capture_width)\n",
        "    bbox_category = 3\n",
        "\n",
        "  # i represents the frame, minimum of 50 frames or len(bbox)\n",
        "  for i in range(min(50*frame_multiplier, len(bbox))):\n",
        "    # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "    for j in range(len(bbox[i])):\n",
        "      if (bbox[i][j][1] > 0.90 and bbox[i][j][0][1] > x_min and bbox[i][j][0][1] < x_max and bbox[i][j][2] == bbox_category):\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[0].append(bbox[i][j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[1].append(bbox[i][j][0][0])  \n",
        "        #Appends x width value:\n",
        "        Box_Size[0].append(bbox[i][j][0][3] - bbox[i][j][0][1])\n",
        "        #Appends y width value:\n",
        "        Box_Size[1].append(bbox[i][j][0][2] - bbox[i][j][0][0])\n",
        "\n",
        "  x_average = average_list(sum_of_boxes[0])\n",
        "  y_average = average_list(sum_of_boxes[1])\n",
        "\n",
        "  # scoring_box_size_average [Width, Height]\n",
        "  box_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  box_size_average.append(int(average_list(Box_Size[0])))\n",
        "  # Appends the average scoring box height\n",
        "  box_size_average.append(int(average_list(Box_Size[1])))\n",
        "\n",
        "  display(f'The Average Box Size for {object_to_size} is {box_size_average}')\n",
        "\n",
        "  return (box_size_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q00swZNxhP5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Tracking_Box_Default\n",
        "def tracking_box_default(Left, Right, Score_Box, x_padding, y_padding, engarde_length):\n",
        "  # Creates a default tracking box\n",
        "\n",
        "  Tracking_Bounding_Boxes_Temp = [[],[],[]]\n",
        "  Tracking_Bounding_Boxes = []\n",
        "\n",
        "  for i in range(engarde_length):\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[1] + y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[1] + y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[1]+ y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "\n",
        "  return (Tracking_Bounding_Boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy0ESby5beUc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Bell_Guard_Position_Finding\n",
        "def Bell_Guard_Position_Finding(bbox, capture_width, capture_height, fencer_data, positions, frame_count, left_torso_size_average, right_torso_size_average, engarde_length, previous_certainty, camera_steady, camera_motion_threshold):\n",
        "  # Format positions = [Left_Position, Right_Position, Score_Box_Position, Left_Torso_Position, Right_Torso_Position]\n",
        "\n",
        "  x_min = []\n",
        "  x_max = []\n",
        "  y_min = []\n",
        "  y_max = []\n",
        "\n",
        "  Left_Position = positions[0]\n",
        "  Right_Position = positions[1]\n",
        "  Scoring_Box_Position = positions[2]\n",
        "  Left_Torso_Position = positions[3]\n",
        "  Right_Torso_Position = positions[4]\n",
        "\n",
        "  # Any of the First engarde_length position can be used since the engarde position is an averaged constant\n",
        "  # Certainty is used here as a counter for how many times a bounding box does not fall in the tracking box\n",
        "  # And increases the size of the bounding box based on each miss\n",
        "\n",
        "  certainty = [0,0,0,0,0]\n",
        "  display(f'Previous Certainty at frame {frame_count - 1} is {previous_certainty}.')\n",
        "\n",
        "  #Establishes Previous Positions to determine speed and expected positions\n",
        "  previous_position_Left = Left_Position[-1]\n",
        "  twice_previous_position_Left = Left_Position[-2]\n",
        "  previous_position_Right = Right_Position[-1]\n",
        "  twice_previous_position_Right = Right_Position[-2]\n",
        "  previous_position_Scoring_Box = Scoring_Box_Position[-1]\n",
        "  twice_previous_position_Scoring_Box = Scoring_Box_Position[-2]\n",
        "  previous_position_Left_Torso = Left_Torso_Position[-1]\n",
        "  twice_previous_position_Left_Torso = Left_Torso_Position[-2]\n",
        "  previous_position_Right_Torso = Right_Torso_Position[-1]\n",
        "  twice_previous_position_Right_Torso = Right_Torso_Position[-2]\n",
        "\n",
        "  #Boxes are the bounding boxes for the current frame, passes less data to tracking function\n",
        "  boxes = bbox\n",
        "\n",
        "  Tracking_Bounding_Boxes_Temp = [[],[],[],[],[]]\n",
        "\n",
        "  # Torso Positions are calculated prior to the BellGuard because they are an input to the bellguard position\n",
        "\n",
        "  # Bellguard Position Tracking focuses on Tracking as opposed to detection\n",
        "  # Left_Torso Position\n",
        "  [current_position, certainty[3], Tracking_Bounding_Boxes_Left_Torso] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left_Torso, \\\n",
        "    twice_previous_position_Left_Torso, previous_certainty[3], 'Left_Torso', \\\n",
        "    frame_count, 'None', left_torso_size_average, capture_width, capture_height, 'None', engarde_length, camera_steady, camera_motion_threshold)\n",
        "  Tracking_Bounding_Boxes_Temp[3] = Tracking_Bounding_Boxes_Left_Torso\n",
        "  Left_Torso_Position = current_position\n",
        "\n",
        "  # Right_Torso Position\n",
        "  [current_position, certainty[4], Tracking_Bounding_Boxes_Right_Torso] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Right_Torso, \\\n",
        "    twice_previous_position_Right_Torso, previous_certainty[4], \"Right_Torso\", \\\n",
        "    frame_count, 'None', right_torso_size_average, capture_width, capture_height, 'None', engarde_length, camera_steady, camera_motion_threshold)\n",
        "  Tracking_Bounding_Boxes_Temp[4] = Tracking_Bounding_Boxes_Right_Torso\n",
        "  Right_Torso_Position = current_position\n",
        "\n",
        "  fencer_data = fencer_data_verification(Left_Torso_Position, left_torso_size_average, Right_Torso_Position, right_torso_size_average, \\\n",
        "    fencer_data, frame_count)\n",
        "\n",
        "  # Left Position\n",
        "  [current_position, certainty[0], Tracking_Bounding_Boxes_Left] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left, \\\n",
        "    twice_previous_position_Left, previous_certainty[0], 'Left_BellGuard', \\\n",
        "    frame_count, Left_Torso_Position, left_torso_size_average, capture_width, \\\n",
        "    capture_height, fencer_data, engarde_length, camera_steady, camera_motion_threshold)\n",
        "  Tracking_Bounding_Boxes_Temp[0] = Tracking_Bounding_Boxes_Left\n",
        "  Left_Position = current_position\n",
        "\n",
        "  #  Right Position\n",
        "  [current_position, certainty[1], Tracking_Bounding_Boxes_Right] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Right, \\\n",
        "    twice_previous_position_Right, previous_certainty[1], 'Right_BellGuard', \\\n",
        "    frame_count, Right_Torso_Position, right_torso_size_average, capture_width, \\\n",
        "    capture_height, fencer_data, engarde_length, camera_steady, camera_motion_threshold)\n",
        "  Tracking_Bounding_Boxes_Temp[1] = Tracking_Bounding_Boxes_Right\n",
        "  Right_Position = current_position\n",
        "\n",
        "  # Scoring_Box Position\n",
        "  [current_position, certainty[2], Tracking_Bounding_Boxes_Scoring_Box] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Scoring_Box, \\\n",
        "    twice_previous_position_Scoring_Box, previous_certainty[2], 'Scoring_Box', \\\n",
        "    frame_count, 'None', left_torso_size_average, capture_width, capture_height, 'None', engarde_length, camera_steady, camera_motion_threshold)\n",
        "  Tracking_Bounding_Boxes_Temp[2] = Tracking_Bounding_Boxes_Scoring_Box\n",
        "  Scoring_Box_Position = current_position\n",
        "\n",
        "  Tracking_Bounding_Boxes = Tracking_Bounding_Boxes_Temp\n",
        "\n",
        "  display(f'The Length of the Left and Right Positions after the Position Finding are: {len(Left_Position)} and {len(Right_Position)}.')\n",
        "\n",
        "  display(f'At frame {frame_count} the certainty and previous certainty before linear approx analysis is:')\n",
        "  display(f'{certainty} and {previous_certainty}')\n",
        "\n",
        "  return (Left_Position, Right_Position, Scoring_Box_Position, Tracking_Bounding_Boxes, Left_Torso_Position, Right_Torso_Position, engarde_length, certainty)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY9eLcBIMerm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Bell_Guard_Position_Tracking\n",
        "def Bell_Guard_Position_Tracking(boxes, previous_position, twice_previous_position, certainty, tracked_item, frame, Torso_Position, Torso_Size, capture_width, capture_height, fencer_data, engarde_length, camera_steady, camera_motion_threshold):\n",
        "  # Tracks the position of items\n",
        "  # tracked_item is needed since boxes only has the class of the item tracked, not the Left or Right\n",
        "  # tracked_item Format: [0,1,2,3] = [Background, Bell_Guard, Score_Box, Torso]\n",
        "\n",
        "  #Assumed inherent uncertainty\n",
        "  certainty_default = int(capture_width/16)\n",
        "  certainty_multiplier = int(capture_width/80)\n",
        "\n",
        "  #Reduces the max value of y as compared to x\n",
        "  y_limiter = 24\n",
        "\n",
        "  boxes_temp = []\n",
        "  #Filters out potential boxes based on Tracked Item, Confidence and Saturation of the Box\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard'):\n",
        "\n",
        "    #Uses only the Fencer_Data and Certainty for the Appropriate side.\n",
        "    if tracked_item == 'Left_BellGuard':\n",
        "      fencer_data = fencer_data[0]\n",
        "      # display(f'The certainty here is {certainty}.')\n",
        "      bell_certainty = certainty\n",
        "    else:\n",
        "      #Assumes the Right BellGuard\n",
        "      fencer_data = fencer_data[1]\n",
        "      bell_certainty = certainty\n",
        "\n",
        "    for j in range(len(boxes)):\n",
        "      #The minimum required certainty for a bellguard box\n",
        "      if ((boxes[j][2] == 1) and (boxes[j][1] > (bellguard_confidence - bellguard_tracking_det_offset))):\n",
        "        boxes_temp.append(boxes[j])\n",
        "\n",
        "  elif (tracked_item == 'Left_Torso' or tracked_item == 'Right_Torso'):\n",
        "    for j in range(len(boxes)):\n",
        "      if ((boxes[j][2] == 3) and (boxes[j][1] > min_torso_confidence)):\n",
        "        #Bypasses the Saturation Test\n",
        "        # test_result = saturation_test(boxes[j], frame)\n",
        "        test_result = True\n",
        "        if test_result == True:\n",
        "          boxes_temp.append(boxes[j])\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'The saturation test failed at frame {frame_count}.')\n",
        "          else:\n",
        "            pass\n",
        "  elif (tracked_item == 'Scoring_Box'):\n",
        "    for j in range(len(boxes)):\n",
        "      if (boxes[j][2] == 2):\n",
        "        boxes_temp.append(boxes[j])\n",
        "\n",
        "  # Assigns boxes_temp to boxes\n",
        "  boxes = boxes_temp\n",
        "\n",
        "  # Creates points at the centers of the bounding boxes that are in this frame\n",
        "  x_center = []\n",
        "  y_center = []\n",
        "  for i in range(len(boxes)):\n",
        "    x_center.append(int((boxes[i][0][1] + boxes[i][0][3])/2))\n",
        "    y_center.append(int((boxes[i][0][0] + boxes[i][0][2])/2))\n",
        "\n",
        "  # Max allowed speed of a bellguard in a single frame\n",
        "  # Accounts for a position jump following the engarde positioning\n",
        "  if frame < engarde_length + 3:\n",
        "    max_speed = int(capture_width/64)\n",
        "  else:\n",
        "    max_speed = int(capture_width/24)\n",
        "\n",
        "  # Converts previous position into a speed\n",
        "  x_pos = int(previous_position[0])\n",
        "  if verbose == True:\n",
        "    display(f'previous_position is {previous_position} and twice_previous_position is {twice_previous_position}.')\n",
        "  x_speed = int(min(previous_position[0] - twice_previous_position[0], max_speed))\n",
        "  y_pos = int(previous_position[1])\n",
        "  y_speed = int(min(previous_position[1] - twice_previous_position[1], int(max_speed/y_limiter)))\n",
        "  y_speed = int(max(y_speed, int(max_speed*(-1)/y_limiter)))\n",
        "\n",
        "  if (frame - 1)  == engarde_length and verbose == True:\n",
        "      display(f'THe x_speed is {x_speed} and the y_speed is {y_speed} at the engarde length, frame {frame - 1}.')\n",
        "\n",
        "  # Flips the tracking box to be between the two fencers\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Left_Torso':\n",
        "    horiz_flip = False\n",
        "    if verbose == True:\n",
        "      display(f'The horizontal flip is {horiz_flip} for the {tracked_item} at frame {frame - 1}.')\n",
        "  elif tracked_item == 'Right_BellGuard' or tracked_item == 'Right_Torso':\n",
        "    horiz_flip = True\n",
        "    if verbose == True:\n",
        "      display(f'The horizontal flip is {horiz_flip} for the {tracked_item} at frame {frame - 1}.')\n",
        "  else:\n",
        "    horiz_flip = False\n",
        "\n",
        "\n",
        "  # Defines the tracking box\n",
        "  expected_position = [(x_pos + x_speed),(y_pos + y_speed)]\n",
        "  padding = int(certainty*certainty_multiplier + certainty_default)\n",
        "  boundary_box_for_tracking = [padding, padding, padding, padding]\n",
        "  tracking_box = create_boundary_box(expected_position, boundary_box_for_tracking, horiz_flip)\n",
        "  positions = []\n",
        "\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    # Sets the values for the torso boundary box, limits Bellguard distance from Torso center\n",
        "    boundary_box_for_torso = [int(Torso_Size[0]*0.20), int(Torso_Size[0]*3.25), int(Torso_Size[1]*.75), int(Torso_Size[1]*1.0)]\n",
        "    # Uses the boundary box to create a box based on Left/Right and expected/previous position\n",
        "    torso_box = create_boundary_box(Torso_Position, boundary_box_for_torso, horiz_flip)\n",
        "    # Finds the overlap of multiple boxes to satisy multiple restrictions\n",
        "    [x_min, x_max, y_min, y_max] = boundary_box_overlap(tracking_box, torso_box)\n",
        "    if verbose == True:\n",
        "      display(f'The Torso_Size[0] is {Torso_Size[0]}, the Horizontal Flip is {horiz_flip} and Torso_Position is {Torso_Position}.')\n",
        "  else:\n",
        "    [x_min, x_max, y_min, y_max] = tracking_box\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The tracking box for the {tracked_item} at frame {frame - 1} is: {tracking_box}.')\n",
        "\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard') and verbose == True:\n",
        "    display(f'The torso box for the {tracked_item} at frame {frame - 1} is: {torso_box}.')\n",
        "    display(f'The overlapping tracking box for the {tracked_item} at frame {frame - 1} is: {[x_min, x_max, y_min, y_max]}.')\n",
        "\n",
        "  # Creates a list of positions within the bounding boxes\n",
        "  for i in range(len(boxes)):\n",
        "    center = [x_center[i], y_center[i]]\n",
        "    tracking_result = boundary_box_test(center,tracking_box)\n",
        "    # If the center point is within both boxes for Bellguards or tracking box for other items, then it is appended to positions\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      torso_result = boundary_box_test(center,torso_box)\n",
        "      # Allows for an incorrect engarde position for the bellguard\n",
        "      if (frame - 1) > engarde_length + 3:\n",
        "        if tracking_result == True and torso_result == True:\n",
        "          positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "      else:\n",
        "        # Only the torso results is required for the engarde positioning\n",
        "        if torso_result == True:\n",
        "          positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "    else:\n",
        "      if tracking_result == True:\n",
        "        positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "\n",
        "  # Maximum distance only applies if there are multiple bounding boxes within the tracking box\n",
        "  maximum_distance_from_expected = int(capture_width/24)\n",
        "  # Expected Position [x,y], Limits expected position in front of the fencer\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    display(f'The expected position is {expected_position} and Torso Position and size is {Torso_Position[0]} and {Torso_Size[0]}.')\n",
        "    if (expected_position[0] > Torso_Position[0] + Torso_Size[0]*2.0) and tracked_item == 'Left_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was too far in front of the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0] + Torso_Size[0]*2.0), y_pos]\n",
        "    if (expected_position[0] < Torso_Position[0]) and tracked_item == 'Left_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was behind the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0]), y_pos]\n",
        "    if expected_position[0] < Torso_Position[0] - Torso_Size[0]*2.0 and tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was too far from the Torso, adjusting expected.')\n",
        "        display(f'Torso_Position[0] is {Torso_Position[0]}, Torso_Size[0] is {Torso_Size[0]}, y_pos is {y_pos}.')\n",
        "      expected_position = [int(Torso_Position[0] - Torso_Size[0]*2.0), y_pos]\n",
        "    if (expected_position[0] > Torso_Position[0]) and tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was behind the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0]), y_pos]\n",
        "\n",
        "  #Assumed maximum distance from wrist to bellguard\n",
        "  wrist_to_bellguard_max = int(Torso_Size[0]/8)\n",
        "\n",
        "  #Sets Initial Conditions for Type of Tracking\n",
        "  using_human_pose = False\n",
        "  using_difference_images = False\n",
        "  using_expected = False\n",
        "  using_position = False\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The camera steady value for frame {frame - 1} is {camera_steady[frame - 1]}.')\n",
        "    if camera_steady[frame - 1] >= camera_motion_threshold:\n",
        "      display(f'The camera is in motion and motion detection is less reliable.')\n",
        "\n",
        "  # Determines the Bellguard Position based on number of detections, confidence, box location and motion\n",
        "  if (len(positions)) == 0:\n",
        "    display(f'There where no positions found for the {tracked_item} at frame {frame - 1}.')\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      #Uses a larger boundary box if high confidence in wrist position\n",
        "      if fencer_data[0][2] > wrist_conf_very_high:\n",
        "        display(f'The wrist confidence is very high, using a larger human pose boundary.')\n",
        "        human_pose_boundary = [int(Torso_Size[0]*1.5), int(Torso_Size[0]*1.75), int(Torso_Size[0]), int(Torso_Size[0])]\n",
        "      else:\n",
        "        display(f'The wrist confidence is not very high, using a smaller human pose boundary.')\n",
        "        human_pose_boundary = [int(Torso_Size[0]/2), int(Torso_Size[0]*1.25), int(Torso_Size[0]*3/4), int(Torso_Size[0]*3/4)]\n",
        "      wrist_position = [fencer_data[0][0], fencer_data[0][1]]\n",
        "      display(f'Attempting Human Pose Approximation for the {tracked_item} at frame {frame - 1}.')\n",
        "      if tracked_item == 'Left_BellGuard':\n",
        "        boundary_box = create_boundary_box(expected_position, human_pose_boundary, False)\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "      else:\n",
        "        boundary_box = create_boundary_box(expected_position, human_pose_boundary, True)\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "      if verbose == True:\n",
        "        display(f'{tracked_item} : wrist conf:{fencer_data[0][2]}, box_test:{box_test}.')\n",
        "      if fencer_data[0][2] > wrist_conf_min and box_test:\n",
        "        #Wrist Pose Approximation\n",
        "        if verbose == True:\n",
        "          display(f'Using the Wrist Approximation for the {tracked_item} at frame {frame - 1}.')\n",
        "          display(f'The fencer data for frame {frame - 1} is:')\n",
        "          display(fencer_data)\n",
        "        using_human_pose = True\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          position = [fencer_data[0][0] + int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "        elif tracked_item == 'Right_BellGuard':\n",
        "          #Right_Bellguard is assumed\n",
        "          position = [fencer_data[0][0] - int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "        else:\n",
        "          display(f'The tracked item was not a Bell Guard at frame {frame - 1}.')\n",
        "      #If the Human Pose is outside of bounds then motion difference is tried\n",
        "      else:\n",
        "        motion_difference_boundary = [int(Torso_Size[0]/4), int(Torso_Size[0]), int(Torso_Size[0]/3), int(Torso_Size[0]/3)]\n",
        "        if tracked_item == 'Left_BellGuard' and camera_steady[frame - 1] < camera_motion_threshold:\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "          position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, 0.5, 3, 4)\n",
        "          if position == 'None':\n",
        "            display(f'Attempting to use a smaller kernel for motion difference tracking.')\n",
        "            position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, 1, 3, 4)\n",
        "            if position == 'None':\n",
        "              display(f'Attempting to use a smallest kernel for motion difference tracking.')\n",
        "              position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, 3, 3, 4)\n",
        "          if verbose == True:\n",
        "            display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "            display(f'The boundary box test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          #Uses the Expected position if the motion difference is out of bounds\n",
        "          if box_test == False:\n",
        "            display(f'Motion difference failed, using the Expected Position for the {tracked_item} for frame {frame - 1}.')\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "          else:\n",
        "            display(f'The motion difference position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "            using_difference_images = True\n",
        "        elif tracked_item == 'Right_BellGuard' and camera_steady[frame - 1] < camera_motion_threshold:\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "          position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, 0.5, 3, 4)\n",
        "          if position == 'None':\n",
        "            display(f'Attempting to use a smaller kernel for motion difference tracking.')\n",
        "            position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, 1, 3, 4)\n",
        "            if position == 'None':\n",
        "              display(f'Attempting to use a smallest kernel for motion difference tracking.')\n",
        "              position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, 3, 3, 4)\n",
        "          if verbose == True:\n",
        "            display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "            display(f'The boundary box test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          # box_test = False\n",
        "          if box_test == False:\n",
        "            display(f'Motion difference failed, using the Expected Position for the {tracked_item} for frame {frame - 1}.')\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "          else:\n",
        "            display(f'The motion difference position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "            using_difference_images = True\n",
        "        else:\n",
        "          display(f'Too much camera motion, using expected position')\n",
        "          position = expected_position\n",
        "          using_expected = True\n",
        "    else:\n",
        "      position = expected_position    \n",
        "\n",
        "    # Criteria for Setting Certainty to zero preventing a linear appoximation adjustment of this point\n",
        "    if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high) or \\\n",
        "    (using_difference_images == True and position[1] < Torso_Position[1] + Torso_Size[1]/2 and camera_steady[frame - 1] < camera_motion_threshold):\n",
        "      if using_difference_images == True:\n",
        "        display(f'Using difference images for frame {frame - 1} with no detected positions')\n",
        "      certainty = 0\n",
        "    else:\n",
        "      certainty = certainty + 1\n",
        "\n",
        "  # For a single detected Bellguard Position\n",
        "  elif (len(positions)) == 1:\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      display(f'There is one possible position, {positions[0]} for {tracked_item} in the tracking box for frame {frame - 1}.')\n",
        "      # single_position_box = [int(Torso_Size[0]/2*(1+bell_certainty/4)), int(Torso_Size[0]*(1+bell_certainty/4)), int((Torso_Size[0]/2)*(1+bell_certainty/4)), int((Torso_Size[0]/2)*(1+bell_certainty/4))]\n",
        "      single_position_box = [int(Torso_Size[0]*3/4*(1+bell_certainty/4)), int(Torso_Size[0]*(1+bell_certainty/4)), int(Torso_Size[0]), int(Torso_Size[0])]\n",
        "      if tracked_item == 'Left_BellGuard':\n",
        "        boundary_box = create_boundary_box(expected_position, single_position_box, False)\n",
        "      else:\n",
        "        boundary_box = create_boundary_box(expected_position, single_position_box, True)\n",
        "      box_test = boundary_box_test(positions[0], boundary_box)\n",
        "      display(f'The expected position for frame {frame - 1} is {expected_position}.')\n",
        "      display(f'The single_position_box is {single_position_box} and the boundary box is {boundary_box}.')\n",
        "      # Requires Box Boundary and Human Pose Wrist confidence less than high confidence.\n",
        "      # if box_test == True and fencer_data[0][2] < wrist_conf_high:\n",
        "      if box_test == True and positions[0][2] > bellguard_confidence_high:\n",
        "        display(f'The detected position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "        position = positions[0]\n",
        "        using_position = True\n",
        "      else:\n",
        "        #Human Pose\n",
        "        display(f'Attempting to use Human Pose for the {tracked_item} at frame {frame - 1}')\n",
        "        if fencer_data == 'None':\n",
        "          fencer_data = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "        human_pose_boundary = [int(Torso_Size[0]*3/4), int(Torso_Size[0]), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "        display(f'Fencer data for frame {frame - 1} is: {fencer_data}.')\n",
        "        wrist_position = [fencer_data[0][0], fencer_data[0][1]]\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, False)\n",
        "        else:\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, True)\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "        if fencer_data[0][2] > wrist_conf_min and box_test:\n",
        "          if verbose == True:\n",
        "            display(f'{tracked_item}: wrist conf:{fencer_data[0][2]}, box_test:{box_test}.')\n",
        "            display(f'Using the Wrist Approximation for the {tracked_item} at frame {frame - 1}.')\n",
        "            display(f'The fencer data for frame {frame - 1} is:')\n",
        "            using_human_pose = True\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            position = [fencer_data[0][0] + int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "          else:\n",
        "            #Right_Bellguard is assumed\n",
        "            position = [fencer_data[0][0] - int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "        else:\n",
        "          #Image Difference\n",
        "          display(f'Attempting to use Image Difference for the {tracked_item} at frame {frame - 1}')\n",
        "          motion_difference_boundary = [int(Torso_Size[0]/8), int(Torso_Size[0]/2), int(Torso_Size[0]/4), int(Torso_Size[0]/4)]\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "            diff_position = motion_difference_tracking(frame, 'Left', [x_min, x_max, y_min, y_max], capture_width, capture_height, 1, 1, 2)\n",
        "            if diff_position == 'None':\n",
        "              diff_position = motion_difference_tracking(frame, 'Left', [x_min, x_max, y_min, y_max], capture_width, capture_height, 2, 1, 2)\n",
        "          else:\n",
        "            #Right Bellguard is assumed\n",
        "            boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "            diff_position = motion_difference_tracking(frame, 'Right', [x_min, x_max, y_min, y_max], capture_width, capture_height, 1, 1, 2)\n",
        "            if diff_position == 'None':\n",
        "              diff_position = motion_difference_tracking(frame, 'Right', [x_min, x_max, y_min, y_max], capture_width, capture_height, 2, 1, 2)\n",
        "          box_test = boundary_box_test(diff_position, motion_difference_boundary)\n",
        "          if box_test == True and diff_position != 'None':\n",
        "            position = diff_position\n",
        "            using_difference_images = True\n",
        "          else:\n",
        "            #Expected Position\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "          if verbose == True:\n",
        "            display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "            display(f'The motion_difference_boundary test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "\n",
        "      # Designed to catch an engarde position that is outside the tracking box\n",
        "      if frame < (engarde_length + 3) and position == twice_previous_position:\n",
        "        position = positions[0]\n",
        "\n",
        "      #Sets Certainty Box\n",
        "      if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high) or (using_position == True):\n",
        "        certainty = 0\n",
        "        display(f'Certainty set to zero for frame {frame - 1} for the {tracked_item}.')\n",
        "      else:\n",
        "        certainty = certainty + 1\n",
        "\n",
        "    else:\n",
        "      position = positions[0]\n",
        "\n",
        "  # Multiple bounding boxes within the tracking box\n",
        "  elif (len(positions)) > 1:\n",
        "    display(f'Multiple Bounding Boxes Detected for the {tracked_item} at frame {frame - 1}')\n",
        "    # One set of conditions is used for Bell_Guards and another for all else\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      # If the fencer_data wrist is confident, then it is used for Bell_Guards\n",
        "      if fencer_data[0][2] > wrist_conf_min:\n",
        "        display(f'Wrist Confidence Greater than Minimum for the {tracked_item} at frame {frame - 1}.')\n",
        "        display(f'The Pose Confidence is {fencer_data[0][2]} with a required minimum of {wrist_conf_min}.')\n",
        "        human_pose_boundary = [int(Torso_Size[0]/4), int(Torso_Size[0]/2), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "        wrist_position = [fencer_data[0][0], fencer_data[0][1]]\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, False)\n",
        "        else:\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, True)\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "        if tracked_item == 'Left_BellGuard' and box_test == True:\n",
        "          position = [fencer_data[0][0] + int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/6)]\n",
        "          using_human_pose = True\n",
        "          display(f'Using the wrist position of {position} for the {tracked_item} at frame {frame - 1}.')\n",
        "        elif tracked_item == 'Right_BellGuard' and box_test == True:\n",
        "          position = [fencer_data[0][0] - int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/6)]\n",
        "          using_human_pose = True\n",
        "          display(f'Using the wrist position of {position} for the {tracked_item} at frame {frame - 1}.')\n",
        "        else:\n",
        "          # Tests if the Position Confidence is High for the Bellguard\n",
        "          if positions[0][2] > bellguard_confidence_high:\n",
        "            position = multiple_box_determination(expected_position, positions, [human_pose_boundary[0], human_pose_boundary[1]], bellguard_confidence, horiz_flip)\n",
        "            using_position = True\n",
        "          else:\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "            display(f'The Human Pose Box Test failed for the {tracked_item} at frame {frame - 1}, using expected position.')\n",
        "          display(f'The point tested is {wrist_position} and the box is {boundary_box} for human pose at for the {tracked_item} at frame {frame - 1}')\n",
        "        # If the wrist confidence is not High, while the bellguard is, then uses the Bellguard Position\n",
        "        if fencer_data[0][2] < wrist_conf_high and positions[0][2] > bellguard_confidence_high:\n",
        "          single_position_box = [int(Torso_Size[0]/2), int(Torso_Size[0]), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            boundary_box = create_boundary_box(expected_position, single_position_box, False)\n",
        "          else:\n",
        "            boundary_box = create_boundary_box(expected_position, single_position_box, True)\n",
        "          box_test = boundary_box_test(positions[0], boundary_box)\n",
        "          if box_test:\n",
        "            display(f'Using the High Confidence Bellguard for Multiple Boxes for the {tracked_item} at frame {frame - 1}.')\n",
        "            position = positions[0]\n",
        "            using_position = True\n",
        "\n",
        "      # If the fencer_data wrist is not confident\n",
        "      else:\n",
        "        display(f'Insufficient Pose Confidence for the {tracked_item} at frame {frame - 1}.')\n",
        "        if verbose == True:\n",
        "          display(f'The Pose Confidence is {fencer_data[0][2]} with a required minimum of {wrist_conf_min}.')\n",
        "          display(f'The x value is  {fencer_data[0][0]} with a minimum of {x_min} and a maximum of {x_max}.')\n",
        "          display(f'The x value is  {fencer_data[0][1]} with a minimum of {y_min} and a maximum of {y_max}.')\n",
        "        # Excludes Positions too far from expected but still within the tracking box\n",
        "        within_distance_from_expected = []\n",
        "        for i in range(len(positions)):\n",
        "          expected_box = [int(Torso_Size[0]/2*(1+bell_certainty/4)), int(Torso_Size[0]*(1+bell_certainty/4)), int(Torso_Size[0]/6*(1+bell_certainty/4)), int(Torso_Size[0]/6)]\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            boundary_box = create_boundary_box(expected_position, expected_box, False)\n",
        "          else:\n",
        "            boundary_box = create_boundary_box(expected_position, expected_box, True)\n",
        "          box_test = boundary_box_test(positions[i], boundary_box)\n",
        "          if box_test:\n",
        "            within_distance_from_expected.append(positions[i])\n",
        "\n",
        "        # Uses the most confident, i.e. the first position in the list\n",
        "        if len(within_distance_from_expected) > 0:\n",
        "          position_boundary = [int(Torso_Size[0]/4), int(Torso_Size[0]/2), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "          position = multiple_box_determination(expected_position, positions, [position_boundary[0], position_boundary[1]], bellguard_confidence, horiz_flip)\n",
        "          certainty = 0\n",
        "          using_position = True\n",
        "        else:\n",
        "        # If the length of within_distance_from_expected is zero\n",
        "          display(f'Error occured finding a position within the required distance and the {tracked_item} set to expeced position at frame {frame - 1}.')\n",
        "          display(f'The expected position is {expected_position}, while the expected box is {expected_box}.')\n",
        "          position = [(x_pos + x_speed),(y_pos + y_speed)]\n",
        "          using_expected = True\n",
        "\n",
        "      #Sets Certainty Box\n",
        "      if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high) or (using_position == True):\n",
        "        if verbose == True:\n",
        "          display(f'Confidence for the {tracked_item} is High so the certainty is set to zero.')\n",
        "        certainty = 0\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'Confidence for the {tracked_item} is Low so the certainty is incremented higher.')\n",
        "        certainty = certainty + 1\n",
        "\n",
        "    # If the tracked item is not a bell_guard\n",
        "    else:\n",
        "      #Uses the most confident position within the tracking box\n",
        "      position = positions[0]\n",
        "\n",
        "\n",
        "  #Prevents the Bellguard being hidden behind the knee by setting a bellguard position behind the knee to the knee position.\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard') and fencer_data[1][2] > knee_conf_min:\n",
        "    distance_from_position_to_knee = abs(int(((position[0] - fencer_data[1][0])**2 + (position[1] - fencer_data[1][1])**2)**(0.5)))\n",
        "    \n",
        "    display(f'The distance for the {tracked_item} from the knee is {distance_from_position_to_knee} and the min is {(Torso_Size[0]/2)} at frame {frame - 1}.')\n",
        "    if tracked_item == 'Left_BellGuard':\n",
        "      if distance_from_position_to_knee < (Torso_Size[0]/2) and (position[0] < fencer_data[1][0]) and (fencer_data[0][2] > wrist_conf_min):\n",
        "        position = [fencer_data[1][0] + int(Torso_Size[0]/8), fencer_data[1][1] - int(Torso_Size[0]/12)]\n",
        "        display(f'The {tracked_item} is near the knee at frame {frame - 1}.')\n",
        "    else:\n",
        "      #Assumes Right_BellGuard\n",
        "      if distance_from_position_to_knee < (Torso_Size[0]/2) and (position[0] > fencer_data[1][0]) and (fencer_data[0][2] > wrist_conf_min):\n",
        "        position = [fencer_data[1][0] - int(Torso_Size[0]/8), fencer_data[1][1] - int(Torso_Size[0]/12)]\n",
        "        display(f'The {tracked_item} is near the knee at frame {frame - 1}.')\n",
        "\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    display(f'The position of the {tracked_item} at frame {frame - 1} is {position}.')\n",
        "\n",
        "  return (position, certainty, [x_min, x_max, y_min, y_max])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRV4tGfTVF2",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Weight_Average_List\n",
        "def weight_average_list(List):\n",
        "  # Finds the Weight Average of a List\n",
        "\n",
        "  # Prevents division by zero\n",
        "  try:\n",
        "    value_sum = 0\n",
        "    value_weight = 0\n",
        "    for i in range(len(List)):\n",
        "      value_sum = (value_sum + List[i][0]) * List[i][1]\n",
        "      value_weight = value_weight + List[i][1]\n",
        "    weighted_average = value_sum/value_weight\n",
        "  except:\n",
        "    weighted_average = 0\n",
        "\n",
        "  return (weighted_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAkHbcyRCmQk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Average_List\n",
        "def average_list(List):\n",
        "  # Finds the Average of a List\n",
        "  try:\n",
        "    average = sum(List) / len(List)\n",
        "  except:\n",
        "    average = 0\n",
        "  return (average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZCO-GE5eqUl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Color_Tester\n",
        "def color_tester(box, frame):\n",
        "  #Takes a given box and tests for a specific color range\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "  file_name = str(frame) + '.jpg'\n",
        "  name = os.path.join(path, file_name)\n",
        "  img = cv2.imread(name)\n",
        "\n",
        "  display(f'The file names to be color tested is {name}.')\n",
        "  # box[0] are the coordinates ([y1,x1,y2,x2]), box[1] is confidence and box[2] is object\n",
        "  # Tests if Bellguard is the correct color\n",
        "  if box[2] == 1:\n",
        "    blue_range = [50, 150]\n",
        "    green_range = [50, 150]\n",
        "    red_range = [50, 160]\n",
        "    max_delta = 25\n",
        "  elif box[2] == 3:\n",
        "    blue_range = [60, 150]\n",
        "    green_range = [60, 150]\n",
        "    red_range = [60, 160]\n",
        "    max_delta = 30\n",
        "  else:\n",
        "    display(f'The object to test does not have a color profile.')\n",
        "\n",
        "  # OpenCV uses Blue, Green, Red order\n",
        "  b, g, r = 0, 0, 0\n",
        "\n",
        "  width = (box[0][3]-box[0][1])\n",
        "  height = (box[0][2]-box[0][0])\n",
        "\n",
        "  #i is the x value of the image\n",
        "  for i in range(width):\n",
        "    #j is y value of the image\n",
        "    for j in range(height):\n",
        "      #color channel of the image [B,G,R]\n",
        "      #image, img, is of format [y,x] \n",
        "      b = b + img[box[0][0] + j, box[0][1] + i, 0]\n",
        "      g = g + img[box[0][0] + j, box[0][1] + i, 1]\n",
        "      r = r + img[box[0][0] + j, box[0][1] + i, 2]\n",
        "\n",
        "  # Finds the Color Averages\n",
        "  b_average = int(b/(width*height))\n",
        "  g_average = int(g/(width*height))\n",
        "  r_average = int(r/(width*height))\n",
        "\n",
        "  # Finds maximum differences between colors\n",
        "  max_1 = abs(b_average - g_average)\n",
        "  max_2 = abs(b_average - r_average)\n",
        "  max_3 = abs(g_average - r_average)\n",
        "  max_delta = max(max_1, max_2, max_3)\n",
        "\n",
        "  if test_result == False:\n",
        "    display(f'The Color Test Result Failed for object {box[2]}.')\n",
        "\n",
        "  return (test_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPntbZz1PTB-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Symmetry_Test\n",
        "def symmetry_test(width, height, left_x, left_y, right_x, right_y):\n",
        "\n",
        "  # Tests the potential left and right positions for left/right symmetry and removes outlier points\n",
        "  display(f'Commencing Symmetry Test...')\n",
        "\n",
        "  # Sets how large the allowable band is with respect to height or width\n",
        "  band_width_ratio_x = 8\n",
        "  band_width_ratio_y = 8\n",
        "\n",
        "  all_positions_x = left_x + right_x\n",
        "  all_positions_y = left_y +right_y\n",
        "  if len(all_positions_x) != len(all_positions_y):\n",
        "    display(f'ERROR...The length of the x and y positions are different.')\n",
        "\n",
        "\n",
        "  # Keeps track of which positions are most in line with the other positions\n",
        "  # Finds the X Band\n",
        "  x_distances_from_center = []\n",
        "  x_distances_from_other_points_score = []\n",
        "  for i in range(len(all_positions_x)):\n",
        "    #Determines the x_min band for each position by distance from center\n",
        "    x_distances_from_center.append(abs(int((width/2)-all_positions_x[i])))\n",
        "  #Creates an iterator that determines which x_point is close to the most other points and finds its index\n",
        "  for j in range(len(x_distances_from_center)):\n",
        "    score = 0\n",
        "    for k in range(len(x_distances_from_center) - 1):\n",
        "      if abs(x_distances_from_center[j] - x_distances_from_center[k+1]) < width/band_width_ratio_x:\n",
        "        score = score + 1\n",
        "      else:\n",
        "        pass\n",
        "    x_distances_from_other_points_score.append(score)\n",
        "  x_index_band = x_distances_from_other_points_score.index(max(x_distances_from_other_points_score))\n",
        "\n",
        "  x_min = abs(int(all_positions_x[x_index_band] - width/band_width_ratio_x))\n",
        "  x_max = abs(int(all_positions_x[x_index_band] + width/band_width_ratio_x))\n",
        "\n",
        "  # Finds the Y Band\n",
        "  y_distances_from_center = []\n",
        "  y_distances_from_other_points_score = []\n",
        "  for i in range(len(all_positions_y)):\n",
        "    y_distances_from_center.append(abs(int((height/2)-all_positions_y[i])))\n",
        "  for j in range(len(y_distances_from_center)):\n",
        "    score = 0\n",
        "    for k in range(len(y_distances_from_center) - 1):\n",
        "      if abs(y_distances_from_center[j] - y_distances_from_center[k+1]) < width/band_width_ratio_y:\n",
        "        score = score + 1\n",
        "      else:\n",
        "        pass\n",
        "    y_distances_from_other_points_score.append(score)\n",
        "  y_index_band = y_distances_from_other_points_score.index(max(y_distances_from_other_points_score))\n",
        "\n",
        "  y_min = abs(int(all_positions_y[y_index_band] - width/band_width_ratio_y))\n",
        "  y_max = abs(int(all_positions_y[y_index_band] + width/band_width_ratio_y))\n",
        "\n",
        "  # Cycles through the positions and keeps values that are in the horizontal x band\n",
        "  positionsx_temp = []\n",
        "  positionsy_temp = []\n",
        "\n",
        "  display(f'The x_min/max is {x_min}/{x_max}, the band width is {width/band_width_ratio_x} and the center is {width/2}.')\n",
        "\n",
        "  for i in range(len(all_positions_x)):\n",
        "    if ((all_positions_x[i] < (width/2 - x_min)) and (all_positions_x[i] > (width/2 - x_max))) or ((all_positions_x[i] < (width/2 + x_max)) and (all_positions_x[i] > (width/2 + x_min))):\n",
        "      positionsx_temp.append(all_positions_x[i])\n",
        "      positionsy_temp.append(all_positions_y[i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  # Replaces the all position x and y lists with the temp list limited by the bands\n",
        "  all_positions_x = positionsx_temp\n",
        "  all_positions_y = positionsy_temp\n",
        "\n",
        "  #Cycles through the positions and keeps values that are in the vertical y band\n",
        "  positionsx_temp = []\n",
        "  positionsy_temp = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The y_min/max is {y_min}/{y_max}, the band width is {height/band_width_ratio_y} and the center is {height/2}.')\n",
        "\n",
        "  for i in range(len(all_positions_y)):\n",
        "    if ((all_positions_y[i] > (y_min)) and (all_positions_y[i] < (y_max))):\n",
        "      positionsx_temp.append(all_positions_x[i])\n",
        "      positionsy_temp.append(all_positions_y[i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  # Replaces the all position x and y lists with the temp list limited by the bands\n",
        "  all_positions_x = positionsx_temp\n",
        "  all_positions_y = positionsy_temp\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'There were originaly {len(left_x) + len(right_x)} values and {len(all_positions_x) - (len(left_x) + len(right_x))} were removed.')\n",
        "\n",
        "  # Returns the x and y values to left and right positions\n",
        "  ret_left_x, ret_left_y, ret_right_x, ret_right_y = [],[],[],[]\n",
        "\n",
        "  \n",
        "  for i in range(len(all_positions_x)):\n",
        "    # Tests if the x value is on the left or right side\n",
        "    if all_positions_x[i] < width/2:\n",
        "      ret_left_x.append(all_positions_x[i])\n",
        "      ret_left_y.append(all_positions_y[i])\n",
        "    else:\n",
        "      ret_right_x.append(all_positions_x[i])\n",
        "      ret_right_y.append(all_positions_y[i])\n",
        "  # Prevents an off center camera from removing all engarde points\n",
        "  if (len(ret_left_x) == 0) or (len(ret_left_y) == 0) or (len(ret_right_x) == 0) or (len(ret_right_y) == 0):\n",
        "    ret_left_x = left_x\n",
        "    ret_left_y = left_y\n",
        "    ret_right_x = right_x\n",
        "    ret_right_y = right_y\n",
        "\n",
        "  return (ret_left_x, ret_left_y, ret_right_x, ret_right_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR6IATJKmVDz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title List_Threshold_Test\n",
        "def list_threshold_test(threshold, list_to_test):\n",
        "  #Determines if a list meets a minimum threshold\n",
        "  threshold_met = False\n",
        "\n",
        "  for k in range(len(list_to_test)):\n",
        "    if list_to_test[k][1] > threshold:\n",
        "      threshold_met = True\n",
        "      break\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return(threshold_met)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnsMsig-fR4M",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Multiple_Box_Determination\n",
        "def multiple_box_determination(expected_position, positions, x_boundaries, min_conf, horiz_flip):\n",
        "\n",
        "  confidence_weighting = .9\n",
        "\n",
        "  delta_x_forward = x_boundaries[1]\n",
        "  delta_x_backward = x_boundaries[0]\n",
        "\n",
        "  if horiz_flip == True:\n",
        "    delta_temp = delta_x_forward\n",
        "    delta_x_forward = delta_x_backward\n",
        "    delta_x_backward = delta_temp\n",
        "\n",
        "  position_ratings = []\n",
        "\n",
        "  display(f'There are {len(positions)} positions available.')\n",
        "  display(f'The positions are:')\n",
        "  display(positions)  \n",
        "\n",
        "  for i in range(len(positions)):\n",
        "    delta_position = positions[i][0] - expected_position[0]\n",
        "    if verbose == True:\n",
        "      display(f'The positions{i}[0] is {positions[i][0]} and the expected_position[0] is {expected_position[0]} therefore delta position is {delta_position}.')\n",
        "    if delta_position > 0:\n",
        "      if verbose == True:\n",
        "        display(f'Position {i} is forward of the expected position.')\n",
        "      position_ratings.append(abs((delta_position/delta_x_forward)*(1-positions[i][2])**confidence_weighting))\n",
        "      display(f'delta_position is {delta_position}.')\n",
        "      display(f'delta_x_forward is {delta_x_forward}.')\n",
        "      display(f'positions[i][2] is {positions[i][2]}.')\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'Position {i} is behind the expected position.')\n",
        "      position_ratings.append(abs((delta_position/delta_x_backward)*(1-positions[i][2])**confidence_weighting))\n",
        "      display(f'delta_position is {delta_position}.')\n",
        "      display(f'delta_x_backward is {delta_x_backward}.')\n",
        "      display(f'positions[i][2] is {positions[i][2]}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(position_ratings)\n",
        "\n",
        "  position = positions[position_ratings.index(min(position_ratings))]\n",
        "\n",
        "  return (position)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtyzedR9Aief",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Boundary_Box_Overlap\n",
        "def boundary_box_overlap(box1, box2):\n",
        "  #Finds the overlap of two boxes assume (x_min, x_max, y_min, y_max)\n",
        "  \n",
        "  box_overlap = [max(box1[0], box2[0]), min(box1[1], box2[1]), max(box1[2], box2[2]), min(box1[3], box2[3])]\n",
        "\n",
        "  return(box_overlap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8uvTRrIxL3T",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create_Boundary_Box\n",
        "def create_boundary_box(center, padding, horiz_flip):\n",
        "  # Creates a Boundary Box based on Center Padding and if the Left and Right Boundaries should be flipped.\n",
        "  # Center is [x,y]\n",
        "  # Padding is [Left, Right, Top, Bottom]\n",
        "  # horiz_flip is True or False\n",
        "\n",
        "  if horiz_flip == False:\n",
        "    left = center[0] - padding[0]\n",
        "    right = center[0] + padding[1]\n",
        "  elif horiz_flip == True:\n",
        "    left = center[0] - padding[1]\n",
        "    right = center[0] + padding[0]\n",
        "  else:\n",
        "    display(f'ERROR Horiz Flip not True or False.')\n",
        "\n",
        "  top = center[1] - padding[2]\n",
        "  bottom = center[1] + padding[3]\n",
        "\n",
        "  return ([left, right, top, bottom])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnDPSNJoQINX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Boundary_Box_Test\n",
        "def boundary_box_test(test_point, boundary):\n",
        "  # Tests if a give point is in a Boundary Box.\n",
        "  #Format Test_Point is of the form (x,y)\n",
        "  #Format Boundary is of the form (x_min, x_max, y_min, y_max)\n",
        "  #Format Boundary is of the form (behind the fencer, in front of the fencer, above the fencer, below the fencer)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(test_point)\n",
        "    display(boundary)\n",
        "\n",
        "  if test_point != 'None':\n",
        "    if test_point[0] > boundary[0] and test_point[0] < boundary[1] and test_point[1] > boundary[2] and test_point[1] < boundary[3]:\n",
        "      box_test = True\n",
        "    else:\n",
        "      box_test = False\n",
        "  else:\n",
        "    box_test = False\n",
        "\n",
        "  return (box_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPik4RGlWPYB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Engarde_Failure_Test\n",
        "def engarde_failure_test(bbox, bellguard_confidence, x_max, y_max, side):\n",
        "  # Tests for reasons the engarde positioning failed to detect a BellGuard\n",
        "\n",
        "  display(f'The {side} engarde position failed due to...')\n",
        "\n",
        "  if side == 'Left':\n",
        "    oppside = 'Right'\n",
        "    k = 0\n",
        "  else:\n",
        "    oppside = 'Left'\n",
        "    k = 1\n",
        "\n",
        "\n",
        "  for j in range(len(bbox)):\n",
        "    if bbox[j][1] < bellguard_confidence:\n",
        "      display(f'The confidence in the {side} bellguard is too low at {bellguard_confidence}.')\n",
        "    else: \n",
        "      pass\n",
        "    if side == 'Left':\n",
        "      if bbox[j][k] > x_max:\n",
        "        display(f'The {side} bellguard was too far {oppside} at {bbox[j][0]} while the maximum is {x_max}.')\n",
        "      else:\n",
        "        pass\n",
        "    else:\n",
        "      display(f'bbox at this point is: {bbox}. J is {j} and k is {k}.')\n",
        "      display(bbox[j])\n",
        "      display(bbox[j][k])\n",
        "      if bbox[j][k] < x_max:\n",
        "        display(f'The {side} bellguard was too far {oppside} at {bbox[j][0]} while the maximum is {x_max}.')\n",
        "    if bbox[j][k] > y_max:\n",
        "      display(f'The {side} bellguard was too low at {bbox[j][0]} while the maximum allowed is {y_max}.')\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzdGZ-f52Ucw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Torso_Failure_Test\n",
        "def torso_failure_test(bbox, capture_width, capture_height, y_average, Bell_Guard_Size_average, side, frame_count, min_torso_confidence):\n",
        "  # Tests for reasons the engarde positioning failed to detect a Torso\n",
        "  # Is tested at finding tracking boxes\n",
        "\n",
        "  display(f'The {side} Torso failed due to...') \n",
        "  for j in range(len(bbox)):\n",
        "    if bbox[j][1] > min_torso_confidence:\n",
        "      pass\n",
        "    else:\n",
        "      display(f'The confidence is of the box is too low at only {int(bbox[j][1]*100)}% at frame {frame_count}.')\n",
        "    if bbox[j][0][2] > y_average:\n",
        "      pass\n",
        "    else:\n",
        "      display(f'The Torso was not lower than the Bell Guard with a lower height of {bbox[j][0][2]} with a max value of {y_average} at frame {frame_count}.')\n",
        "    if bbox[j][0][2] < (y_average + 3*Bell_Guard_Size_average[1]):\n",
        "      pass\n",
        "    else:\n",
        "      display(f'The bottom of the torso box was too low at {bbox[j][0][2]} with a max value of {int(y_average + 3*Bell_Guard_Size_average[1])} at frame {frame_count}.')\n",
        "\n",
        "  display(f'y_average is {y_average}.')\n",
        "  display(f'Bell_Guard_Size_average[1] is {Bell_Guard_Size_average[1]}.')\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrP957nvDFvz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Torso_Position_Failure_Test\n",
        "def torso_position_failure_test(bbox, engarde_length, x_min_torso, x_max_torso, y_min_torso, y_max_torso, y_average, side, frame_count):\n",
        "  # Tests for reasons the engarde positioning failed to detect a Torso\n",
        "  # Is tested at torso positions\n",
        "\n",
        "  confidence = min_torso_confidence\n",
        "\n",
        "  display(f'Analyzing the Torso Position Failure at frame {frame_count} for the {side} side...')\n",
        "  count = 0\n",
        "  \n",
        "  for k in range(len(bbox)):\n",
        "\n",
        "    if bbox[k][2] == 3 and bbox[k][1] > confidence:\n",
        "      count = count + 1\n",
        "  display(f'There are {len(bbox)} ROIs, {count} of them are Torsos with greater than {confidence}%.')\n",
        "\n",
        "  for j in range(len(bbox)):\n",
        "    y_center = int((bbox[j][0][0] + bbox[j][0][2])/2)\n",
        "    x_center = int((bbox[j][0][1] + bbox[j][0][3])/2)\n",
        "    if bbox[j][2] == 3 and bbox[j][1] > confidence:\n",
        "      if x_center > x_min_torso:\n",
        "        pass\n",
        "      else:\n",
        "        display(f'The Torso center at {x_center} is to the Left of the Box side at {x_min_torso} at frame {frame_count}.')\n",
        "      if x_center < x_max_torso:\n",
        "        pass\n",
        "      else:\n",
        "        display(f'The Torso center at {x_center} is to the Right of the Box side at {x_max_torso} at frame {frame_count}.')\n",
        "      if y_center > y_min_torso:\n",
        "        pass\n",
        "      else:\n",
        "        display(f'The Torso center at {y_center} is Above the Box at {y_min_torso} at frame {frame_count}.')\n",
        "      if y_center < y_max_torso:\n",
        "        pass\n",
        "      else:\n",
        "        display(f'The Torso center at {y_center} is Below the Box at {y_max_torso} at frame {frame_count}.')\n",
        "      if bbox[j][0][2] > y_average:\n",
        "        pass\n",
        "      else:\n",
        "        display(f'The Torso center is Below the Bell Guard at frame {frame_count}.')\n",
        "      if bbox[j][2] == 3:\n",
        "        pass\n",
        "      else:\n",
        "        display(f'The Torso is not labelled as a Torso at frame {frame_count}.')\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrY_K5mZafoc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Engarde_Position\n",
        "def engarde_position(bbox, capture_width, capture_height, engarde_length, frame_count):\n",
        "  #Finds the initial positions to start tracking\n",
        "  #Format of bbox[frame][roi], ([y1,x1,y2,x2], percent certainty, type)\n",
        "\n",
        "  # Initializes the Bell Guard Positions\n",
        "  # Position format [x,y]\n",
        "  # Size format [[Width],[Height]]\n",
        "  Left_Position = []\n",
        "  Right_Position = []\n",
        "  Bell_Guard_Size = [[],[]]\n",
        "  Scoring_Box_Position = []\n",
        "  Scoring_Box_Size = [[],[]]\n",
        "  Left_Torso_Position = []\n",
        "  Left_Torso_Size = [[],[]]\n",
        "  Right_Torso_Position = []\n",
        "  Right_Torso_Size = [[],[]]\n",
        "\n",
        "  display(f'The bbox for the engarde capture at frame {frame_count} is:')\n",
        "  display(bbox)\n",
        "\n",
        "  #sum_of_boxes is used to average the Left (x,y)(0), Right (x,y)(1), ScoreBox (x,y)(2), Left_Torso (x,y)(3), Right_Torso(x,y)(4) values\n",
        "  sum_of_boxes = [[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]]\n",
        "\n",
        "  # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "  for j in range(len(bbox)):\n",
        "    # The percent confidence for each roi is [i][j][1]\n",
        "    # This uses the minimum value of the bbox (top-left) to determine Left, Right, Scorebox\n",
        "    # The Bellguards must be centered within the frame, classified as Bellguards with a minimum confidence and have the correct color saturation\n",
        "    # Adds values to the Left engarde box\n",
        "    if (bbox[j][1] > bellguard_confidence and bbox[j][0][1] < int(capture_width*2/5) and bbox[j][0][0] < int(capture_height*3/4) and bbox[j][0][0] > int(capture_height*1/4) and bbox[j][2] == 1):\n",
        "      test_result = saturation_test(bbox[j], frame_count)\n",
        "      if verbose == True:\n",
        "        display(f'The result of the saturation test for the Left Engarde Position is {test_result} at frame {frame_count}.')\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        # sum_of_boxes[0][0].append(bbox[j][0][1])\n",
        "        sum_of_boxes[0][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[0][1].append([bbox[j][0][0], bbox[j][1]])\n",
        "        #Appends x width value:\n",
        "        Bell_Guard_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Bell_Guard_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    #Adds values to the Right engarde box\n",
        "    elif (bbox[j][1] > bellguard_confidence and bbox[j][0][1] > int(capture_width*3/5) and bbox[j][0][0] < int(capture_height*3/4) and bbox[j][0][0] > int(capture_height*1/4) and bbox[j][2] == 1):\n",
        "      # test_result = color_tester(bbox[i][j], i)\n",
        "      test_result = saturation_test(bbox[j], frame_count)\n",
        "      if verbose == True:\n",
        "        display(f'The result of the saturation test for the Right Engarde Position is {test_result} at frame {frame_count}.')\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[1][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[1][1].append([bbox[j][0][0], bbox[j][1]])\n",
        "        #Appends x width value:\n",
        "        Bell_Guard_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Bell_Guard_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    #Adds values to the ScoreBox Position\n",
        "    elif (bbox[j][1] > 0.50 and bbox[j][0][1] > int(capture_width/3) and bbox[j][0][1] < int(capture_width*(2/3)) and bbox[j][2] == 2):\n",
        "      #Appends x value:\n",
        "      sum_of_boxes[2][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "      #Appends y value:\n",
        "      sum_of_boxes[2][1].append([bbox[j][0][0], bbox[j][1]])  \n",
        "      #Appends x width value:\n",
        "      Scoring_Box_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "      #Appends y width value:\n",
        "      Scoring_Box_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  try:\n",
        "    # Tests for cause of Left Engarde Position Failure\n",
        "    if len(sum_of_boxes[0][0]) == 0:\n",
        "      engarde_failure_test(bbox[j], bellguard_confidence, int(capture_width*2/5), int(capture_height*2/3), 'Left')\n",
        "    # Tests for cause of Right Engarde Position Failure\n",
        "    if len(sum_of_boxes[1][0]) == 0:\n",
        "      engarde_failure_test(bbox[j], bellguard_confidence, int(capture_width*3/5), int(capture_height*3/4), 'Right')\n",
        "  except:\n",
        "    display(f'There was an error in the engarde failure test and it was skipped.')\n",
        "\n",
        "  # Finds the center point\n",
        "  x_average_left = weight_average_list(sum_of_boxes[0][0])\n",
        "  y_average_left = weight_average_list(sum_of_boxes[0][1])\n",
        "  x_average_right = weight_average_list(sum_of_boxes[1][0])\n",
        "  y_average_right = weight_average_list(sum_of_boxes[1][1])\n",
        "  x_average_scorebox = weight_average_list(sum_of_boxes[2][0])\n",
        "  y_average_scorebox = weight_average_list(sum_of_boxes[2][1])\n",
        "\n",
        "  # Prevents a failure to detect the bellguard from failing to detect the torso\n",
        "  # If the bellguard is unusually high or low then it is set to the height of the opposing BellGuard\n",
        "  if (y_average_left < capture_height/5) or (y_average_left > capture_height*4/5):\n",
        "    if verbose == True:\n",
        "      display(f'The y_average_left was too high or low and was set to y_average_right.')\n",
        "    y_average_left = y_average_right\n",
        "  if (y_average_right < capture_height/5) or (y_average_right > capture_height*4/5):\n",
        "    if verbose == True:\n",
        "      display(f'The y_average_right was too high or low and was set to y_average_left.')\n",
        "    y_average_right = y_average_left\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average left position is ({x_average_left},{y_average_left}).')\n",
        "    display(f'The average right position is ({x_average_right},{y_average_right}).')\n",
        "\n",
        "  # Bell_Guard_Size_average [Width, Height]\n",
        "  Bell_Guard_Size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  Bell_Guard_Size_average.append(average_list(Bell_Guard_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  Bell_Guard_Size_average.append(average_list(Bell_Guard_Size[1]))\n",
        "\n",
        "  # Finds the Torso Position After the Bell_Guard Position because the Bell_Guard is used as a constraint\n",
        "  # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "  for j in range(len(bbox)):\n",
        "    # Adds values to the Left_Torso Position, similar requirements to Left guard\n",
        "    # Minimum Torso confidence, on the left half of the screen, bottom of the box is below the bellguard, but also above 3 three times the bellguard height and is labeled torso\n",
        "    if (bbox[j][1] > min_torso_confidence and bbox[j][0][1] < int(capture_width/2) and bbox[j][0][2] > y_average_left \\\n",
        "        and bbox[j][0][2] < (y_average_left + 3*Bell_Guard_Size_average[1]) and bbox[j][2] == 3):\n",
        "      \n",
        "      # Tests the Torso Color Saturation\n",
        "      test_result = saturation_test(bbox[j], frame_count)\n",
        "      if test_result == True:\n",
        "        # Appends x value:\n",
        "        sum_of_boxes[3][0].append(bbox[j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[3][1].append(bbox[j][0][0])\n",
        "        #Appends x width value:\n",
        "        Left_Torso_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Left_Torso_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The saturation test failed at frame {frame_count}.')\n",
        "        else:\n",
        "          pass\n",
        "    # Adds values to the Right_Torso Position, similar requirements to Right guard\n",
        "    elif (bbox[j][1] > min_torso_confidence and bbox[j][0][1] > int(capture_width/2) and \\\n",
        "          bbox[j][0][2] > (y_average_right) and bbox[j][0][2] < (y_average_right + 3*Bell_Guard_Size_average[1]) and bbox[j][2] == 3):\n",
        "      test_result = saturation_test(bbox[j], frame_count)\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[4][0].append(bbox[j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[4][1].append(bbox[j][0][0])\n",
        "        #Appends x width value:\n",
        "        Right_Torso_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Right_Torso_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The saturation test failed at frame {frame_count}.')\n",
        "        else:\n",
        "          pass\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  if len(sum_of_boxes[3][0]) == 0:\n",
        "    torso_failure_test(bbox, capture_width, capture_height, y_average_left, Bell_Guard_Size_average, 'Left', frame_count, min_torso_confidence)\n",
        "  if verbose == True:\n",
        "    display(f'Prior to torso failure test for right torso the y_average_left is {y_average_left}.')\n",
        "\n",
        "  if len(sum_of_boxes[4][0]) == 0:\n",
        "    torso_failure_test(bbox, capture_width, capture_height, y_average_right, Bell_Guard_Size_average, 'Right', frame_count, min_torso_confidence) \n",
        "  if verbose == True:\n",
        "    display(f'Prior to torso failure test for left torso the y_average_right is {y_average_right}.')\n",
        "\n",
        "  #Finds the top left corner then moves the average point to the center\n",
        "  x_average_left_torso = average_list(sum_of_boxes[3][0]) + average_list(Left_Torso_Size[0])/2\n",
        "  y_average_left_torso = average_list(sum_of_boxes[3][1]) + average_list(Left_Torso_Size[1])/2\n",
        "  x_average_right_torso = average_list(sum_of_boxes[4][0]) + average_list(Right_Torso_Size[0])/2\n",
        "  y_average_right_torso = average_list(sum_of_boxes[4][1]) + average_list(Right_Torso_Size[1])/2\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average left engarde position is:({x_average_left},{y_average_left})')\n",
        "    display(f'The average right engarde position is:({x_average_right},{y_average_right})')\n",
        "\n",
        "    display(f'The average left torso is:({int(x_average_left_torso)},{int(y_average_left_torso)})')\n",
        "    display(f'The average right torso is:({int(x_average_right_torso)},{int(y_average_right_torso)})')\n",
        "\n",
        "  # scoring_box_size_average [Width, Height]\n",
        "  scoring_box_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  scoring_box_size_average.append(average_list(Scoring_Box_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  scoring_box_size_average.append(average_list(Scoring_Box_Size[1]))\n",
        "\n",
        "  # left_torso_size_average [Width, Height]\n",
        "  left_torso_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  left_torso_size_average.append(average_list(Left_Torso_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  left_torso_size_average.append(average_list(Left_Torso_Size[1]))\n",
        "\n",
        "  # right_torso_size_average [Width, Height]\n",
        "  right_torso_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  right_torso_size_average.append(average_list(Right_Torso_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  right_torso_size_average.append(average_list(Right_Torso_Size[1]))\n",
        "\n",
        "  #Creates Padding for the EnGarde Tracking Box\n",
        "  engarde_box_padding = int(capture_width/15)\n",
        "  torso_padding = int(capture_width/20)\n",
        "\n",
        "  x_min_engardeL = int(x_average_left - engarde_box_padding)\n",
        "  x_max_engardeL = int(x_average_left + engarde_box_padding)\n",
        "  y_min_engardeL = int(y_average_left - engarde_box_padding)\n",
        "  y_max_engardeL = int(y_average_left + engarde_box_padding)\n",
        "\n",
        "  x_min_engardeR = int(x_average_right - engarde_box_padding)\n",
        "  x_max_engardeR = int(x_average_right + engarde_box_padding)\n",
        "  y_min_engardeR = int(y_average_right - engarde_box_padding)\n",
        "  y_max_engardeR = int(y_average_right + engarde_box_padding)\n",
        "\n",
        "  x_min_engardeScore = int(x_average_scorebox - engarde_box_padding)\n",
        "  x_max_engardeScore = int(x_average_scorebox + engarde_box_padding)\n",
        "  y_min_engardeScore = int(y_average_scorebox - engarde_box_padding)\n",
        "  y_max_engardeScore = int(y_average_scorebox + engarde_box_padding)\n",
        "\n",
        "  x_min_torsoL = int(x_average_left_torso - torso_padding)\n",
        "  x_max_torsoL = int(x_average_left_torso + torso_padding)\n",
        "  y_min_torsoL = int(y_average_left_torso - torso_padding*3/2)\n",
        "  y_max_torsoL = int(y_average_left_torso + torso_padding*3/2)\n",
        "\n",
        "  x_min_torsoR = int(x_average_right_torso - torso_padding)\n",
        "  x_max_torsoR = int(x_average_right_torso + torso_padding)\n",
        "  y_min_torsoR = int(y_average_right_torso - torso_padding*3/2)\n",
        "  y_max_torsoR = int(y_average_right_torso + torso_padding*3/2)\n",
        "\n",
        "  #Iterates through the first engarde_length frames and checks if there are rois in the expected engarde position\n",
        "  for j in range(len(bbox)):\n",
        "    y_center = int((bbox[j][0][0] + bbox[j][0][2])/2)\n",
        "    x_center = int((bbox[j][0][1] + bbox[j][0][3])/2)\n",
        "    # Checks for rois in the Left Engarde Position\n",
        "    if (x_center > x_min_engardeL and x_center < x_max_engardeL and y_center > y_min_engardeL and y_center < y_max_engardeL and bbox[j][2] == 1):\n",
        "      # display(f'The roi is in the left en garde position')\n",
        "      Left_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Right Engarde Position\n",
        "    if (x_center > x_min_engardeR and x_center < x_max_engardeR and y_center > y_min_engardeR and y_center < y_max_engardeR and bbox[j][2] == 1):\n",
        "      # display(f'The roi is in the right en garde position')\n",
        "      Right_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Scoring Box Position\n",
        "    if (x_center > x_min_engardeScore and x_center < x_max_engardeScore and y_center > y_min_engardeScore and y_center < y_max_engardeScore and bbox[j][2] == 2):\n",
        "      Scoring_Box_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Left Torso Position\n",
        "    if (x_center > x_min_torsoL and x_center < x_max_torsoL and y_center > y_min_torsoL and y_center < y_max_torsoL and bbox[j][0][2] > y_average_left and bbox[j][2] == 3):\n",
        "      Left_Torso_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Right Torso Position \n",
        "    if (x_center > x_min_torsoR and x_center < x_max_torsoR and y_center > y_min_torsoR and y_center < y_max_torsoR and bbox[j][0][2] > y_average_right and bbox[j][2] == 3):\n",
        "      Right_Torso_Position.append([x_center, y_center])\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp = [[],[],[]]\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(x_min_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(x_max_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(y_min_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(y_max_engardeL)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(x_min_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(x_max_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(y_min_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(y_max_engardeR)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(x_min_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(x_max_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(y_min_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(y_max_engardeScore)\n",
        "\n",
        "    Tracking_Bounding_Boxes = Tracking_Bounding_Boxes_Temp\n",
        "\n",
        "  # Tests for why a Torso Position is not Found\n",
        "  if (len(Left_Torso_Position) == 0):\n",
        "    torso_position_failure_test(bbox, engarde_length, x_min_torsoL, x_max_torsoL, y_min_torsoL, y_max_torsoL, y_average_left, 'Left', frame_count)    \n",
        "  if (len(Right_Torso_Position) == 0):\n",
        "    torso_position_failure_test(bbox, engarde_length, x_min_torsoR, x_max_torsoR, y_min_torsoR, y_max_torsoR, y_average_right, 'Right', frame_count)\n",
        "\n",
        "\n",
        "  # Averages the Left and Right x,y positions for engarde\n",
        "  # Left Bell Guard engarde position\n",
        "  if verbose == True:\n",
        "    display(f'The length of the built Tracking Bounding Boxes is {len(Tracking_Bounding_Boxes[0])}.')\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Left_Position) > 0:\n",
        "    for i in range(len(Left_Position)):\n",
        "      x = x + Left_Position[i][0]\n",
        "      y = y + Left_Position[i][1]\n",
        "    x = int(x/(len(Left_Position)))\n",
        "    y = int(y/(len(Left_Position)))\n",
        "    Left_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Left_Position at Engarde is:')\n",
        "    display(Left_Position)\n",
        "\n",
        "  # Right Bell Guard engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Right_Position) > 0:\n",
        "    for i in range(len(Right_Position)):\n",
        "      x = x + Right_Position[i][0]\n",
        "      y = y + Right_Position[i][1]\n",
        "    x = int(x/(len(Right_Position)))\n",
        "    y = int(y/(len(Right_Position)))\n",
        "    Right_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Right_Position at Engarde is:')\n",
        "    display(Right_Position)\n",
        "\n",
        "  # Scoring_Box engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Scoring_Box_Position) > 0:\n",
        "    for i in range(len(Scoring_Box_Position)):\n",
        "      x = x + Scoring_Box_Position[i][0]\n",
        "      y = y + Scoring_Box_Position[i][1]\n",
        "    x = int(x/(len(Scoring_Box_Position)))\n",
        "    y = int(y/(len(Scoring_Box_Position)))\n",
        "    Scoring_Box_Position = [x,y]\n",
        "\n",
        "  if Scoring_Box_Position == [0,0]:\n",
        "    Tracking_Bounding_Boxes_Temp[2] = [0,0,0,0]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Scoring_Box_Position at Engarde is:')\n",
        "    display(Scoring_Box_Position)\n",
        "\n",
        "  # Left_Torso engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Left_Torso_Position) > 0:\n",
        "    for i in range(len(Left_Torso_Position)):\n",
        "      x = x + Left_Torso_Position[i][0]\n",
        "      y = y + Left_Torso_Position[i][1]\n",
        "    x = int(x/(len(Left_Torso_Position)))\n",
        "    y = int(y/(len(Left_Torso_Position)))\n",
        "    Left_Torso_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Left_Torso_Position at Engarde is:')\n",
        "    display(Left_Torso_Position)\n",
        "\n",
        "  # Right_Torso engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Right_Torso_Position) > 0:\n",
        "    for i in range(len(Right_Torso_Position)):\n",
        "      x = x + Right_Torso_Position[i][0]\n",
        "      y = y + Right_Torso_Position[i][1]\n",
        "    x = int(x/(len(Right_Torso_Position)))\n",
        "    y = int(y/(len(Right_Torso_Position)))\n",
        "    Right_Torso_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Right_Torso_Position at Engarde is:')\n",
        "    display(Right_Torso_Position)\n",
        "\n",
        "  return (Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Tracking_Bounding_Boxes, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0ssV1qoSLh",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Draw_Bell_Guard_Position\n",
        "def draw_Bell_Guard_Position(Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Left_Torso_Position, Right_Torso_Position, frame_count, Tracking_Bounding_Boxes, video_filename, capture_width, capture_height, engarde_length, keypoints, score_box_empty, camera_steady, camera_motion_threshold):\n",
        "  #Adds an overlay on the image to visualize the location of tracked objects\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/'\n",
        "  capture = cv2.VideoCapture(os.path.join(path, video_filename))\n",
        "\n",
        "  capture.set(cv2.CAP_PROP_FRAME_WIDTH, capture_width)\n",
        "  capture.set(cv2.CAP_PROP_FRAME_HEIGHT, capture_height)\n",
        "\n",
        "  #Color format is [B,G,R]\n",
        "  left_light_color_default = [[],[],[]]\n",
        "  right_light_color_default = [[],[],[]]\n",
        "  left_light_color = []\n",
        "  right_light_color = []\n",
        "\n",
        "  # Creates a list of Files from a Directory\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "  path_orig = r'/content/Mask_RCNN/videos/original/'\n",
        "  files = [i for i in os.listdir(path)]\n",
        "  # Sorts the Files after cropping '.jpg'\n",
        "  files.sort(key=lambda x: int(x[:-4]))\n",
        "\n",
        "  left_light_comparison, right_light_comparison, default_color = [], [], []\n",
        "\n",
        "  for i, file in enumerate(files):\n",
        "    # Reads the image\n",
        "    # name = os.path.join(path_orig, file)\n",
        "    name = os.path.join(path, file)\n",
        "    img = cv2.imread(name)\n",
        "\n",
        "    # OpenCV uses Blue, Green, Red order\n",
        "    # Light_Color is of the format [[[B0],[G0],[R0]],[[B1],[G1],[R1]],[[B2],[G2],[R2]],...]\n",
        "    \n",
        "    if i <= engarde_length:\n",
        "      if scoring_box_size_average == [0,0]:\n",
        "        scoring_box_size_average = [int(capture_width/5), int(capture_height/5)]\n",
        "      if verbose == True:\n",
        "        display(f'The average scoring box size is {scoring_box_size_average}.')\n",
        "      # Uses a comparison of frames and scoring box position to determine the light off colors\n",
        "      [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = scoring_box_lights(img, Scoring_Box_Position[i], scoring_box_size_average, [], i, score_box_empty)\n",
        "      left_light_comparison.append(left_light_comparison_temp)\n",
        "      right_light_comparison.append(right_light_comparison_temp)\n",
        "      default_color.append(defualt_color_temp)\n",
        "      # Averages the Default Color on the Last iteration\n",
        "      if i == engarde_length:\n",
        "        b_temp = int(sum(default_color[0])/len(default_color[0]))\n",
        "        g_temp = int(sum(default_color[1])/len(default_color[1]))\n",
        "        r_temp = int(sum(default_color[2])/len(default_color[2]))\n",
        "        default_color = [b_temp,g_temp,r_temp]\n",
        "    elif i > engarde_length:\n",
        "      try:\n",
        "        [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = scoring_box_lights(img, Scoring_Box_Position[i], scoring_box_size_average, default_color, i, score_box_empty)\n",
        "      except:\n",
        "        display(f'Light Comparison Failed due to Error at frame {i}.')\n",
        "        [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = [0,0,[]]\n",
        "      left_light_comparison.append(left_light_comparison_temp)\n",
        "      right_light_comparison.append(right_light_comparison_temp)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'Frame Count is {frame_count}.')\n",
        "\n",
        "    #Creates the dots on the Bell Guards\n",
        "    frame = cv2.circle(img, (Left_Position[i][0], Left_Position[i][1]), 4, (255, 0, 0), -1)\n",
        "    frame = cv2.circle(frame, (Right_Position[i][0], Right_Position[i][1]), 4, (0, 255, 0), -1)\n",
        "    frame = cv2.circle(frame, (Scoring_Box_Position[i][0], Scoring_Box_Position[i][1]), 4, (255, 255, 0), -1)\n",
        "    frame = cv2.circle(frame, (Left_Torso_Position[i][0], Left_Torso_Position[i][1]), 4, (0, 255, 0), -1)\n",
        "    frame = cv2.circle(frame, (Right_Torso_Position[i][0], Right_Torso_Position[i][1]), 4, (255, 255, 0), -1)\n",
        "\n",
        "    # Adds Frame Number to the Image\n",
        "    text = 'Frame' + str(i)\n",
        "    frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*1/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, )\n",
        "    # Adds if the Camera Motion is detected by difference images\n",
        "    if camera_steady[i] > camera_motion_threshold:\n",
        "      text = 'Camera'\n",
        "      frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*3/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4, )\n",
        "      text = 'Motion'\n",
        "      frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*4/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4, )\n",
        "    \n",
        "    # Sets BellGuard Position Colors\n",
        "    left_color = (255, 0, 0)\n",
        "    right_color = (0, 255, 0)\n",
        "\n",
        "    #Creates the Tracking Boxes\n",
        "    frame = cv2.putText(frame, 'Tracking Box', (Tracking_Bounding_Boxes[i][0][0], Tracking_Bounding_Boxes[i][0][2]), cv2.FONT_HERSHEY_COMPLEX, 0.7, left_color, 2)\n",
        "    frame = cv2.rectangle(frame, (Tracking_Bounding_Boxes[i][0][0], Tracking_Bounding_Boxes[i][0][2]),(Tracking_Bounding_Boxes[i][0][1], Tracking_Bounding_Boxes[i][0][3]),left_color, 2)\n",
        "    frame = cv2.putText(frame, 'Tracking Box', (Tracking_Bounding_Boxes[i][1][0], Tracking_Bounding_Boxes[i][1][2]), cv2.FONT_HERSHEY_COMPLEX, 0.7, right_color, 2)\n",
        "    frame = cv2.rectangle(frame, (Tracking_Bounding_Boxes[i][1][0], Tracking_Bounding_Boxes[i][1][2]),(Tracking_Bounding_Boxes[i][1][1], Tracking_Bounding_Boxes[i][1][3]),right_color, 2)\n",
        "\n",
        "    [frame, none] = overlay_keypoints(frame, keypoints[i][0], keypoints[i][1], True)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The Tracking Box for the Left Fencer at frame {i} is:')\n",
        "      display(f'{Tracking_Bounding_Boxes[i][0][0]},{Tracking_Bounding_Boxes[i][0][2]}')\n",
        "      display(f'The Tracking Box for the Right Fencer at frame {i} is:')\n",
        "      display(f'{Tracking_Bounding_Boxes[i][1][0]},{Tracking_Bounding_Boxes[i][1][2]}')\n",
        "\n",
        "    #Saves the image frame overwriting the original image\n",
        "    name = os.path.join(path, file)\n",
        "    cv2.imwrite(name, frame)\n",
        "    display(f'The Draw Bell Guard frame {i} is being saved at {name}.')\n",
        "\n",
        "  #Releases capture so that other files can be used\n",
        "  capture.release()\n",
        "\n",
        "  return (left_light_comparison, right_light_comparison)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZNUlGxZAMYv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Mask_Image\n",
        "def mask_image(frame, width, height, masking_box):\n",
        "  # Used to Mask parts of the image that are not of interest\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The masking box is:')\n",
        "    display(masking_box)\n",
        "\n",
        "  #Create the Mask\n",
        "  mask = np.zeros((height, width, 3), dtype = np.uint8);\n",
        "  for i in range(len(masking_box)):\n",
        "    mask = cv2.rectangle(mask, (masking_box[i][0], masking_box[i][2]) ,(masking_box[i][1], masking_box[i][3]), (255,255,255), -1)\n",
        "\n",
        "  #Applies the mask to Frame\n",
        "  frame = cv2.bitwise_and(mask, frame)\n",
        "\n",
        "  return (frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkcK4BxFc-lJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create_Representative_Image\n",
        "def create_representative_image(clip_vector, capture_width, capture_height):\n",
        "  # Allows for an overlay that represents the bellguard horizontal motion and box lights\n",
        "\n",
        "  #Creates a Folder to save the images and removes previous version\n",
        "  os.chdir('/content/Mask_RCNN/videos')\n",
        "  # !mkdir save_white_dot\n",
        "  # os.mkdir('save_white_dot')\n",
        "  # !rm -r /content/Mask_RCNN/videos/save_white_dot\n",
        "  # os.rmdir('/content/Mask_RCNN/videos/save_white_dot')\n",
        "  # Removes and Recreates the Save_White_Dot to ensure the directory is empty\n",
        "  try:\n",
        "    shutil.rmtree('save_white_dot') \n",
        "  except:\n",
        "    display(f'ERROR removing the Save_White_Dot folder.')\n",
        "  # os.rmdir('/save_white_dot')\n",
        "  # !mkdir save_white_dot\n",
        "  os.mkdir('save_white_dot')\n",
        "\n",
        "  rect_size = int(capture_width/40)\n",
        "\n",
        "  #Defines the File Path\n",
        "  path = r'/content/Mask_RCNN/videos/save_white_dot/'\n",
        "  \n",
        "  for i in range(len(clip_vector)):\n",
        "    img = np.zeros((capture_height,capture_width,3), np.uint8)\n",
        "\n",
        "    #Creates the Left Bell_Guard\n",
        "    img = cv2.circle(img, (clip_vector[i][0], int(capture_height/2)), 20, (118, 37, 217), -1)\n",
        "    #Creates the Right Bell_Guard\n",
        "    img = cv2.circle(img, (clip_vector[i][1], int(capture_height/2)), 20, (157, 212, 19), -1)\n",
        "\n",
        "    if (clip_vector[i][2] == 1):\n",
        "      #Creates the Left Score Light\n",
        "      img = cv2.rectangle(img, (rect_size, rect_size), (rect_size*5, rect_size*3), (0, 0, 255), -1)\n",
        "    if (clip_vector[i][3] == 1):\n",
        "      #Creates the Right Score Light\n",
        "      img = cv2.rectangle(img, (capture_width - rect_size, rect_size), (capture_width - rect_size*5, rect_size*3), (0, 255, 0), -1)\n",
        "\n",
        "    name = str(i) + '.jpg'\n",
        "    name = os.path.join(path, name)\n",
        "\n",
        "    cv2.imwrite(name, img)\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCrXhYq2f3sb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create_Overlay_Image\n",
        "def create_overlay_image(frame_count):\n",
        "  # Allows for an overlay that represents the bellguard horizontal motion and box lights\n",
        "\n",
        "  #Creates a Folder to save the images and removes previous version\n",
        "  os.chdir('/content/Mask_RCNN/videos/')\n",
        "  # !rm -r /content/Mask_RCNN/videos/overlay\n",
        "  # Attempts to remove the Overlay folder and recreate it to ensure that it is empty\n",
        "  try:\n",
        "    shutil.rmtree('overlay')\n",
        "  except:\n",
        "    display(f'ERROR removing the Overlay folder.')\n",
        "  # !mkdir overlay\n",
        "  os.mkdir('overlay')\n",
        "\n",
        "\n",
        "  #Defines the File Path\n",
        "  path = r'/content/Mask_RCNN/videos/overlay/'\n",
        "  path_background = r'/content/Mask_RCNN/videos/save/'\n",
        "  path_overlay = r'/content/Mask_RCNN/videos/save_white_dot/'\n",
        "  for i in range(frame_count):\n",
        "    background_name = str(i) + '.jpg'\n",
        "    background_name = os.path.join(path_background, background_name)\n",
        "\n",
        "    overlay_name = str(i) + '.jpg'\n",
        "    overlay_name = os.path.join(path_overlay, overlay_name)\n",
        "    \n",
        "    background = cv2.imread(background_name)\n",
        "    overlay = cv2.imread(overlay_name)\n",
        "\n",
        "    added_image = cv2.addWeighted(background,0.8,overlay,1.0,0)\n",
        "\n",
        "    combined_name = str(i) + '.jpg'\n",
        "    combined_name = os.path.join(path, combined_name)\n",
        "\n",
        "    cv2.imwrite(combined_name, added_image)\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RQ1thQAPxbc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Light_Color_Comparison\n",
        "def light_color_comparison(light_color, light_color_default, color):\n",
        "  # Deterines if a light turned on based on a default color, an input color and expected color\n",
        "\n",
        "  light_comparison = []\n",
        "  # A high max distance is less sensitive and a lower max distance is more sensitive\n",
        "  max_distance_total = 180\n",
        "  max_distance_specific_color = 90\n",
        "\n",
        "  if color == 'Red':\n",
        "    color_specific = 2\n",
        "  elif color == 'Green':\n",
        "    color_specific = 1\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Color being analyzed is {color}.')\n",
        "    display(f'The default color is:')\n",
        "    display(light_color_default)\n",
        "    display(f'With the specific color being {light_color_default[color_specific]}')\n",
        "    display(f'The max distance total is {max_distance_total}.')\n",
        "    display(f'The max distance for a specific color is {max_distance_specific_color}.')\n",
        "\n",
        "  #i cycles through each light value corresponding to each frame\n",
        "  for i in range(len(light_color)):\n",
        "    distance = 0\n",
        "    for j in range(3):\n",
        "      distance = distance + (light_color[i][j] - light_color_default[j])**2\n",
        "\n",
        "    distance_specific_color = abs(light_color[i][color_specific] - light_color_default[color_specific])\n",
        "\n",
        "    distance = int((distance)**(0.5))\n",
        "    if vebose == True:\n",
        "      display(f'The distance is {distance} and the color specific distance is {distance_specific_color} for frame {i}.')\n",
        "    #0 is no color change from the default color)\n",
        "    if (distance > max_distance_total and distance_specific_color > max_distance_specific_color):\n",
        "      light_comparison.append(1)\n",
        "      if verbose == True:\n",
        "        display(f'The light is ON.')\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      light_comparison.append(0)\n",
        "      if verbose == True:\n",
        "        display(f'The light is OFF.')\n",
        "\n",
        "  return (light_comparison)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU0k7BCzVbOD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Clip_Vector_Generator\n",
        "def clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous, width):\n",
        "  #Compiles the clip_vector that is used for the action analysis\n",
        "\n",
        "  # Allows for the assumption that both lights are on if the positions are close to each other.\n",
        "  # Useful if there is difficulty detecting the scoring box.\n",
        "  close_bellguards = False\n",
        "  # Once lights turn on it is assumed the lights stay on for the rest of the action\n",
        "  light_assumption = False\n",
        "\n",
        "  if len(Left_Position) != len(Right_Position):\n",
        "    display(f'The Left and Right Positions do not match up')\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  # This is either [] or the Previously saved Clip_Vector\n",
        "  clip_vector = clip_vector_previous\n",
        "\n",
        "  for i in range(len(Left_Position)):  \n",
        "    # Checks the lights should be assumed on if they are not already\n",
        "    # Determines if the bellguards are close to each other\n",
        "    if (abs(Left_Position[i][0] - Right_Position[i][0]) < width*.050) and (light_assumption == False):\n",
        "      close_bellguards = True\n",
        "\n",
        "    # Adjusts the clip vector to reflect scoring box light assumptions\n",
        "    clip_vector_temp = [[],[],[],[]]\n",
        "    clip_vector_temp[0] = Left_Position[i][0]\n",
        "    clip_vector_temp[1] = Right_Position[i][0]\n",
        "    if (assume_lights == True and close_bellguards == True) or light_assumption == True:\n",
        "      clip_vector_temp[2] = 1\n",
        "      clip_vector_temp[3] = 1\n",
        "      light_assumption = True\n",
        "    else:\n",
        "      if ignore_box_lights == True:\n",
        "        clip_vector_temp[2] = 0\n",
        "        clip_vector_temp[3] = 0\n",
        "      else:\n",
        "        clip_vector_temp[2] = left_light_comparison[i]\n",
        "        clip_vector_temp[3] = right_light_comparison[i]\n",
        "\n",
        "    clip_vector.append(clip_vector_temp)\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zNZqZbF3AKY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Clip_Vector_Np_Save\n",
        "def clip_vector_np_save(clip_call, file_number, clip_vector):\n",
        "  # Saves the clip vector for future use\n",
        "  # Clip_Call Left_Touch, Right_Touch, Simul\n",
        "\n",
        "  # Generates the clip_vector speed based on the clip_vector\n",
        "  clip_vector_speed = []\n",
        "  for i in range(len(clip_vector)-1):\n",
        "    clip_vector_speed.append([])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][0]-clip_vector[i][0])\n",
        "    # Reverses the Right Fencers position so that positive is towards the opponent\n",
        "    clip_vector_speed[i].append(clip_vector[i][1]-clip_vector[i+1][1])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][2])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][3])\n",
        "\n",
        "  # Generates the clip_vector acceleration based on the clip_vector\n",
        "  clip_vector_acceleration = []\n",
        "  for i in range(len(clip_vector_speed)-1):\n",
        "    clip_vector_acceleration.append([])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][0]-clip_vector[i][0])\n",
        "    # Reverses the Right Fencers position so that positive is towards the opponent\n",
        "    clip_vector_acceleration[i].append(clip_vector[i][1]-clip_vector[i+1][1])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][2])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][3])\n",
        "\n",
        "  path = '/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "\n",
        "  # Saves the clip_vector as a numpy array\n",
        "  clip_vector_np = np.asarray(clip_vector)\n",
        "  name = os.path.join(path, clip_call)\n",
        "  name_2 = clip_call + '_Vector_Clips'\n",
        "  name = os.path.join(name, name_2)\n",
        "  if verbose == True:\n",
        "    display(f'The name of the path for clip vectors to be saved is:')\n",
        "    display(name)\n",
        "\n",
        "  # Changes the directory to the sub folder of the fenncing clip\n",
        "  # %cd $name\n",
        "  os.chdir(name)\n",
        "\n",
        "  clip_vector_np_name = 'clip_vector_np' + str(file_number) + '.csv'\n",
        "  # Saves to the current directory\n",
        "  np.savetxt(clip_vector_np_name, clip_vector_np, delimiter=',')\n",
        "\n",
        "  # Saves the clip_vector_speed\n",
        "  clip_vector_speed_np = np.asarray(clip_vector_speed)\n",
        "  name = os.path.join(path, clip_call)\n",
        "  name_2 = clip_call + '_Vector_Clips_Speed'\n",
        "  name = os.path.join(name, name_2)\n",
        "\n",
        "  # Changes the directory to the sub folder for the speed fencing clip\n",
        "  # %cd $name\n",
        "  os.chdir(name)\n",
        "\n",
        "  clip_vector_speed_np_name = 'clip_vector_speed_np' + str(file_number) + '.csv'\n",
        "  # Saves to the current directory\n",
        "  np.savetxt(clip_vector_speed_np_name, clip_vector_speed_np, delimiter=',')\n",
        "\n",
        "  # Saves the clip_vector_acceleration\n",
        "  clip_vector_acceleration_np = np.asarray(clip_vector_acceleration)\n",
        "  name = os.path.join(path, clip_call)\n",
        "  name_2 = clip_call + '_Vector_Clips_Acceleration'\n",
        "  name = os.path.join(name, name_2)\n",
        "\n",
        "  # Changes the directory to the sub folder for the acceleration fencning clip\n",
        "  # %cd $name\n",
        "  os.chdir(name)\n",
        "\n",
        "  clip_vector_acceleration_np_name = 'clip_vector_acceleration_np' + str(file_number) + '.csv'\n",
        "  #Saves to the current directory\n",
        "  np.savetxt(clip_vector_acceleration_np_name, clip_vector_acceleration_np, delimiter=',')\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1WK5HCkX_eX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Left_Right_Test\n",
        "def Left_Right_Test(Left_Position, Right_Position):\n",
        "  # Requires that the Left and Right BellGuards be on the Left and Right sides respectively\n",
        "\n",
        "  #Left_Position is chosen arbitrarily for length\n",
        "  for i in range(len(Left_Position)):\n",
        "    if Left_Position[i][0] > Right_Position[i][0]:\n",
        "      display(f'The Left and Right were swapped on frame {i} and are now corrected.')\n",
        "      position_temp = Left_Position[i]\n",
        "      Left_Position[i] = Right_Position[i]\n",
        "      Right_Position[i] = position_temp\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return (Left_Position, Right_Position)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHKEKonnjipI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Camera_Motion_Adjustment\n",
        "def camera_motion_adjustment(Position, Score_Box_Position):\n",
        "  # Takes a Position as an input and adjusts the position to compensate for camera motion\n",
        "  # Uses solely the x position of the scoring box to calculate motion\n",
        "  # Ignores the change in angle as the camera is rotated\n",
        "  # This is only used when it is assumed that the Scoring Box is well detected and tracked\n",
        "\n",
        "  Score_Box_Position_Temp = []\n",
        "  #Converts Scoring Box Positions to solely x value\n",
        "  #Scoring Box Position is of the format [x0,x1,x2...]\n",
        "  for i in range(len(Score_Box_Position)):\n",
        "    Score_Box_Position_Temp.append(Score_Box_Position[i][0])\n",
        "\n",
        "  for j in range(len(Position)):\n",
        "    score_box_delta = Score_Box_Position_Temp[j] - Score_Box_Position_Temp[0]\n",
        "    Position[j][0] = Position[j][0] - score_box_delta\n",
        "\n",
        "  return (Position)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAKPWXQz0R4l",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Position_Down_Scale\n",
        "def position_down_scale(Position1, Position2, capture_width, capture_height):\n",
        "  # Scales the Position Down to the Capture Width in the x axis if required for visualization convenience\n",
        "  # Does not alter the Clip Vector Data\n",
        "  \n",
        "  position_temp = []\n",
        "\n",
        "  for i in range(len(Position1)):\n",
        "    position_temp.append(Position1[i][0])\n",
        "\n",
        "  for j in range(len(Position2)):\n",
        "    position_temp.append(Position2[j][0])\n",
        "\n",
        "  min_x_position = min(position_temp)\n",
        "  max_x_position = max(position_temp)\n",
        "\n",
        "  if min_x_position < 0:\n",
        "    #Shifts the bellguards to the right for the camera moving to the left\n",
        "    for i in range(len(Position1)):\n",
        "      Position1[i][0] = int(Position1[i][0] - min_x_position)\n",
        "\n",
        "    for j in range(len(Position2)):\n",
        "      Position2[j][0] = int(Position2[j][0] - min_x_position)\n",
        "\n",
        "  # Absolute Pixel\n",
        "  if max_x_position > capture_width:\n",
        "    #Scales the max x position if greater than the screen\n",
        "    for i in range(len(Position1)):\n",
        "      Position1[i][0] = int(Position1[i][0] * capture_width / max_x_position)\n",
        "\n",
        "    for j in range(len(Position2)):\n",
        "      Position2[j][0] = int(Position2[j][0] * capture_width / max_x_position)\n",
        "\n",
        "  return (Position1, Position2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbuXubXE4XRf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Random_Colors\n",
        "def random_colors(N):\n",
        "    np.random.seed(1)\n",
        "    colors = [tuple(255 * np.random.rand(3)) for _ in range(N)]\n",
        "    return colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFVyAiia4ZcB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Apply_Mask\n",
        "def apply_mask(image, mask, color, alpha=0.5):\n",
        "    \"\"\"apply mask to image\"\"\"\n",
        "    for n, c in enumerate(color):\n",
        "        image[:, :, n] = np.where(\n",
        "            mask == 1,\n",
        "            image[:, :, n] * (1 - alpha) + alpha * c,\n",
        "            image[:, :, n]\n",
        "        )\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urekTB1e4bf4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Display_Instances\n",
        "def display_instances(image, boxes, masks, ids, names, scores, file_name):\n",
        "    \"\"\"\n",
        "        take the image and results and apply the mask, box, and Label\n",
        "    \"\"\"\n",
        "    n_instances = boxes.shape[0]\n",
        "    colors = random_colors(n_instances)\n",
        "\n",
        "    if not n_instances:\n",
        "        print('NO INSTANCES TO DISPLAY')\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        if not np.any(boxes[i]):\n",
        "            continue\n",
        "\n",
        "        y1, x1, y2, x2 = boxes[i]\n",
        "        # label = boxes[i][4]\n",
        "        label = names[ids[i]]\n",
        "        score = scores[i] if scores is not None else None\n",
        "        caption = '{} {:.2f}'.format(label, score) if score else label\n",
        "        mask = masks[:, :, i]\n",
        "        # display(f'The mask is: {mask}')\n",
        "        # image = apply_mask(image, mask, color)\n",
        "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "        image = cv2.putText(\n",
        "            image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2\n",
        "        )\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKAQZNzFfoWM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Save_Clip_Progress\n",
        "def save_clip_progress(bbox, frame_count, capture_width, capture_height, clip_vector_previous):\n",
        "\n",
        "  # Counts the files in the directory '/content/Mask_RCNN/videos/save/'\n",
        "\n",
        "  # %cd /content/Mask_RCNN/videos/save/\n",
        "  os.chdir('/content/Mask_RCNN/videos/save/')\n",
        "  number_of_stored_frames = len(os.listdir())\n",
        "  display(f'The number of stored frames in the save folder is {number_of_stored_frames}.')\n",
        "\n",
        "  if number_of_stored_frames > 500:\n",
        "    #Passes the Bounding Boxes to Determine position of items of interest\n",
        "    [Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Tracking_Bounding_Boxes, \\\n",
        "     Left_Torso_Position, Right_Torso_Position] = Bell_Guard_Position_Finding(bbox, capture_width, capture_height)\n",
        "\n",
        "    #Draws the Boxes on the image frame and determines scoring lights turned on\n",
        "    [left_light_comparison, right_light_comparison] = draw_Bell_Guard_Position(Left_Position, Right_Position, \\\n",
        "      Scoring_Box_Position, scoring_box_size_average, Left_Torso_Position, Right_Torso_Position, frame_count, \\\n",
        "      Tracking_Bounding_Boxes, video_filename, capture_width, capture_height, engarde_length)\n",
        "\n",
        "    #Adjusts the Bellguard Position Based on the Camera motion as determined by the Score_Box Position\n",
        "    Left_Position = camera_motion_adjustment(Left_Position, Scoring_Box_Position)\n",
        "    Right_Position = camera_motion_adjustment(Right_Position, Scoring_Box_Position)\n",
        "\n",
        "    #Adjusts Left and Right Position for convenient visualization\n",
        "    [Left_Position, Right_Position] = position_down_scale(Left_Position, Right_Position, capture_width, capture_height)\n",
        "\n",
        "    #Creates a vector representing the clip, format [left_x, right_x, left_lights, right_lights]\n",
        "    clip_vector = clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous)\n",
        "\n",
        "    # clip_vector = smooth_clip_vector(clip_vector, engarde_length)\n",
        "\n",
        "    clip_call = 'Temp_Clip_Vector'\n",
        "    file_number = '1'\n",
        "\n",
        "    #Saves the Clip, Speed and Acceleration Vectors\n",
        "    clip_vector_np_save(clip_call, file_number, clip_vector)\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  return()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UhmJ-_-kyG3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Smooth_Clip_Vector\n",
        "def smooth_clip_vector(clip_vector, engarde_length):\n",
        "  # Allows for smoothing the clip_vector\n",
        "\n",
        "  a = []\n",
        "  b = []\n",
        "  for i in range(engarde_length, len(clip_vector)):\n",
        "    a.append(clip_vector[i][0])\n",
        "    b.append(clip_vector[i][1])\n",
        "\n",
        "  x = np.linspace(engarde_length,len(clip_vector), len(clip_vector) - engarde_length)\n",
        "\n",
        "  # sos = signal.ellip(13, 0.009, 80, 0.05, output='sos')\n",
        "  # yhata = signal.sosfilt(sos, a)\n",
        "  if len(a)%2 == 1:\n",
        "    yhata = signal.savgol_filter(a, len(a), 11)\n",
        "    yhatb = signal.savgol_filter(b, len(b), 11)\n",
        "  else:\n",
        "    yhata = signal.savgol_filter(a, len(a) - 1, 11)\n",
        "    yhatb = signal.savgol_filter(b, len(b) - 1, 11)    \n",
        "\n",
        "  # plt.plot(x,a, color='black')\n",
        "  # plt.plot(x,yhata, color='red')\n",
        "  plt.plot(x,b, color='black')\n",
        "  plt.plot(x,yhatb, color='blue')\n",
        "  plt.show()\n",
        "\n",
        "  vector_clip_smooth = []\n",
        "\n",
        "  for j in range(len(clip_vector)):\n",
        "    if j <= engarde_length:\n",
        "      clip_vector_smooth_temp = [clip_vector[j][0], clip_vector[j][1], clip_vector[j][2], clip_vector[j][3]]\n",
        "    else:\n",
        "      clip_vector_smooth_temp = [int(yhata[j - engarde_length]), int(yhatb[j - engarde_length]), clip_vector[j][2], clip_vector[j][3]]\n",
        "    vector_clip_smooth.append(clip_vector_smooth_temp)\n",
        "\n",
        "  return (vector_clip_smooth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug5H_4QojaEk",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load_Clip_Vector\n",
        "def load_clip_vector():\n",
        "  # Only used for large clips\n",
        "\n",
        "  display(f'Loading the Clip Vector...')\n",
        "  filename = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Temp_Clip_Vector/Temp_Clip_Vector_Clips/clip_vector_np1.csv'\n",
        "\n",
        "  display(f'Attempting to load:')\n",
        "  display(filename)\n",
        "  try:\n",
        "    vector_data = pd.read_csv(filename, header=None)\n",
        "    arr = vector_data.to_numpy(dtype = np.int32)\n",
        "    clip_vector = arr.tolist()\n",
        "  except:\n",
        "    display(f'Load Failure...')\n",
        "    display(f'The clip_vector did not exist so it is set to []')\n",
        "    clip_vector = []\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22PKpGGUnmKK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create_Tracking_Masks\n",
        "def create_tracking_masks(previous_positions, certainty, frame_count, torso_size, width, height):\n",
        "  #Creates Tracking Boxes that can be used to mask the image, ignoring parts that are not of interest\n",
        "  #Format, Tracking_Boxes = [Left, Right, Scorebox], Left = [x_min, x_max, y_min, y_max]\n",
        "  #Format, Previous Positions\n",
        "          # previous_positions  = [[Left_Position[-1], Left_Position[-2]], \\\n",
        "          #                      [Right_Position[-1], Right_Position[-2]], \\\n",
        "          #                      [Scoring_Box_Position[-1], Scoring_Box_Position[-2]], \\\n",
        "          #                      [Left_Torso_Position[-1], Left_Torso_Position[-2]], \\\n",
        "          #                      [Right_Torso_Position[-1], Right_Torso_Position[-2]]]\n",
        "\n",
        "          #Format, positions are [x,y]\n",
        "\n",
        "  #Format, torso_position = [[Left_x,Lefty],[Right_x,Right_y]]\n",
        "  #Format, torso_size = [[Lw,Lh], [Rw,Rh]]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Creating Tracking Masks...')\n",
        "    display(f'The Previous Positions are:')\n",
        "    display(previous_positions)\n",
        "    display(f'The torso sizes are:')\n",
        "    display(torso_size)\n",
        "\n",
        "  frame_mask = []\n",
        "\n",
        "  #Certainty is the number of times the bellguard has not been detected in previous frames\n",
        "  #certainty_default is the minimum size of the tracking box\n",
        "  certainty_default = int(width/16)\n",
        "  #certainty_multiplier is how much the tracking box enlarges following a missed\n",
        "  certainty_multiplier = int(width/80)\n",
        "  y_limiter = 24\n",
        "\n",
        "  #Max allowed speed of a bellguard in a single frame\n",
        "  max_speed = int(width/48)\n",
        "\n",
        "  display(f'The length of the previous positions is: {len(previous_positions)}.')\n",
        "\n",
        "  for i in range(len(previous_positions)):\n",
        "    display(f'The masking iteration for frame {frame_count} is {i}.')\n",
        "    #FINDS THE LEFT MASKING BOX\n",
        "    x_pos = previous_positions[i][0][0]\n",
        "    y_pos = previous_positions[i][0][1]\n",
        "    #Converts previous position into a speed\n",
        "    x_speed = min(previous_positions[i][0][0] - previous_positions[i][1][0], max_speed)\n",
        "    # Limits the maximum vertical speed with relation to x\n",
        "    y_speed = min(previous_positions[i][0][1] - previous_positions[i][1][1], int(max_speed/y_limiter))\n",
        "\n",
        "    display(f'x and y position is ({x_pos},{y_pos}) and the speeds are ({x_speed},{y_speed}).')\n",
        "\n",
        "    x_min = x_pos + (x_speed) - (certainty[i]*certainty_multiplier) - certainty_default\n",
        "    x_max = x_pos + (x_speed) + (certainty[i]*certainty_multiplier) + certainty_default\n",
        "    y_min = y_pos + (y_speed) - (certainty[i]*certainty_multiplier) - certainty_default\n",
        "    y_max = y_pos + (y_speed) + (certainty[i]*certainty_multiplier) + certainty_default\n",
        "\n",
        "    #Appends the mask to collection of tracked areas\n",
        "    frame_mask.append([x_min, x_max, y_min, y_max])\n",
        "\n",
        "  display(f'The Frame Mask for frame {frame_count} is:')\n",
        "  display(frame_mask)\n",
        "\n",
        "  return(frame_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrtkw3MVw-H4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Make_Video\n",
        "def make_video(outvid, images=None, fps=25, size=None,\n",
        "               is_color=True, format=\"FMP4\"):\n",
        "  \n",
        "    \"\"\"\n",
        "    Create a video from a list of images.\n",
        " \n",
        "    @param      outvid      output video\n",
        "    @param      images      list of images to use in the video\n",
        "    @param      fps         frame per second\n",
        "    @param      size        size of each frame\n",
        "    @param      is_color    color\n",
        "    @param      format      see http://www.fourcc.org/codecs.php\n",
        "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
        " \n",
        "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
        "    By default, the video will have the size of the first image.\n",
        "    It will resize every image to this size before adding them to the video.\n",
        "    \"\"\"\n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*format)\n",
        "    vid = None\n",
        "    for image in images:\n",
        "        if not os.path.exists(image):\n",
        "            raise FileNotFoundError(image)\n",
        "        img = imread(image)\n",
        "        if vid is None:\n",
        "            if size is None:\n",
        "                size = img.shape[1], img.shape[0]\n",
        "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
        "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
        "            img = resize(img, size)\n",
        "        vid.write(img)\n",
        "    vid.release()\n",
        "    return vid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZZzHy8XnxaE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Mean_of_a_Numpy_Percentile\n",
        "def mean_of_a_numpy_percentile(arr, percentile_cutoff):\n",
        "  # Returns a percentile value of a numpy array\n",
        "\n",
        "  display(f'The average of arr is {np.average(arr)}.')\n",
        "\n",
        "  percentile_value = np.percentile(arr, percentile_cutoff)\n",
        "\n",
        "  # Uses just the percentile without averaging\n",
        "  array_percentile_mean = percentile_value\n",
        "\n",
        "  return (array_percentile_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5E4dnezv1Mf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Frame_Comparison_SSIM\n",
        "def frame_comparison_ssim(frame_number, frame, width, height, frame_count, engarde_length):\n",
        "\n",
        "  # Determines if the subsequent frame is identical to the current or if there was camera motion\n",
        "  # Uses Mean Square Error and Structural Similarity Index\n",
        "\n",
        "  save_path = r'/content/Mask_RCNN/videos/original/'\n",
        "  image_num = frame_number\n",
        "  image_name1 = str(image_num-1) + '.jpg'\n",
        "  file_name1 = os.path.join(save_path, image_name1)\n",
        "  # file_name2 = os.path.join(save_path, image_name2)\n",
        "\n",
        "  image1 = cv2.imread(file_name1)\n",
        "  image2 = frame\n",
        "\n",
        "  # Uses a tighter crop for engarde positioning to minimize motion outside the bout\n",
        "  if frame_number <= engarde_length:\n",
        "    crop_image1 = image1[int(height*1/5):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*1/5):int(height*3/4), 0:width]\n",
        "  else:\n",
        "    # Removes the bottom of the frame to minimize the effect of overlays and shadowing in the foreground\n",
        "    crop_image1 = image1[int(height*0):int(height*2/4), 0:width]\n",
        "    crop_image2 = image2[int(height*0):int(height*2/4), 0:width]\n",
        "\n",
        "  # Calculate MSE\n",
        "  m = np.linalg.norm(image1 - image2)\n",
        "  \n",
        "  # # If GrayScale\n",
        "  # s = ssim(imageA, imageB)\n",
        "  # If Color\n",
        "  s = ssim(crop_image1, crop_image2, multichannel=True)\n",
        "\n",
        "  if verbose == True:\n",
        "\n",
        "    display(f'The Mean Square Error of frame {frame_count} is {m}.')\n",
        "    display(f'The Structural Similarity Index of frame {frame_count} is {s}.')\n",
        "\n",
        "  return(m, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jpMiJc_pD9c",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Frame_Comparison\n",
        "def frame_comparison(frame_number, frame, width, height, frame_count, engarde_length):\n",
        "  # Determines if the subsequent frame is identical to the current or if there was camera motion\n",
        "  # By calculating an average Hue from an HSV image. The Hue is then correlated to an average \n",
        "  # color difference between frames.\n",
        "\n",
        "  save_path = r'/content/Mask_RCNN/videos/original/'\n",
        "  image_num = frame_number\n",
        "  # image_name2 = str(image_num) + '.jpg'\n",
        "  image_name1 = str(image_num-1) + '.jpg'\n",
        "  file_name1 = os.path.join(save_path, image_name1)\n",
        "  # file_name2 = os.path.join(save_path, image_name2)\n",
        "\n",
        "  image1 = cv2.imread(file_name1)\n",
        "  image2 = frame\n",
        "\n",
        "  # Uses a tighter crop for engarde positioning to minimize motion outside the bout\n",
        "  if frame_number <= engarde_length:\n",
        "    crop_image1 = image1[int(height*1/5):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*1/5):int(height*3/4), 0:width]\n",
        "  else:\n",
        "    # Removes the bottom of the frame to minimize the effect of overlays and shadowing in the foreground\n",
        "    crop_image1 = image1[int(height*0):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*0):int(height*3/4), 0:width]\n",
        "\n",
        "  #Convert to Grayscale and find the Difference\n",
        "  image1_gray = cv2.cvtColor(crop_image1, cv2.COLOR_BGR2GRAY)\n",
        "  image2_gray = cv2.cvtColor(crop_image2, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "\n",
        "  # Finds the HSV of image2\n",
        "  image2_HSV = cv2.cvtColor(image2, cv2.COLOR_BGR2HSV)\n",
        "  h_average = np.average(image2_HSV[0])\n",
        "\n",
        "  # Uses Uncropped Frames\n",
        "  # image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "  # image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
        "  # image_diff = cv2.absdiff(image1_gray,image2_gray)\n",
        "  image_diff = cv2.absdiff(crop_image1,crop_image2)\n",
        "  # image_diff_color = cv2.absdiff(image1,image2)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The max for the difference of frame {frame_count} is {np.amax(image_diff)}.')\n",
        "    display(f'The average/median for the difference of frame {frame_count} is {np.average(image_diff)}/{np.median(image_diff)}.')\n",
        "    # display(f'The max for the color difference of frame {frame_count} is {np.amax(image_diff_color)}.')\n",
        "    # display(f'The average/median for the color difference of frame {frame_count} is {np.average(image_diff_color)}/{np.median(image_diff_color)}.')\n",
        "\n",
        "\n",
        "  display(f'The shape of image_diff is {image_diff.shape}.')\n",
        "\n",
        "  # display(f'The shape of image_diff is {image_diff_color.shape}.')\n",
        "\n",
        "  # if frame_number <= engarde_length:\n",
        "  #   image_percentile_mean = mean_of_a_numpy_percentile(image_diff, image_percentile)\n",
        "  #   # image_percentile_mean = mean_of_a_numpy_percentile(image_diff_color, image_percentile)\n",
        "  #   display(f'The image percentile mean is {image_percentile_mean} for frame {frame_count}.')\n",
        "  # else:\n",
        "  #   image_percentile_mean = np.average(image_diff)\n",
        "  #   display(f'The image difference average is {np.average(image_diff)} for frame {frame_count}.')\n",
        "\n",
        "  average_image_diff = np.average(image_diff)\n",
        "  display(f'The image difference average is {np.average(image_diff)} for frame {frame_count}.')\n",
        "\n",
        "  # average_diff = np.average(image_diff)\n",
        "  # average_diff = np.average(image_diff_color)\n",
        "\n",
        "  return(average_image_diff, h_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fop2haomlTNr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test_and_Remove_Duplicate_Frames\n",
        "def test_and_remove_duplicate_frames(file_name, touch_folder, ROOT_DIR, engarde_length):\n",
        "  # Creates a List of unique frames with by comparing the previous and current frames\n",
        "  # This compensates for video compression that may give duplicate frames when FPS is changed\n",
        "\n",
        "  camera_steady = []\n",
        "  engarde_diff_average_arr = np.array([])\n",
        "  engarde_hue_average_arr = np.array([])\n",
        "\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"save\")\n",
        "  VIDEO_ORIG_DIR = os.path.join(VIDEO_DIR, \"original\")\n",
        "  VIDEO_ORIGWORPT_DIR = os.path.join(VIDEO_DIR, \"original_without_repeats\")\n",
        "\n",
        "  display(f'The video directory is {VIDEO_DIR}/{file_name}')\n",
        "  capture = cv2.VideoCapture(os.path.join(VIDEO_DIR, file_name))\n",
        "\n",
        "  total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  if total_frames == 0:\n",
        "    display(f'ERROR: The Video Clip selected has no frames.')\n",
        "  display(f'The total number of frames in the video are: {total_frames}')\n",
        "\n",
        "  width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "  capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
        "  capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "\n",
        "  #Tests for the Same Frames and removes identical frames\n",
        "  frame_count = 0\n",
        "  num_of_deleted_frames = 0\n",
        "  frame_test = True\n",
        "\n",
        "  while True:\n",
        "    ret, frame = capture.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    display(f'frame_count is {frame_count}.')\n",
        "\n",
        "    #Saves an original version of the frame without Regions of Interest\n",
        "    name_orig = '{0}.jpg'.format(frame_count)\n",
        "    name_orig = os.path.join(VIDEO_ORIG_DIR, name_orig)\n",
        "    name_orig_worpt = '{0}.jpg'.format(frame_count - num_of_deleted_frames)\n",
        "    name_orig_worpt = os.path.join(VIDEO_ORIGWORPT_DIR, name_orig_worpt)\n",
        "\n",
        "    if frame_count > 0:\n",
        "      if verbose == True:\n",
        "        display(f'Performing Difference Check for Frame {frame_count}')\n",
        "      # True implies a unique frame while False is a repeat\n",
        "      [average_diff, h_average] = frame_comparison(frame_count, frame, width, height, frame_count, engarde_length)\n",
        "\n",
        "      # [m,s] = frame_comparison_ssim(frame_count, frame, width, height, frame_count, engarde_length)\n",
        "      # average_diff = s\n",
        "      # duplicate_threshold = s/10\n",
        "      # camera_motion_threshold = s*10\n",
        "\n",
        "      # Uses the Engarde Positioning to determine a baseline difference level between frames\n",
        "      if frame_count < engarde_length:\n",
        "        # engarde_diff_average_list.append(average_diff)\n",
        "        engarde_diff_average_arr = np.append(engarde_diff_average_arr, average_diff)\n",
        "        engarde_hue_average_arr = np.append(engarde_hue_average_arr, h_average)\n",
        "\n",
        "      elif frame_count == engarde_length:\n",
        "\n",
        "\n",
        "        engarde_diff_average_arr = np.append(engarde_diff_average_arr, average_diff)\n",
        "        engarde_hue_average_arr = np.append(engarde_hue_average_arr, h_average)\n",
        "\n",
        "        # Emperically Derived Threshold Based on Hue\n",
        "        duplicate_threshold = np.average(engarde_hue_average_arr) * -.037+3.8663\n",
        "        camera_motion_threshold = np.percentile(engarde_diff_average_arr, 40) * camera_motion_threshold_factor\n",
        "                \n",
        "        if verbose == True:\n",
        "          display(f'The Duplicate Threshold at the Engarde Length is {duplicate_threshold}.')\n",
        "          display(f'The Camera Motion Threshold at the Engarde Length is {camera_motion_threshold}.')\n",
        "\n",
        "      elif frame_count > engarde_length:\n",
        "        if average_diff < duplicate_threshold:\n",
        "          if verbose == True:\n",
        "            display(f'The frame {frame_count} is identical to frame {frame_count - 1}.')\n",
        "          frame_test = False\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'Frame {frame_count} is unique.')\n",
        "          frame_test = True\n",
        "        # display(f'{frame_count} greater than engarde length')\n",
        "\n",
        "    # Saves the Image in Either Original or Original and Without Repeat\n",
        "    # Excludes frames that are Part of the Engarde Positioning\n",
        "    if (frame_test == True) or (frame_count <= engarde_length):\n",
        "      cv2.imwrite(name_orig, frame)\n",
        "      cv2.imwrite(name_orig_worpt, frame)\n",
        "      if frame_count > 0:\n",
        "        camera_steady.append(average_diff)\n",
        "      else:\n",
        "        camera_steady.append(0)\n",
        "    else:\n",
        "      cv2.imwrite(name_orig, frame)\n",
        "      num_of_deleted_frames += 1\n",
        "    frame_count += 1\n",
        "\n",
        "  # Releases the Video Capture\n",
        "  capture.release()\n",
        "\n",
        "  # Saves the new video file over the original\n",
        "  # Directory of images to run detection on\n",
        "  # %cd /content/Mask_RCNN/\n",
        "  os.chdir('/content/Mask_RCNN/')\n",
        "  ROOT_DIR = os.getcwd()\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"original_without_repeats\")\n",
        "  images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
        "  # Sort the images by integer index\n",
        "  images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "  # name = str(iterator) + '.mp4'\n",
        "  # name = str(file_name) + '.mp4'\n",
        "  name = file_name\n",
        "  display(f'The iterator file_name is {name}.')\n",
        "  outvid = os.path.join(VIDEO_DIR, name)\n",
        "  make_video(outvid, images, fps=fps)\n",
        "\n",
        "  return (camera_steady, duplicate_threshold, camera_motion_threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw5PNoa-gmJa",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Process_Video_Clip\n",
        "def process_video_clip(file_name, touch_folder, remove_duplicate_frames):\n",
        "  # Processes the video\n",
        "  display(f'The video file_name is: {file_name}')\n",
        "\n",
        "  # Initiates Conditions\n",
        "  score_box_empty = False\n",
        "  right_torso_empty = False\n",
        "  left_torso_empty = False\n",
        "  left_position_empty = False\n",
        "  right_position_empty = False\n",
        "\n",
        "  # %cd /content/Mask_RCNN\n",
        "  os.chdir('/content/Mask_RCNN/')\n",
        "  # !mkdir videos\n",
        "  try:\n",
        "    os.mkdir('videos')\n",
        "  except:\n",
        "    display(f'ERROR creating the video directory')\n",
        "  display(f'os.getcwd() is: {os.getcwd()}')\n",
        "  ROOT_DIR = os.getcwd()\n",
        "  MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"save\")\n",
        "  VIDEO_ORIG_DIR = os.path.join(VIDEO_DIR, \"original\")\n",
        "  VIDEO_ORIGWORPT_DIR = os.path.join(VIDEO_DIR, \"original_without_repeats\")\n",
        "  display(f'The ROOT_DIR is: {ROOT_DIR}')\n",
        "\n",
        "  COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "\n",
        "  display(f'The COCO model path is : {COCO_MODEL_PATH}')\n",
        "\n",
        "  # Removes and recreates the save directory effectively emptying the folder\n",
        "  # !rm -r /content/Mask_RCNN/videos/save\n",
        "  # !rm -r /content/Mask_RCNN/videos/original\n",
        "  # !rm -r /content/Mask_RCNN/videos/original_without_repeats\n",
        "  # Attempts to remove folders to ensure folders are empty\n",
        "  os.chdir('/content/Mask_RCNN/videos')\n",
        "  try:\n",
        "    shutil.rmtree('save')\n",
        "    shutil.rmtree('original')\n",
        "    shutil.rmtree('original_without_repeats')\n",
        "  except:\n",
        "    display(f'Error removing save/original/original_without_repeats folders')\n",
        "  # %cd /content/Mask_RCNN/videos\n",
        "  # os.chdir('/content/Mask_RCNN/videos')\n",
        "  # !mkdir save\n",
        "  # !mkdir original\n",
        "  # !mkdir original_without_repeats\n",
        "  os.mkdir('save')\n",
        "  os.mkdir('original')\n",
        "  os.mkdir('original_without_repeats')\n",
        "\n",
        "  # %cd /content/Mask_RCNN\n",
        "  os.chdir('/content/Mask_RCNN')\n",
        "\n",
        "  # Copies the Video from the Video Clip folder \n",
        "  # path = r'/content/drive/My\\ Drive/projects/fencing/Fencing\\ Clips/' + touch_folder + '/' + file_name\n",
        "  path = '/content/drive/My Drive/projects/fencing/Fencing Clips/' + touch_folder + '/' + file_name\n",
        "  display(f'The path is: {path}')\n",
        "  # !cp $path /content/Mask_RCNN/videos\n",
        "  destination = '/content/Mask_RCNN/videos'\n",
        "  shutil.copy(path, destination)\n",
        "\n",
        "  engarde_length = 10\n",
        "\n",
        "  # Removes Duplicates and Detects Camera motion in Frames\n",
        "  if remove_duplicate_frames == True:\n",
        "    [camera_steady, duplicate_threshold, camera_motion_threshold] = test_and_remove_duplicate_frames(file_name, touch_folder, ROOT_DIR, engarde_length)\n",
        "    display(f'The duplicate frames of video {file_name}.mp4 have been removed')\n",
        "\n",
        "  display(f'The COCO model path is : {COCO_MODEL_PATH}')\n",
        "\n",
        "  if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH) \n",
        "\n",
        "  config = InferenceConfig()\n",
        "  config.display()\n",
        "  # model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
        "  model = MaskRCNN(mode='inference', model_dir='./', config=PredictionConfig())\n",
        "\n",
        "  model_path = 'mask_rcnn_bell_guard_cfg_0005.h5'\n",
        "  model.load_weights(model_path, by_name=True)\n",
        "\n",
        "  class_names = ['BG', 'Bell_Guard', 'Score_Box', 'Torso']\n",
        "\n",
        "  capture = cv2.VideoCapture(os.path.join(VIDEO_DIR, file_name))\n",
        "\n",
        "  total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  if total_frames == 0:\n",
        "    display(f'ERROR: The Video Clip selected has no frames.')\n",
        "  display(f'The total number of frames in the video are: {total_frames}')\n",
        "\n",
        "  try:\n",
        "    if not os.path.exists(VIDEO_SAVE_DIR):\n",
        "          os.makedirs(VIDEO_SAVE_DIR)\n",
        "  except OSError:\n",
        "    print ('Error: Creating directory of data')\n",
        "\n",
        "  frames = []\n",
        "  frame_count = 0\n",
        "\n",
        "  width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "  display(f'The capture width is: {width}')\n",
        "  display(f'The capture height is: {height}')\n",
        "\n",
        "  capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
        "  capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "\n",
        "  # bbox = []\n",
        "  keypoints = []\n",
        "  fencer_data = []\n",
        "  Left_Position = []\n",
        "  Right_Position = []\n",
        "  Scoring_Box_Position = []\n",
        "  Left_Torso_Position = []\n",
        "  Right_Torso_Position = []\n",
        "  left_torso_size_average = []\n",
        "  right_torso_size_average = []\n",
        "  Tracking_Bounding_Boxes = []\n",
        "  \n",
        "  t0 = time.time()\n",
        "\n",
        "  skip_frames = False\n",
        "  skip_frame_counter = 0\n",
        "  frames_to_skip = 300\n",
        "  number_of_frames_skipped = 0\n",
        "  # Assumes any video clip of greater than 10000 frames may require multiple runs\n",
        "  if total_frames > 10000:\n",
        "    clip_vector_previous = load_clip_vector()\n",
        "    frame_count = len(clip_vector_previous)\n",
        "  else:\n",
        "    clip_vector_previous = []\n",
        "  \n",
        "  # Continues until it breaks by not finding a return from attempting to read a frame capture\n",
        "  while True:\n",
        "    if (frame_count%20) == 0:\n",
        "      t1 = time.time()\n",
        "      display(f'Processing frame {frame_count} of {total_frames}. Time elapsed {hms_string(t1 - t0)}.')\n",
        "    if (frame_count%2000) == 0 and (frame_count != 0):\n",
        "      display(f'Saving Clip Progress...')\n",
        "      save_clip_progress(bbox, frame_count, width, height, clip_vector_previous)\n",
        "    #Creates Bounding Box List\n",
        "    ret, frame = capture.read()\n",
        "    #Excludes the Tracking Boxes for the Engarde Length    \n",
        "    if not ret:\n",
        "      break\n",
        "    # Save each frame of the video to a list\n",
        "    frame_count += 1\n",
        "    if skip_frames == False:\n",
        "      t10 = time.time()\n",
        "      frames = [frame]\n",
        "\n",
        "      # Runs the Detection Model for Bellguard, Torso, Scoring_Box\n",
        "      results = model.detect(frames, verbose=0)\n",
        "      # Runs the Human Pose Analysis\n",
        "      [fencer_data_temp, keypoints_temp] = human_pose_analysis(frame)\n",
        "      fencer_data.append(fencer_data_temp)\n",
        "      keypoints.append(keypoints_temp)\n",
        "\n",
        "      for i, item in enumerate(zip(frames, results)):\n",
        "        frame = item[0]\n",
        "        r = item[1]        \n",
        "        name = '{0}.jpg'.format(frame_count + i - 1)\n",
        "        #Saves an original version of the frame without Regions of Interest\n",
        "        name_orig = os.path.join(VIDEO_ORIG_DIR, name)\n",
        "        cv2.imwrite(name_orig, frame)\n",
        "\n",
        "        # Format: display_instances(image, boxes, masks, ids, names, scores):\n",
        "        frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'], file_name)\n",
        "\n",
        "        #Saves the farme as an image with ids overlayed\n",
        "        name = os.path.join(VIDEO_SAVE_DIR, name)\n",
        "        cv2.imwrite(name, frame)\n",
        "\n",
        "        #Captures the Bounding Box data in bbox\n",
        "        bbox = []\n",
        "        for j in range(len(r['rois'])):\n",
        "          bbox.append([r['rois'][j],r['scores'][j],r['class_ids'][j]])\n",
        "        a = r['class_ids'].tolist()\n",
        "\n",
        "      t11 = time.time()\n",
        "      # Displays the time required to process a given frame\n",
        "      display(f'The time to process frame {frame_count} is {hms_string(t11 - t10)}.')\n",
        "\n",
        "      if frame_count <= engarde_length:\n",
        "        if frame_count == 1:\n",
        "          t2 = time.time()\n",
        "        [Left_Position_Temp, Right_Position_Temp, Scoring_Box_Position_Temp, scoring_box_size_average, Tracking_Bounding_Boxes_Temp, \\\n",
        "          Left_Torso_Position_Temp, Right_Torso_Position_Temp, left_torso_size_average_Temp, right_torso_size_average_Temp] \\\n",
        "          = engarde_position(bbox, width, height, engarde_length, frame_count-1)\n",
        "\n",
        "        Left_Position.append(Left_Position_Temp)\n",
        "        Right_Position.append(Right_Position_Temp)\n",
        "        Scoring_Box_Position.append(Scoring_Box_Position_Temp)\n",
        "        Left_Torso_Position.append(Left_Torso_Position_Temp)\n",
        "        Right_Torso_Position.append(Right_Torso_Position_Temp)\n",
        "        left_torso_size_average.append(left_torso_size_average_Temp)\n",
        "        right_torso_size_average.append(right_torso_size_average_Temp)\n",
        "        display(f'At frame {frame_count} the tracking Bounding Boxes Temp is:')\n",
        "        display(Tracking_Bounding_Boxes_Temp)\n",
        "        Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "\n",
        "      # Displays the time to process the engarde positioning frames\n",
        "      display(f'Processing frame {frame_count} of {total_frames}. Time elapsed {hms_string(t1 - t0)}.')\n",
        "\n",
        "      if frame_count == engarde_length:\n",
        "\n",
        "        display(f'Time elapsed processing the engarde positions: {hms_string(t2 - t0)}.')\n",
        "        display(f'Commencing the Engarde Length Processing.')\n",
        "        tracked_items = [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average]\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'The Scoring Box Position is:')\n",
        "          display(Scoring_Box_Position)\n",
        "        if max(Scoring_Box_Position) == []:\n",
        "          display(f'The Scoring Box Position was empty so a default was used.')\n",
        "          score_box_empty = True\n",
        "          tracked_items[2] = [[width/2, height/2], [width/2, height/2], [width/2, height/2]]\n",
        "\n",
        "\n",
        "        # Tests for empty Positions\n",
        "        if max(Right_Torso_Position) == []:\n",
        "          right_torso_empty = True\n",
        "\n",
        "        if max(Left_Torso_Position) == []:\n",
        "          left_torso_empty = True\n",
        "\n",
        "        if max(Left_Position) == [] and max(Left_Torso_Position) != []:\n",
        "          left_position_empty = True\n",
        "\n",
        "        if max(Right_Position) == [] and max(Right_Torso_Position) != []:\n",
        "          right_position_empty = True\n",
        "        \n",
        "        # Replaces [0,0] with [] for torso width and height\n",
        "        for k in range(len(tracked_items)):\n",
        "          for j in range(len(tracked_items[k])):\n",
        "            if tracked_items[k][j] == [0,0]:\n",
        "              tracked_items[k][j] = []\n",
        "\n",
        "        for k in range(len(tracked_items)):\n",
        "          if verbose == True:\n",
        "            display(f'k is {k}.')\n",
        "            display(tracked_items[k])\n",
        "          try:\n",
        "            tracked_items[k] = [item for item in tracked_items[k] if item != []]\n",
        "            tracked_items[k] = np.hstack(tracked_items[k])\n",
        "            if verbose == True:\n",
        "              display(f'The length of len(tracked_items[k]) is {int(len(tracked_items[k])/2)}.')\n",
        "            tracked_items[k] = tracked_items[k].reshape(int(len(tracked_items[k])/2), 2)\n",
        "            tracked_items[k] = np.median(tracked_items[k], axis = 0)\n",
        "            tracked_items[k] = [int(tracked_items[k][0]), int(tracked_items[k][1])]\n",
        "            if verbose == True:\n",
        "              display(f'The tracked item position {tracked_items[k]}.')\n",
        "          except:\n",
        "            display(f'Failure to detect the tracked item {k} during the engarde positioning.')\n",
        "            display(tracked_items[k])\n",
        "            tracked_items[k] = [0,0]\n",
        "\n",
        "        if right_torso_empty == True:\n",
        "          display(f'The Right Torso Position was empty so a default based on the Right BellGuard was used.')\n",
        "          display(f'tracked_items[1] is {tracked_items[1]}, left_torso_size_average is {left_torso_size_average}.')\n",
        "          torso_position_default = [tracked_items[1][0] + int(left_torso_size_average[0][0]*3/4), tracked_items[1][1] - int(left_torso_size_average[0][1]/4)]\n",
        "          tracked_items[4] = torso_position_default\n",
        "          display(f'As a comparison, the Score Box Position is:')\n",
        "          display(Scoring_Box_Position)\n",
        "          display(f'The Right Torso Position is:')\n",
        "          display(Right_Torso_Position)\n",
        "          display(f'As a comparison, the tracked_items[2] is:')\n",
        "          display(tracked_items[2])\n",
        "          display(f'The tracked_items[4] is:')\n",
        "          display(tracked_items[4])\n",
        "\n",
        "        if left_torso_empty == True:\n",
        "          display(f'The Left Torso Position was empty so a default based on the Left BellGuard was used.')\n",
        "          display(f'tracked_items[0] is {tracked_items[0]}, right_torso_size_average is {right_torso_size_average}.')\n",
        "          torso_position_default = [tracked_items[0][0] - int(right_torso_size_average[0][0]*3/4), tracked_items[0][1] - int(right_torso_size_average[0][1]/4)]\n",
        "          tracked_items[3] = torso_position_default\n",
        "          display(f'As a comparison, the Score Box Position is:')\n",
        "          display(Scoring_Box_Position)\n",
        "          display(f'The Right Torso Position is:')\n",
        "          display(Right_Torso_Position)\n",
        "          display(f'As a comparison, the tracked_items[2] is:')\n",
        "          display(tracked_items[2])\n",
        "          display(f'The tracked_items[4] is:')\n",
        "          display(tracked_items[4])\n",
        "\n",
        "        if left_position_empty == True:\n",
        "          display(f'The Left  Position was empty so a default based on the Left Torso was used.')\n",
        "          left_position_default = [tracked_items[3][0] + int(left_torso_size_average[0][0]), tracked_items[3][1] + int(left_torso_size_average[0][0]/4)]\n",
        "          tracked_items[0] = left_position_default\n",
        "\n",
        "        if right_position_empty == True:\n",
        "          display(f'The Right  Position was empty so a default based on the Right Torso was used.')\n",
        "          right_position_default = [tracked_items[4][0] - int(right_torso_size_average[0][0]), tracked_items[4][1] + int(right_torso_size_average[0][0]/4)]\n",
        "          tracked_items[1] = right_position_default\n",
        "\n",
        "        [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position] = [[],[],[],[],[]]\n",
        "\n",
        "        # Builds the Positions of the Tracked Items\n",
        "        for i in range(engarde_length):\n",
        "          Left_Position.append(tracked_items[0])\n",
        "          Right_Position.append(tracked_items[1])\n",
        "          Scoring_Box_Position.append(tracked_items[2])\n",
        "          Left_Torso_Position.append(tracked_items[3])\n",
        "          Right_Torso_Position.append(tracked_items[4])\n",
        "\n",
        "        left_torso_size_average = tracked_items[5]\n",
        "        if right_torso_empty == False:\n",
        "          right_torso_size_average = tracked_items[6]\n",
        "        elif right_torso_empty == True:\n",
        "          right_torso_size_average = tracked_items[5]\n",
        "        torso_size = [left_torso_size_average, right_torso_size_average]\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'The left_torso_size_average is {left_torso_size_average} and the right_torso_size_average is {right_torso_size_average}.')\n",
        "\n",
        "      if frame_count > engarde_length:\n",
        "\n",
        "        positions = [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position]\n",
        "\n",
        "        # Sets the certainty following the engarde positioning\n",
        "        if frame_count == engarde_length + 1:\n",
        "          t3 = time.time()\n",
        "          certainty = [0,0,0,0,0]\n",
        "          if verbose == True:\n",
        "            display(f'The positions following the engarde positioning are:')\n",
        "            display(positions)\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'Certainty just prior to Bell Guard Positioning is {certainty}.')\n",
        "\n",
        "        previous_certainty = certainty\n",
        "\n",
        "        if right_torso_size_average[0] == 0 and left_torso_size_average[0] == 0:\n",
        "          display(f'Error, both Torso sizes are zero.')   \n",
        "        elif right_torso_size_average[0] == 0:\n",
        "            display(f'Right Torso Size was zero, using the Left as a Defualt.')\n",
        "            right_torso_size_average = left_torso_size_average\n",
        "        elif left_torso_size_average[0] == 0:\n",
        "            display(f'Left Torso Size was zero, using the Right as a Defualt.')\n",
        "            left_torso_size_average = right_torso_size_average\n",
        "\n",
        "        # Finds the Tracked Items and Returns their positions\n",
        "        [Left_Position_Temp, Right_Position_Temp, Scoring_Box_Position_Temp, Tracking_Bounding_Boxes_Temp, \\\n",
        "         Left_Torso_Position_Temp, Right_Torso_Position_Temp, engarde_length, certainty] \\\n",
        "         = Bell_Guard_Position_Finding(bbox, width, height, fencer_data_temp, positions, frame_count, \\\n",
        "         left_torso_size_average, right_torso_size_average, engarde_length, certainty, camera_steady, camera_motion_threshold)\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'Certainty just after to Bell Guard Positioning is {certainty}.')\n",
        "          display(f'The Left Position at frame {frame_count - 1} is {Left_Position_Temp}.')\n",
        "\n",
        "        # Appends the Returned Positions\n",
        "        Left_Position.append(Left_Position_Temp)\n",
        "        Right_Position.append(Right_Position_Temp)\n",
        "        Scoring_Box_Position.append(Scoring_Box_Position_Temp)\n",
        "        Left_Torso_Position.append(Left_Torso_Position_Temp)\n",
        "        Right_Torso_Position.append(Right_Torso_Position_Temp)\n",
        "        Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "\n",
        "        # Tests for a change in certainty to zero from non-zero. If a position has become certain during\n",
        "        # this frame then it back calculates previous uncertain position up to a certain position.\n",
        "        if (certainty[0] == 0 and previous_certainty[0] != 0):\n",
        "          Left_Position = position_linear_approximation(Left_Position, previous_certainty[0])\n",
        "          if verbose == True:\n",
        "            display(f'Using a Linear Approximation for frame {frame_count} for the Left Bellguard Position.')\n",
        "        elif (certainty[1] == 0 and previous_certainty[1] != 0):\n",
        "          if verbose == True:\n",
        "            display(f'The Right Position is: {Right_Position}.')\n",
        "            display(f'The Previous Certainty is: {previous_certainty[1]}')\n",
        "            display(f'Using a Linear Approximation for frame {frame_count} for the Right Bellguard Position.')\n",
        "          Right_Position = position_linear_approximation(Right_Position, previous_certainty[1])\n",
        "        elif (certainty[2] == 0 and previous_certainty[2] != 0):\n",
        "          Scoring_Box_Position = position_linear_approximation(Scoring_Box_Position, previous_certainty[2])\n",
        "        elif (certainty[3] == 0 and previous_certainty[3] != 0):\n",
        "          Left_Torso_Position = position_linear_approximation(Left_Torso_Position, previous_certainty[3])\n",
        "        elif (certainty[4] == 0 and previous_certainty[4] != 0):\n",
        "          Right_Torso_Position = position_linear_approximation(Right_Torso_Position, previous_certainty[4])\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "  t4 = time.time()\n",
        "  if verbose == True:\n",
        "    display(f'Time elapsed to process the engarde frames is  {hms_string(t3 - t2)}.')\n",
        "    display(f'Time elapsed to process the post engarde frames is  {hms_string(t4 - t3)}.')\n",
        "    display(f'Time elapsed processing the clip the total clip is {hms_string(t4 - t0)}.')\n",
        "    # Reduces the Frame Count to account for skipped frames\n",
        "    display(f'The original frame count was: {frame_count - 1} and the number of frames skipped is: {number_of_frames_skipped}.')\n",
        "  frame_count = len(bbox)\n",
        "  if verbose == True:\n",
        "    display(f'The length of the frame_count is {frame_count - 1} while the number of bboxes is {len(bbox)}.')\n",
        "\n",
        "  file_to_remove = r'/Mask_RCNN/videos/' + file_name\n",
        "  # Removes the File if it already exists\n",
        "  # !rm $file_to_remove\n",
        "  try:\n",
        "    shutil.rmtree(file_to_remove)\n",
        "  except:\n",
        "    display(f'ERROR removing the video file to analyze.')\n",
        "\n",
        "  capture.release()\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Left Position just prior to drawing the Bell_Guards is:')\n",
        "    display(Left_Position)\n",
        "    display(f'The Right Position just prior to drawing the Bell_Guards is:')\n",
        "    display(Right_Position)\n",
        "\n",
        "  t5 = time.time()\n",
        "  #Draws the Boxes on the image frame and determines scoring lights turned on\n",
        "  [left_light_comparison, right_light_comparison] = draw_Bell_Guard_Position(Left_Position, Right_Position, Scoring_Box_Position, \\\n",
        "    scoring_box_size_average, Left_Torso_Position, Right_Torso_Position, frame_count, Tracking_Bounding_Boxes, \\\n",
        "    video_filename, width, height, engarde_length, keypoints, score_box_empty, camera_steady, camera_motion_threshold)\n",
        "  t6 = time.time()\n",
        "  display(f'Time elapsed while drawing the Bell_Guard positions is {hms_string(t6 - t5)}.')\n",
        "\n",
        "  if camera_motion_compensate == True and score_box_empty == False:\n",
        "    #Adjusts the Bellguard Position Based on the Camera motion as determined by the Score_Box Position\n",
        "    Left_Position = camera_motion_adjustment(Left_Position, Scoring_Box_Position)\n",
        "    Right_Position = camera_motion_adjustment(Right_Position, Scoring_Box_Position)\n",
        "\n",
        "  t7 = time.time()\n",
        "  display(f'Time elapsed for the camera motion adjustment is {hms_string(t7 - t6)}.')\n",
        "\n",
        "  if camera_motion_compensate == True and score_box_empty == False:\n",
        "    #Adjusts Left and Right Position for convenient visualization\n",
        "    [Left_Position, Right_Position] = position_down_scale(Left_Position, Right_Position, width, height)\n",
        "\n",
        "  t8 = time.time()\n",
        "  #Creates a vector representing the clip, format [left_x, right_x, left_lights, right_lights]\n",
        "  clip_vector = clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous, width)\n",
        "\n",
        "  if smooth_video_clip == True:\n",
        "    #Smoothes the Clip using SavitzkyGolay filter\n",
        "    clip_vector = smooth_clip_vector(clip_vector, engarde_length)\n",
        "\n",
        "  t9 = time.time()\n",
        "  display(f'Time elapsed to generate the clip vector is {hms_string(t9 - t8)}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The final clip vector is:')\n",
        "    display(clip_vector)\n",
        "\n",
        "  return (bbox, frame_count, width, height, clip_vector_previous, fencer_data, keypoints, clip_vector, fps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkHx2cfqpmyM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Prepare_Video_Frames\n",
        "def prepare_video_frames(img_directory, video_title):\n",
        "\n",
        "  ROOT_DIR = '/content/Mask_RCNN/'\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, img_directory)\n",
        "  images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
        "  # Sort the images by integer index\n",
        "  images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "  name = str(iterator) + '.' + video_title + '.mp4'\n",
        "  outvid = os.path.join(VIDEO_DIR, name)\n",
        "\n",
        "  return (outvid, images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBlM2EDSieV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Downsample_FPS\n",
        "def downsample_fps(a,b):\n",
        "  # Adjusts the elements of a larger set a to fit into the length of set b\n",
        "\n",
        "  c = []\n",
        "  remainder = 0\n",
        "  for i in range(len(b)):\n",
        "    c_temp = []\n",
        "    if verbose == True:\n",
        "      display(f'The lower range is {math.ceil(len(a)/len(b)*(i+1)-1-remainder)} and the upper range is {math.floor(len(a)/len(b)*(i+1))}.')\n",
        "    for j in range(math.ceil(len(a)/len(b)*(i)-remainder),math.floor(len(a)/len(b)*(i+1))):\n",
        "      c_temp.append(a[j])\n",
        "      if verbose == True:\n",
        "        display(f'i,j = {i},{j} and c_temp = {c_temp}')\n",
        "    remainder = (len(a)/len(b))*(i+1) - int(len(a)/len(b)*(i+1))\n",
        "    c.append(round(sum(c_temp)/len(c_temp)))\n",
        "    if verbose == True:\n",
        "      display(f'The remainder at i = {i} and j = {j} is {remainder} and c is {c}.')\n",
        "\n",
        "  return (c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FDqmsziTCBC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load_Clip\n",
        "def load_clip(folder, clip_number, max_length):\n",
        "  if folder == 'Left' or folder == 'left' or folder == 'Left_Touch':\n",
        "    folder = 0\n",
        "  if folder == 'Right' or folder == 'right'or folder == 'Right_Touch':\n",
        "    folder = 1\n",
        "  if folder == 'Simul' or folder == 'simul'or folder == 'Simul':\n",
        "    folder = 2\n",
        "\n",
        "  touch_folder = ['Left_Touch', 'Right_Touch', 'Simul']\n",
        "\n",
        "  i = folder\n",
        "\n",
        "  file = 'clip_vector_acceleration_np' + str(clip_number) + '.csv'\n",
        "  path = r'/content/drive/My Drive/projects/fencing/Fencing Clips/' + touch_folder[i] + '/' + touch_folder[i] + '_Vector_Clips_Acceleration/'\n",
        "\n",
        "  vector_data = pd.read_csv(os.path.join(path, file), header=None)\n",
        "  clip_vector = vector_data.to_numpy(dtype = np.float32)\n",
        "\n",
        "  display(os.path.join(path, file))\n",
        "\n",
        "  # Pads the clip_vector to 103\n",
        "  # If the clip is greater than Max Length, it is truncated\n",
        "  if len(clip_vector) > max_length:\n",
        "    clip_vector = clip_vector[len(clip_vector) - max_length:]\n",
        "  padding = np.array([0,0,0,0])\n",
        "  for k in range(max_length - (len(clip_vector))):\n",
        "    clip_vector = np.vstack((clip_vector, padding))\n",
        "\n",
        "  #Normalizes the Value by 31\n",
        "  max_value = 31\n",
        "  for i in range(len(clip_vector)):\n",
        "    for j in range(2):\n",
        "      if clip_vector[i][j] < max_value:\n",
        "        clip_vector[i][j] = clip_vector[i][j] * (1/max_value)\n",
        "      else:\n",
        "        #Preserves the sign of the value\n",
        "        clip_vector[i][j] = clip_vector[i][j]/(abs(clip_vector[i][j]))\n",
        "\n",
        "  # Removes the First 15 frames to minimize engarde positioning\n",
        "  clip_vector = clip_vector[15:]\n",
        "\n",
        "  # Sets Clip_Vector to Zero if Light is on\n",
        "  for j in range(len(clip_vector)):\n",
        "    if clip_vector[j][2] == 1:\n",
        "      clip_vector[j][0] = 0\n",
        "    if clip_vector[j][3] == 1:\n",
        "      clip_vector[j][1] = 0 \n",
        "\n",
        "  clip_vector = clip_vector.reshape(1,clip_vector_length,4)\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6yO3gTSHox",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create_Folder_Hierarchy\n",
        "def create_folder_hierarchy(file_name):\n",
        "\n",
        "  # Creates File Path in Google Drive\n",
        "  # !mkdir -p '/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips'\n",
        "  try:\n",
        "    os.makedirs('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips')\n",
        "  except:\n",
        "    display(f'ERROR creating the Left_Touch_Vector_Clips')\n",
        "  # %cd '/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "  os.chdir('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/')\n",
        "  # !mkdir Left_Touch_Vector_Clips_Speed\n",
        "  try:\n",
        "    os.mkdir('Left_Touch_Vector_Clips_Speed')\n",
        "  except:\n",
        "    display(f'ERROR creating the Left_Touch_Vector_Clips_Speed')\n",
        "  # !mkdir Left_Touch_Vector_Clips_Acceleration\n",
        "  try:\n",
        "    os.mkdir('Left_Touch_Vector_Clips_Acceleration')\n",
        "  except:\n",
        "    display(f'ERROR creating the Left_Touch_Vector_Clips_Speed')\n",
        "\n",
        "  # Copies Video File to Left_Touch Folder\n",
        "  file_name = r'/content/drive/My Drive/' + file_name\n",
        "  destination = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "  # !cp {file_name} {destination}\n",
        "  shutil.copy(file_name, destination)\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ8y96lwZccc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Evaluate_Model\n",
        "def evaluate_model(dataset, model, cfg):\n",
        "  # calculate the mAP for a model on a given dataset\n",
        "  APs = list()\n",
        "  for image_id in dataset.image_ids:\n",
        "\t\t# load image, bounding boxes and masks for the image id\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
        "\t\t# convert pixel values (e.g. center)\n",
        "    scaled_image = mold_image(image, cfg)\n",
        "\t\t# convert image into one sample\n",
        "    sample = expand_dims(scaled_image, 0)\n",
        "\t\t# make prediction\n",
        "    yhat = model.detect(sample, verbose=0)\n",
        "\t\t# extract results for first sample\n",
        "    r = yhat[0]\n",
        "\t\t# calculate statistics, including AP\n",
        "    AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "\t\t# store\n",
        "    APs.append(AP)\n",
        "\t# calculate the mean AP across all images\n",
        "  mAP = mean(APs)\n",
        "  return mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac1bWiP8DDF2",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Bell_GuardDataset\n",
        "class Bell_GuardDataset(Dataset):\n",
        "  #class that defines and loads the Bell_Guard dataset\n",
        "\t# load the dataset definitions\n",
        "  def load_dataset(self, dataset_dir, is_train=True):\n",
        "\t\t# define the two classes\n",
        "    self.add_class(\"dataset\", 1, \"bellguard\")\n",
        "    self.add_class(\"dataset\", 2, \"scorebox\")\n",
        "    self.add_class(\"dataset\", 3, \"torso\")\n",
        "    # self.add_class(\"dataset\", 4, \"person\")\n",
        "\n",
        "    # Adds the Images to be Analyzed\n",
        "\t\t# Defines data locations\n",
        "    images_dir = dataset_dir + '/images/'\n",
        "    annotations_dir = dataset_dir + '/annots/'\n",
        "\t\t# Finds all images\n",
        "    for filename in listdir(images_dir):\n",
        "      # Extracts image id\n",
        "      image_id = filename[5:-4]\n",
        "\t\t\t# Skips all images after the training set\n",
        "      if is_train and int(image_id) >= train_set_number:\n",
        "        continue\n",
        "\t\t\t# Skips all images before the training set\n",
        "      if not is_train and int(image_id) < train_set_number:\n",
        "        continue\n",
        "      img_path = images_dir + filename\n",
        "      display(img_path)\n",
        "      ann_path = annotations_dir + 'Image' + image_id + '.xml'\n",
        "      display(ann_path)\n",
        "\t\t\t# Add to dataset\n",
        "      self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
        " \n",
        "\t# Extract bounding boxes from an annotation file\n",
        "  def extract_boxes(self, filename):\n",
        "\t\t# load and parse the file\n",
        "    tree = ElementTree.parse(filename)\n",
        "\t\t# get the root of the document\n",
        "    root = tree.getroot()\n",
        "\t\t# extract each bounding box\n",
        "    boxes = list()\n",
        "\n",
        "    objects = root.findall('.//object')\n",
        "    # Adds the object found to the end of boxes. Boxes now has 5 values instead of 4.\n",
        "    objects_to_find = ['bellguard', 'scorebox', 'torso']\n",
        "    for object_to_find_iterator in objects_to_find:\n",
        "      # display(f'The object iterator is: {object_to_find_iterator}')\n",
        "      for obj in objects:\n",
        "        if (obj.find('.name').text) == object_to_find_iterator:\n",
        "          xmin = obj.find('.bndbox/xmin')\n",
        "          ymin = obj.find('.bndbox/ymin')\n",
        "          xmax = obj.find('.bndbox/xmax')\n",
        "          ymax = obj.find('.bndbox/ymax')\n",
        "          coors = [int(xmin.text), int(ymin.text), int(xmax.text), int(ymax.text), object_to_find_iterator]\n",
        "          # display(coors)\n",
        "          boxes.append(coors)\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "\t\t# Extracts image dimensions\n",
        "    width = int(root.find('.//size/width').text)\n",
        "    height = int(root.find('.//size/height').text)\n",
        "    return boxes, width, height\n",
        " \n",
        "\t# Loads the masks for an image\n",
        "  def load_mask(self, image_id):\n",
        "\t\t# Gets details of image\n",
        "    info = self.image_info[image_id]\n",
        "\t\t# Defines box file location\n",
        "    path = info['annotation']\n",
        "\t\t# Loads XML\n",
        "    object_to_find = 'bellguard'\n",
        "    # boxes, w, h = self.extract_boxes(path, object_to_find)\n",
        "    display(f'The image_id is: {image_id}')\n",
        "\n",
        "    boxes, w, h = self.extract_boxes(path)\n",
        "\t\t# Creates one array for all masks, each on a different channel\n",
        "    masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
        "\t\t# Creates masks\n",
        "    class_ids = list()\n",
        "    # len(boxes) is the number of 5 value lists within the list of boxes\n",
        "    for i in range(len(boxes)):\n",
        "      box = boxes[i]\n",
        "      row_s, row_e = box[1], box[3]\n",
        "      col_s, col_e = box[0], box[2]\n",
        "      masks[row_s:row_e, col_s:col_e, i] = 1\n",
        "      class_ids.append(self.class_names.index(box[4]))\n",
        "      # class_ids.append(self.class_names.index('Bell_Guard'))\n",
        "\n",
        "    # display(f'The class_ids in load_mask are: {asarray(class_ids, dtype=\"int32\")}')\n",
        "    return masks, asarray(class_ids, dtype='int32')\n",
        " \n",
        "\t# load an image reference\n",
        "  def image_reference(self, image_id):\n",
        "    info = self.image_info[image_id]\n",
        "    return info['path']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UelgtD9fLzrR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Bell_GuardConfig\n",
        "class Bell_GuardConfig(Config):\n",
        "\t#Bell_GuardConfig\n",
        "\tGPU_COUNT = 1\n",
        "\tIMAGES_PER_GPU = 1\n",
        "\n",
        "\t# define the name of the configuration\n",
        "\tNAME = \"Bell_Guard_cfg\"\n",
        "\tNUM_CLASSES = 1 + 3\n",
        "\t# number of training steps per epoch\n",
        "\tSTEPS_PER_EPOCH = 131"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWX2andCydq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title InferenceConfig\n",
        "class InferenceConfig(Config):\n",
        "  GPU_COUNT = 1\n",
        "  #Same as batch_size\n",
        "  IMAGES_PER_GPU = 1\n",
        "  NUM_CLASSES = 4\n",
        "  KEYPOINT_MASK_POOL_SIZE = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joO1L4fvZWpy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title PredictionConfig\n",
        "class PredictionConfig(Config):\n",
        "  # Evaluate_Model\n",
        "  # Defines the prediction configuration\n",
        "\t# define the name of the configuration\n",
        "  NAME = \"bell_guard_cfg\"\n",
        "\t# number of classes (background + bell_guard + scorebox + torso)\n",
        "  NUM_CLASSES = 1 + 3\n",
        "  GPU_COUNT = 1\n",
        "  IMAGES_PER_GPU = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxP-wmmX_X5a",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Train_Model == True:\n",
        "if train_model == True:\n",
        "  # Transfers the Bell_Guard set from Google Drive, specifically images and annotations\n",
        "  # %cd /content/drive/My Drive/projects/fencing/Bell_Guard\n",
        "  os.chdir('/content/drive/My Drive/projects/fencing/Bell_Guard')\n",
        "  # !cp -r /content/drive/My\\ Drive/projects/fencing/Bell_Guard/ /content/Mask_RCNN/\n",
        "  source = '/content/drive/My Drive/projects/fencing/Bell_Guard/'\n",
        "  destination = '/content/Mask_RCNN/Bell_Guard/'\n",
        "  shutil.copytree(source, destination)\n",
        "\n",
        "  #Finds the number of images used in training and testing\n",
        "  # %cd /content/drive/My Drive/projects/fencing/Bell_Guard/images\n",
        "  os.chdir('/content/drive/My Drive/projects/fencing/Bell_Guard/images')\n",
        "\n",
        "  number_of_images = len(os.listdir())\n",
        "  train_set_number = int(0.8*number_of_images)\n",
        "\n",
        "  # %cd /content/Mask_RCNN/\n",
        "  os.chdir('/content/Mask_RCNN/')\n",
        "\n",
        "  # Training set\n",
        "  train_set = Bell_GuardDataset()\n",
        "  train_set.load_dataset('Bell_Guard', is_train=True)\n",
        "  train_set.prepare()\n",
        "  if verbose == True:\n",
        "    print('Train: %d' % len(train_set.image_ids))\n",
        "  \n",
        "  # Testing/Evaluation set\n",
        "  test_set = Bell_GuardDataset()\n",
        "\n",
        "  test_set.load_dataset('Bell_Guard', is_train=False)\n",
        "  test_set.prepare()\n",
        "  if verbose == True:\n",
        "    print('Test: %d' % len(test_set.image_ids))\n",
        "\n",
        "  # enumerate all images in the dataset\n",
        "  for image_id in train_set.image_ids:\n",
        "  \t# load image info\n",
        "  \tinfo = train_set.image_info[image_id]\n",
        "\n",
        "  # define image id\n",
        "  image_id = 1\n",
        "  # load the image\n",
        "  image = train_set.load_image(image_id)\n",
        "  # load the masks and the class ids\n",
        "  mask, class_ids = train_set.load_mask(image_id)\n",
        "  # extract bounding boxes from the masks\n",
        "  bbox = extract_bboxes(mask)\n",
        "  # display_instances(image, bbox, mask, class_ids, train_set.class_names)\n",
        "\n",
        "  # Train_Model\n",
        "  # prepare train set\n",
        "  train_set = Bell_GuardDataset()\n",
        "  if verbose == True:\n",
        "    display(f'Loading the Train DataSet')\n",
        "  train_set.load_dataset('Bell_Guard', is_train=True)\n",
        "  #train_set.load_dataset('kangaroo', is_train=True)\n",
        "  train_set.prepare()\n",
        "  if verbose == True:\n",
        "    print('Train: %d' % len(train_set.image_ids))\n",
        "\n",
        "  # prepare test/val set\n",
        "  test_set = Bell_GuardDataset()\n",
        "  display(f'Loading the Test DataSet')\n",
        "  test_set.load_dataset('Bell_Guard', is_train=False)\n",
        "  test_set.prepare()\n",
        "  if verbose == True:\n",
        "    print('Test: %d' % len(test_set.image_ids))\n",
        "  # Prepare config\n",
        "  config = Bell_GuardConfig()\n",
        "  config.display()\n",
        "\n",
        "  # Define the model\n",
        "  model = MaskRCNN(mode='training', model_dir='./', config=config)\n",
        "  # Load weights (mscoco) and exclude the output layers\n",
        "  model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "\n",
        "  # Train weights (output layers or 'heads')\n",
        "\n",
        "  augmentation = imgaug.augmenters.Fliplr(0.5)\n",
        "  # # model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')\n",
        "  # model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads', augmentation = imgaug.augmenters.Sometimes(0.5, [\n",
        "  #                     imgaug.augmenters.Fliplr(0.5),\n",
        "  #                     imgaug.augmenters.GaussianBlur(sigma=(0.0, 5.0))\n",
        "  # \t\t\t\t\t\t\t\t]))\n",
        "  #Bypasses an Error\n",
        "  model.keras_model.metrics_tensors = []\n",
        "  # model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads', augmentation=augmentation, IMAGE_META_SIZE = 8)\n",
        "  model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads', augmentation=augmentation)\n",
        " \n",
        "  # Save_Model\n",
        "  # Moves mask_rcnn_bell_guard_cfg_0005.h5 to the Mask_RCNN directory\n",
        "  file = !find  bell_guard_* -type d\n",
        "  file = str(file[0])\n",
        "  file = '/content/Mask_RCNN/' + file + '/mask_rcnn_bell_guard_cfg_0005.h5'\n",
        "  # !cp {file} /content/Mask_RCNN/\n",
        "  source = '/content/drive/My Drive/projects/fencing/Bell_Guard/'\n",
        "  destination = '/content/Mask_RCNN/'\n",
        "  shutil.copy(file, destination)\n",
        "\n",
        "  # load the train dataset\n",
        "  train_set = Bell_GuardDataset()\n",
        "  #train_set = KangarooDataset()\n",
        "  train_set.load_dataset('Bell_Guard', is_train=True)\n",
        "  #train_set.load_dataset('kangaroo', is_train=True)\n",
        "  train_set.prepare()\n",
        "  print('Train: %d' % len(train_set.image_ids))\n",
        "  # load the test dataset\n",
        "  test_set = Bell_GuardDataset()\n",
        "  #test_set = KangarooDataset()\n",
        "  test_set.load_dataset('Bell_Guard', is_train=False)\n",
        "  #test_set.load_dataset('kangaroo', is_train=False)\n",
        "  test_set.prepare()\n",
        "  print('Test: %d' % len(test_set.image_ids))\n",
        "  # create config\n",
        "  cfg = PredictionConfig()\n",
        "  # define the model\n",
        "  model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
        "  # load model weights\n",
        "  model_path = 'mask_rcnn_bell_guard_cfg_0005.h5'\n",
        "  #model_path = 'mask_rcnn_kangaroo_cfg_0005.h5'\n",
        "  model.load_weights(model_path, by_name=True)\n",
        "  # evaluate model on training dataset\n",
        "  train_mAP = evaluate_model(train_set, model, cfg)\n",
        "  print(\"Train mAP: %.3f\" % train_mAP)\n",
        "  # evaluate model on test dataset\n",
        "  test_mAP = evaluate_model(test_set, model, cfg)\n",
        "  print(\"Test mAP: %.3f\" % test_mAP)\n",
        "  \n",
        "else:\n",
        "  # Load the pre-trained fencing object detection model\n",
        "  # !cp -r /content/drive/My\\ Drive/mask_rcnn_bell_guard_cfg_0005.h5 /content/Mask_RCNN/\n",
        "  source = '/content/drive/My Drive/mask_rcnn_bell_guard_cfg_0005.h5'\n",
        "  destination = '/content/Mask_RCNN/'\n",
        "  shutil.copy(source, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-RRulU0obH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if simplified == True:\n",
        "#   video_filename = '82.mp4'\n",
        "  create_folder_hierarchy(video_filename)\n",
        "  clip_call = 0\n",
        "else:\n",
        "  iterator = 103\n",
        "  clip_call = 0\n",
        "  video_filename = str(iterator)+'.mp4'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPQvpuHVa2Bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "touch_folders = ['Left_Touch', 'Right_Touch', 'Simul']\n",
        "\n",
        "root_path = r'/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "video_dir_clips = root_path + touch_folders[clip_call]\n",
        "display(f'The video_dir_clips path is: {video_dir_clips}')\n",
        "\n",
        "path = r'/content/Mask_RCNN/videos/'\n",
        "\n",
        "[bbox, frame_count, capture_width, capture_height, clip_vector_previous, fencer_data, keypoints, clip_vector, fps] = \\\n",
        "  process_video_clip(video_filename, touch_folders[clip_call], remove_duplicate_frames)\n",
        "\n",
        "# Removes the .mp4 from the String\n",
        "if simplified == True:\n",
        "  iterator = video_filename[:-4]\n",
        "\n",
        "#Saves the Clip, Speed and Acceleration Vectors\n",
        "clip_vector_np_save(touch_folders[clip_call], iterator, clip_vector)\n",
        "\n",
        "#Saves Images for the Representative Video\n",
        "create_representative_image(clip_vector, capture_width, capture_height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlYBdce86fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepares and Downloads videos\n",
        "if download_videos == True:\n",
        "  # Prepares Output Video\n",
        "  [outvid, images] = prepare_video_frames('save', 'out')\n",
        "  make_video(outvid, images, fps=fps)\n",
        "\n",
        "  # Downloads Output Video\n",
        "  name = '/content/Mask_RCNN/videos/' + str(iterator) + '.out.mp4'\n",
        "  display(name)\n",
        "  files.download(name)\n",
        "\n",
        "  #Prepares Representative Video\n",
        "  [outvid, images] = prepare_video_frames('save_white_dot', 'representative_out')\n",
        "  make_video(outvid, images, fps=fps)\n",
        "\n",
        "  # Downloads Representative Video\n",
        "  name =  '/content/Mask_RCNN/videos/' + str(iterator) + '.representative_out.mp4'\n",
        "  files.download(name)\n",
        "\n",
        "  # Prepares Overlay Video\n",
        "  create_overlay_image(len(clip_vector))\n",
        "  [outvid, images] = prepare_video_frames('overlay', 'overlay_out')\n",
        "  make_video(outvid, images, fps=fps)\n",
        "\n",
        "  # Downloads Overlay Video\n",
        "  name =  '/content/Mask_RCNN/videos/' + str(iterator) + '.overlay_out.mp4'\n",
        "  files.download(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU0dTKFvRoKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyzes the Video Clip\n",
        "engarde_position_buffer = 15\n",
        "max_length = 103\n",
        "clip_vector_length = max_length - engarde_position_buffer\n",
        "\n",
        "if simplified == True:\n",
        "  save_path = '/content/drive/My Drive/'\n",
        "else:\n",
        "  save_path = '/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "model = load_model(os.path.join(save_path, 'ROW_model.h5'))\n",
        "\n",
        "x = load_clip(touch_folders[clip_call], iterator, max_length)\n",
        "\n",
        "pred = model.predict(x)\n",
        "# display(f'The predicted touch is Left {int(pred[0][0]*100)}%, Right {int(pred[0][1]*100)}%, Simul {int(pred[0][2]*100)}%.')\n",
        "pred_total = pred[0][0] + pred[0][1] + pred[0][2]\n",
        "display(f'The total time elapsed is: {hms_string(time.time() - t_start)}.')\n",
        "display(f'The normalized predicted touch is Left {int(pred[0][0]/pred_total*100)}%, Right {int(pred[0][1]/pred_total*100)}%, Simul {int(pred[0][2]/pred_total*100)}%.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}