{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Virtual Saber Box.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenKohn2004/Saber_Box/blob/master/Virtual_Saber_Box.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI8gKtzJ_7J3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce5c14e7-b640-4b3d-ed7e-73aca0d04f2a"
      },
      "source": [
        "from os.path import exists, join, basename, splitext\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "COLAB = True\n",
        "\n",
        "# Determines if the Sync Folder is used for the Video Clip and Runs the Entire Sync Folder\n",
        "run_most_recent_clip = False\n",
        "run_entire_sync_folder = True\n",
        "use_youtube_link = False\n",
        "\n",
        "print(\"Note: using Google CoLab\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR2dQlPrqZ8"
      },
      "source": [
        "# Leave youtube Link as '' to use saved clip\n",
        "if use_youtube_link == True:\n",
        "  youtube_link = 'https://youtu.be/9Ym1osNQGts?t=147'\n",
        "else:\n",
        "  youtube_link = ''\n",
        "\n",
        "video_filename = '999.mp4'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "2IERHKVkEPos"
      },
      "source": [
        "#@title Initial Parameters\n",
        "# Establishes Initial Parameters\n",
        "import time\n",
        "\n",
        "# Provides additional comment feedback while running\n",
        "verbose = False\n",
        "# Crops each from to the tracking boxes\n",
        "overlay_masking_boxes = False\n",
        "# Adjusts the Reperesentative Dots based on camera motion implied by scoring box movement.\n",
        "camera_motion_compensate = False\n",
        "# Smooths the Bellguard positions\n",
        "smooth_video_clip = False\n",
        "# Assumes two lights if Bellguards are close to each other. Reduces dependency on Box detection.\n",
        "assume_lights = True\n",
        "# Ignores Lights from the Scorebox. Mitigates poor scorebox tracking.\n",
        "ignore_box_lights = True\n",
        "# Tests and Removes Duplicate Frames from the video\n",
        "remove_duplicate_frames = False\n",
        "# Downloads out and representative_out videos\n",
        "download_videos = True\n",
        "# Analyzes the Action\n",
        "analyze_action = True\n",
        "# Allows for Simple Usage\n",
        "simplified = False\n",
        "# Allows for the use of Exclusion Areas\n",
        "use_Exclusion_Areas = True\n",
        "# If the clip is longer than duration limit in seconds than a portion is downloaded instead of the whole clip\n",
        "duration_download_limit = 2400\n",
        "# Allows to run the entire clip or only till the lights turn on\n",
        "run_entire_video = False\n",
        "# Creates an Output video of the motion tracking\n",
        "create_output_video = True\n",
        "# Outputs the Bounding Box Data for the clip as a CSV file\n",
        "output_bbox_clip = True\n",
        "# Sets the output_bbox_clip to output the Foot Data\n",
        "output_foot_data = True\n",
        "\n",
        "min_torso_confidence = 0.70\n",
        "bellguard_confidence = 0.40\n",
        "foot_confidence = 0.60\n",
        "# Provides for a higher confidence of bellguard detection\n",
        "bellguard_confidence_high = 0.45\n",
        "# Provides for a higher confidence of bellguard detection\n",
        "bellguard_confidence_very_high = 0.50\n",
        "# Provides for a higher confidence of bellguard detection\n",
        "bellguard_confidence_extra_very_high = 0.55\n",
        "# Allows for a different required confidence for initial detection than tracking\n",
        "bellguard_tracking_det_offset = 0.15\n",
        "wrist_conf_min = 3\n",
        "wrist_conf_high = 4\n",
        "wrist_conf_very_high = 6\n",
        "wrist_conf_extra_very_high = 8\n",
        "knee_conf_min = 3\n",
        "# The Threshold for determining duplicate frames\n",
        "duplicate_threshold_factor = 0.75\n",
        "# The Threshold for determining camera motion\n",
        "camera_motion_threshold_factor = 8\n",
        "# Bellguard difference as a fraction of capture width to assume lights on\n",
        "position_difference_ratio = .065\n",
        "# Sets an upper limit for the size in kiloBytes of a file to be used with run_most_recent_clip == True\n",
        "size_max = 4000"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Alv-tSgo5hMQ"
      },
      "source": [
        "#@title Imports\n",
        "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "import pandas as pd\n",
        "from xml.etree import ElementTree\n",
        "from PIL import Image\n",
        "# from mrcnn.utils import Dataset\n",
        "from matplotlib.patches import Rectangle\n",
        "import random\n",
        "import math\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "import sys\n",
        "import statistics\n",
        "import PIL\n",
        "import torchvision\n",
        "import torch\n",
        "torch.set_grad_enabled(False)\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "import traceback\n",
        "import os\n",
        "from os import listdir\n",
        "import shutil\n",
        "# from shutil import copyfile\n",
        "from scipy import signal\n",
        "from skimage import data, img_as_float\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from tensorflow.keras.models import load_model\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from IPython.display import display, Javascript, Image, HTML\n",
        "import matplotlib.pyplot as pyplot\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import io\n",
        "import html\n",
        "\n",
        "import imgaug  # https://github.com/aleju/imgaug (pip3 install imageaug)\n",
        "\n",
        "if use_youtube_link == True:\n",
        "  !pip install youtube_dl\n",
        "  import youtube_dl"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIzP-YY8Uw0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe03bd1-ff4b-405d-c82a-efd0ce64eb79"
      },
      "source": [
        "# clone darknet repo\n",
        "!git clone https://github.com/AlexeyAB/darknet\n",
        "\n",
        "# if already_made == False:\n",
        "# change makefile to have GPU, OPENCV and LIBSO enabled\n",
        "%cd darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
        "!sed -i 's/LIBSO=0/LIBSO=1/' Makefile\n",
        "\n",
        "!make"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'darknet' already exists and is not an empty directory.\n",
            "/content/darknet\n",
            "chmod +x *.sh\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/image_opencv.cpp -o obj/image_opencv.o\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid draw_detections_cv_v3(void**, detection*, int, float, char**, image**, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:926:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Krgb\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "                 float \u001b[01;35m\u001b[Krgb\u001b[m\u001b[K[3];\n",
            "                       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid draw_train_loss(char*, void**, int, float, float, int, int, float, int, char*, float, int, int, double)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1127:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "             \u001b[01;35m\u001b[Kif\u001b[m\u001b[K (iteration_old == 0)\n",
            "             \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1130:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "          \u001b[01;36m\u001b[Kif\u001b[m\u001b[K (iteration_old != 0){\n",
            "          \u001b[01;36m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid cv_draw_object(image, float*, int, int, int*, float*, int*, int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1424:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbuff\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         char \u001b[01;35m\u001b[Kbuff\u001b[m\u001b[K[100];\n",
            "              \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1400:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kit_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kit_tb_res\u001b[m\u001b[K = cv::createTrackbar(it_trackbar_name, window_name, &it_trackbar_value, 1000);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1404:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Klr_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Klr_tb_res\u001b[m\u001b[K = cv::createTrackbar(lr_trackbar_name, window_name, &lr_trackbar_value, 20);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1408:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcl_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kcl_tb_res\u001b[m\u001b[K = cv::createTrackbar(cl_trackbar_name, window_name, &cl_trackbar_value, classes-1);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/image_opencv.cpp:1411:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbo_tb_res\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kbo_tb_res\u001b[m\u001b[K = cv::createTrackbar(bo_trackbar_name, window_name, boxonly, 1);\n",
            "         \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/http_stream.cpp -o obj/http_stream.o\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kbool JSON_sender::write(const char*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:253:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                 int \u001b[01;35m\u001b[Kn\u001b[m\u001b[K = _write(client, outputbuf, outlen);\n",
            "                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kbool MJPG_sender::write(const cv::Mat&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:511:113:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%zu\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Ksize_t\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "                 sprintf(head, \"--mjpegstream\\r\\nContent-Type: image/jpeg\\r\\nContent-Length: %zu\\r\\n\\r\\n\", outlen\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid set_track_id(detection*, int, float, float, float, int, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:867:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (int i = 0; \u001b[01;35m\u001b[Ki < v.size()\u001b[m\u001b[K; ++i) {\n",
            "                         \u001b[01;35m\u001b[K~~^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:875:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int old_id = 0; \u001b[01;35m\u001b[Kold_id < old_dets.size()\u001b[m\u001b[K; ++old_id) {\n",
            "                          \u001b[01;35m\u001b[K~~~~~~~^~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:894:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int index = 0; \u001b[01;35m\u001b[Kindex < new_dets_num*old_dets.size()\u001b[m\u001b[K; ++index) {\n",
            "                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/http_stream.cpp:930:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     if (\u001b[01;35m\u001b[Kold_dets_dq.size() > deque_size\u001b[m\u001b[K) old_dets_dq.pop_front();\n",
            "         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/gemm.c -o obj/gemm.o\n",
            "\u001b[01m\u001b[K./src/gemm.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kconvolution_2d\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/gemm.c:2044:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kout_w\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     const int \u001b[01;35m\u001b[Kout_w\u001b[m\u001b[K = (w + 2 * pad - ksize) / stride + 1;    // output_width=input_width for stride=1 and pad=1\n",
            "               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gemm.c:2043:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kout_h\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     const int \u001b[01;35m\u001b[Kout_h\u001b[m\u001b[K = (h + 2 * pad - ksize) / stride + 1;    // output_height=input_height for stride=1 and pad=1\n",
            "               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/utils.c -o obj/utils.o\n",
            "\u001b[01m\u001b[K./src/utils.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcustom_hash\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/utils.c:1045:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around assignment used as truth value [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            "     while (\u001b[01;35m\u001b[Kc\u001b[m\u001b[K = *str++)\n",
            "            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/dark_cuda.c -o obj/dark_cuda.o\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcudnn_check_error_extended\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:231:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[KcudaError_t {aka enum cudaError}\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum <anonymous>\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "         if (status \u001b[01;35m\u001b[K!=\u001b[m\u001b[K CUDNN_STATUS_SUCCESS)\n",
            "                    \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcublas_check_error_extended\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:265:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between ‘\u001b[01m\u001b[KcudaError_t {aka enum cudaError}\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kenum cudaError_enum\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wenum-compare\u001b[m\u001b[K]\n",
            "       if (status \u001b[01;35m\u001b[K!=\u001b[m\u001b[K CUDA_SUCCESS)\n",
            "                  \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kpre_allocate_pinned_memory\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:396:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%u\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\"pre_allocate: size = \u001b[01;35m\u001b[K%Iu\u001b[m\u001b[K MB, num_of_blocks = %Iu, block_size = %Iu MB \\n\",\n",
            "                                      \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                      \u001b[32m\u001b[K%Ilu\u001b[m\u001b[K\n",
            "             \u001b[32m\u001b[Ksize / (1024*1024)\u001b[m\u001b[K, num_of_blocks, pinned_block_size / (1024 * 1024));\n",
            "             \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K          \n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:396:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%u\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\"pre_allocate: size = %Iu MB, num_of_blocks = \u001b[01;35m\u001b[K%Iu\u001b[m\u001b[K, block_size = %Iu MB \\n\",\n",
            "                                                              \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                                              \u001b[32m\u001b[K%Ilu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:396:82:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%u\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\"pre_allocate: size = %Iu MB, num_of_blocks = %Iu, block_size = \u001b[01;35m\u001b[K%Iu\u001b[m\u001b[K MB \\n\",\n",
            "                                                                                \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                                                                \u001b[32m\u001b[K%Ilu\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:406:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "                 printf(\" Allocated \u001b[01;35m\u001b[K%d\u001b[m\u001b[K pinned block \\n\", pinned_block_size);\n",
            "                                    \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                    \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcuda_make_array_pinned_preallocated\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:427:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\"\\n Pinned block_id = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, filled = %f %% \\n\", pinned_block_id, filled);\n",
            "                                          \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                          \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:442:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\"Try to allocate new pinned memory, size = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K MB \\n\", \u001b[32m\u001b[Ksize / (1024 * 1024)\u001b[m\u001b[K);\n",
            "                                                               \u001b[01;35m\u001b[K~^\u001b[m\u001b[K         \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "                                                               \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:448:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\"Try to allocate new pinned BLOCK, size = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K MB \\n\", \u001b[32m\u001b[Ksize / (1024 * 1024)\u001b[m\u001b[K);\n",
            "                                                              \u001b[01;35m\u001b[K~^\u001b[m\u001b[K         \u001b[32m\u001b[K~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "At top level:\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:288:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KswitchBlasHandle\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            " static cublasHandle_t \u001b[01;35m\u001b[KswitchBlasHandle\u001b[m\u001b[K[16];\n",
            "                       \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/dark_cuda.c:287:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KswitchBlasInit\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            " static int \u001b[01;35m\u001b[KswitchBlasInit\u001b[m\u001b[K[16] = { 0 };\n",
            "            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/convolutional_layer.c -o obj/convolutional_layer.o\n",
            "\u001b[01m\u001b[K./src/convolutional_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_convolutional_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/convolutional_layer.c:1341:32:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kt_intput_size\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                         size_t \u001b[01;35m\u001b[Kt_intput_size\u001b[m\u001b[K = binary_transpose_align_input(k, n, state.workspace, &l.t_bit_input, ldb_align, l.bit_align);\n",
            "                                \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/list.c -o obj/list.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/image.c -o obj/image.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/activations.c -o obj/activations.o\n",
            "\u001b[01m\u001b[K./src/activations.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kactivate\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KRELU6\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kswitch\u001b[m\u001b[K(a){\n",
            "     \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KSWISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KMISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KHARD_MISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KNORM_CHAN\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KNORM_CHAN_SOFTMAX\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:79:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KNORM_CHAN_SOFTMAX_MAXVAL\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kgradient\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/activations.c:310:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KSWISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kswitch\u001b[m\u001b[K(a){\n",
            "     \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/activations.c:310:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KMISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K./src/activations.c:310:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KHARD_MISH\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/im2col.c -o obj/im2col.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/col2im.c -o obj/col2im.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/blas.c -o obj/blas.o\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbackward_shortcut_multilayer_cpu\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:207:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kout_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                 int \u001b[01;35m\u001b[Kout_index\u001b[m\u001b[K = id;\n",
            "                     \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfind_sim\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:597:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_sim(): sim isn't found: i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, j = %d, z = %d \\n\", i, j, z);\n",
            "                                                          \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                          \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:597:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_sim(): sim isn't found: i = %d, j = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, z = %d \\n\", i, j, z);\n",
            "                                                                  \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:597:75:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_sim(): sim isn't found: i = %d, j = %d, z = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, j, z);\n",
            "                                                                          \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                          \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfind_P_constrastive\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:611:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_P_constrastive(): P isn't found: i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, j = %d, z = %d \\n\", i, j, z);\n",
            "                                                                   \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                   \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:611:76:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_P_constrastive(): P isn't found: i = %d, j = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, z = %d \\n\", i, j, z);\n",
            "                                                                           \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                           \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:611:84:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         printf(\" Error: find_P_constrastive(): P isn't found: i = %d, j = %d, z = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, j, z);\n",
            "                                                                                   \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                                   \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KP_constrastive_f\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:651:79:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, l = %d \\n\", i, l);\n",
            "                                                                              \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:651:87:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = %d, l = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, l);\n",
            "                                                                                      \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                                      \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KP_constrastive\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas.c:785:79:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K, l = %d \\n\", i, l);\n",
            "                                                                              \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/blas.c:785:87:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Ksize_t {aka long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "         fprintf(stderr, \" Error: in P_constrastive must be i != l, while i = %d, l = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K \\n\", i, l);\n",
            "                                                                                      \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                                      \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/crop_layer.c -o obj/crop_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/dropout_layer.c -o obj/dropout_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/maxpool_layer.c -o obj/maxpool_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/softmax_layer.c -o obj/softmax_layer.o\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_contrastive_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:203:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 9 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "     fprintf(stderr, \"contrastive %4d x%4d x%4d x emb_size %4d x batch: %4d  classes = %4d, step = \u001b[01;35m\u001b[K%4d\u001b[m\u001b[K \\n\", w, h, l.n, l.embedding_size, batch, l.classes, step);\n",
            "                                                                                                   \u001b[01;35m\u001b[K~~^\u001b[m\u001b[K\n",
            "                                                                                                   \u001b[32m\u001b[K%4ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_contrastive_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:244:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kmax_truth\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "                     float \u001b[01;35m\u001b[Kmax_truth\u001b[m\u001b[K = 0;\n",
            "                           \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/softmax_layer.c:423:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%d\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Ksize_t {aka const long unsigned int}\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "             printf(\" Error: too large number of bboxes: contr_size = \u001b[01;35m\u001b[K%d\u001b[m\u001b[K > max_contr_size  = %d \\n\", contr_size, max_contr_size);\n",
            "                                                                      \u001b[01;35m\u001b[K~^\u001b[m\u001b[K\n",
            "                                                                      \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/data.c -o obj/data.o\n",
            "\u001b[01m\u001b[K./src/data.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kload_data_detection\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/data.c:1297:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                 int k, \u001b[01;35m\u001b[Kx\u001b[m\u001b[K, y;\n",
            "                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/data.c:1090:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kr_scale\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     float r1 = 0, r2 = 0, r3 = 0, r4 = 0, \u001b[01;35m\u001b[Kr_scale\u001b[m\u001b[K = 0;\n",
            "                                           \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/matrix.c -o obj/matrix.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/network.c -o obj/network.o\n",
            "\u001b[01m\u001b[K./src/network.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktrain_network_waitkey\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/network.c:434:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kema_period\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         int \u001b[01;35m\u001b[Kema_period\u001b[m\u001b[K = (net.max_batches - ema_start_point - 1000) * (1.0 - net.ema_alpha);\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/network.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_network\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/network.c:659:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Knet->input_pinned_cpu, size * sizeof(float), cudaHostRegisterMapped))\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/network.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/connected_layer.c -o obj/connected_layer.o\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_connected_layer_gpu\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:346:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kone\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kone\u001b[m\u001b[K = 1;    // alpha[0], beta[0]\n",
            "           \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:344:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float * \u001b[01;35m\u001b[Kc\u001b[m\u001b[K = l.output_gpu;\n",
            "             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:343:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kb\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float * \u001b[01;35m\u001b[Kb\u001b[m\u001b[K = l.weights_gpu;\n",
            "             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:342:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ka\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float * \u001b[01;35m\u001b[Ka\u001b[m\u001b[K = state.input;\n",
            "             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:341:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kn\u001b[m\u001b[K = l.outputs;\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:340:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kk\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kk\u001b[m\u001b[K = l.inputs;\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/connected_layer.c:339:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Km\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Km\u001b[m\u001b[K = l.batch;\n",
            "         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/cost_layer.c -o obj/cost_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/parser.c -o obj/parser.o\n",
            "\u001b[01m\u001b[K./src/parser.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kparse_network_cfg_custom\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/parser.c:1689:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Knet.input_pinned_cpu, size * sizeof(float), cudaHostRegisterMapped)) net.input_pinned_cpu_flag = 1;\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activation_layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/parser.c:6\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/parser.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kget_classes_multipliers\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/parser.c:431:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kargument 1 range [18446744071562067968, 18446744073709551615] exceeds maximum object size 9223372036854775807 [\u001b[01;35m\u001b[K-Walloc-size-larger-than=\u001b[m\u001b[K]\n",
            "         \u001b[01;35m\u001b[Kclasses_multipliers = (float *)calloc(classes_counters, sizeof(float))\u001b[m\u001b[K;\n",
            "         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K./src/parser.c:3:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/stdlib.h:541:14:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin a call to allocation function ‘\u001b[01m\u001b[Kcalloc\u001b[m\u001b[K’ declared here\n",
            " extern void *\u001b[01;36m\u001b[Kcalloc\u001b[m\u001b[K (size_t __nmemb, size_t __size)\n",
            "              \u001b[01;36m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/option_list.c -o obj/option_list.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/darknet.c -o obj/darknet.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/detection_layer.c -o obj/detection_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/captcha.c -o obj/captcha.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/route_layer.c -o obj/route_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/writing.c -o obj/writing.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/box.c -o obj/box.o\n",
            "\u001b[01m\u001b[K./src/box.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbox_iou_kind\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/box.c:154:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kenumeration value ‘\u001b[01m\u001b[KMSE\u001b[m\u001b[K’ not handled in switch [\u001b[01;35m\u001b[K-Wswitch\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kswitch\u001b[m\u001b[K(iou_kind) {\n",
            "     \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/box.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdiounms_sort\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/box.c:898:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbeta_prob\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                     float \u001b[01;35m\u001b[Kbeta_prob\u001b[m\u001b[K = pow(dets[j].prob[k], 2) / sum_prob;\n",
            "                           \u001b[01;35m\u001b[K^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/box.c:897:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kalpha_prob\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "                     float \u001b[01;35m\u001b[Kalpha_prob\u001b[m\u001b[K = pow(dets[i].prob[k], 2) / sum_prob;\n",
            "                           \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/nightmare.c -o obj/nightmare.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/normalization_layer.c -o obj/normalization_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/avgpool_layer.c -o obj/avgpool_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/coco.c -o obj/coco.o\n",
            "\u001b[01m\u001b[K./src/coco.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvalidate_coco_recall\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/coco.c:248:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kbase\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     char *\u001b[01;35m\u001b[Kbase\u001b[m\u001b[K = \"results/comp4_det_test_\";\n",
            "           \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/dice.c -o obj/dice.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/yolo.c -o obj/yolo.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/detector.c -o obj/detector.o\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktrain_detector\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:386:72:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K&&\u001b[m\u001b[K’ within ‘\u001b[01m\u001b[K||\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            "             \u001b[01;35m\u001b[K(iteration >= (iter_save + 1000) || iteration % 1000 == 0) && net.max_batches < 10000\u001b[m\u001b[K)\n",
            "             \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kprint_cocos\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:486:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat not a string literal and no format arguments [\u001b[01;35m\u001b[K-Wformat-security\u001b[m\u001b[K]\n",
            "                 fprintf(fp, \u001b[01;35m\u001b[Kbuff\u001b[m\u001b[K);\n",
            "                             \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Keliminate_bdd\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:579:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kstatement with no effect [\u001b[01;35m\u001b[K-Wunused-value\u001b[m\u001b[K]\n",
            "                     \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K (k; buf[k + n] != '\\0'; k++)\n",
            "                     \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvalidate_detector\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:700:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kmkd2\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         int \u001b[01;35m\u001b[Kmkd2\u001b[m\u001b[K = make_directory(buff2, 0777);\n",
            "             \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:698:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kmkd\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         int \u001b[01;35m\u001b[Kmkd\u001b[m\u001b[K = make_directory(buff, 0777);\n",
            "             \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvalidate_detector_map\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:1332:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kclass_recall\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         float \u001b[01;35m\u001b[Kclass_recall\u001b[m\u001b[K = (float)tp_for_thresh_per_class[i] / ((float)tp_for_thresh_per_class[i] + (float)(truth_classes_count[i] - tp_for_thresh_per_class[i]));\n",
            "               \u001b[01;35m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:1331:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kclass_precision\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         float \u001b[01;35m\u001b[Kclass_precision\u001b[m\u001b[K = (float)tp_for_thresh_per_class[i] / ((float)tp_for_thresh_per_class[i] + (float)fp_for_thresh_per_class[i]);\n",
            "               \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/detector.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdraw_object\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/detector.c:1867:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kinv_loss\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "             float \u001b[01;35m\u001b[Kinv_loss\u001b[m\u001b[K = 1.0 / max_val_cmp(0.01, avg_loss);\n",
            "                   \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/layer.c -o obj/layer.o\n",
            "\u001b[01m\u001b[K./src/layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfree_layer_custom\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/layer.c:208:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Ksuggest parentheses around ‘\u001b[01m\u001b[K&&\u001b[m\u001b[K’ within ‘\u001b[01m\u001b[K||\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wparentheses\u001b[m\u001b[K]\n",
            "     if (l.delta_gpu && (l.optimized_memory < 1 || \u001b[01;35m\u001b[Kl.keep_delta_gpu && l.optimized_memory < 3\u001b[m\u001b[K)) cuda_free(l.delta_gpu), l.delta_gpu = NULL;\n",
            "                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/compare.c -o obj/compare.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/classifier.c -o obj/classifier.o\n",
            "\u001b[01m\u001b[K./src/classifier.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktrain_classifier\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/classifier.c:146:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcount\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kcount\u001b[m\u001b[K = 0;\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/classifier.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kpredict_classifier\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/classifier.c:855:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktime\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     clock_t \u001b[01;35m\u001b[Ktime\u001b[m\u001b[K;\n",
            "             \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/classifier.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdemo_classifier\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/classifier.c:1287:49:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktval_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         struct timeval tval_before, tval_after, \u001b[01;35m\u001b[Ktval_result\u001b[m\u001b[K;\n",
            "                                                 \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/classifier.c:1287:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktval_after\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         struct timeval tval_before, \u001b[01;35m\u001b[Ktval_after\u001b[m\u001b[K, tval_result;\n",
            "                                     \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/local_layer.c -o obj/local_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/swag.c -o obj/swag.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/shortcut_layer.c -o obj/shortcut_layer.o\n",
            "\u001b[01m\u001b[K./src/shortcut_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_shortcut_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/shortcut_layer.c:55:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kscale\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         float \u001b[01;35m\u001b[Kscale\u001b[m\u001b[K = sqrt(2. / l.nweights);\n",
            "               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/activation_layer.c -o obj/activation_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/rnn_layer.c -o obj/rnn_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/gru_layer.c -o obj/gru_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/rnn.c -o obj/rnn.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/rnn_vid.c -o obj/rnn_vid.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/crnn_layer.c -o obj/crnn_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/demo.c -o obj/demo.o\n",
            "\u001b[01m\u001b[K./src/demo.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kdetect_in_thread\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/demo.c:98:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "         layer \u001b[01;35m\u001b[Kl\u001b[m\u001b[K = net.layers[net.n - 1];\n",
            "               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/tag.c -o obj/tag.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/cifar.c -o obj/cifar.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/go.c -o obj/go.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/batchnorm_layer.c -o obj/batchnorm_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/art.c -o obj/art.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/region_layer.c -o obj/region_layer.o\n",
            "\u001b[01m\u001b[K./src/region_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_region_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/region_layer.c:59:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kold_h\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kold_h\u001b[m\u001b[K = l->h;\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/region_layer.c:58:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kold_w\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int \u001b[01;35m\u001b[Kold_w\u001b[m\u001b[K = l->w;\n",
            "         \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/reorg_layer.c -o obj/reorg_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/reorg_old_layer.c -o obj/reorg_old_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/super.c -o obj/super.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/voxel.c -o obj/voxel.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/tree.c -o obj/tree.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/yolo_layer.c -o obj/yolo_layer.o\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:68:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.output, batch*l.outputs*sizeof(float), cudaHostRegisterMapped)) l.output_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:75:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.delta, batch*l.outputs*sizeof(float), cudaHostRegisterMapped)) l.delta_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:106:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->output, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:115:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->delta, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/activations.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/layer.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/yolo_layer.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kprocess_batch\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:426:25:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kbest_match_t\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "                     int \u001b[01;35m\u001b[Kbest_match_t\u001b[m\u001b[K = 0;\n",
            "                         \u001b[01;35m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kforward_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:707:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kavg_anyobj\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kavg_anyobj\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:706:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kavg_obj\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kavg_obj\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:705:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kavg_cat\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Kavg_cat\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:704:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Krecall75\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Krecall75\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:703:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Krecall\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Krecall\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:702:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_ciou_loss\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_ciou_loss\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:701:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_diou_loss\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_diou_loss\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:698:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_ciou\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_ciou\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:697:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_diou\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_diou\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:696:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Ktot_giou\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     float \u001b[01;35m\u001b[Ktot_giou\u001b[m\u001b[K = 0;\n",
            "           \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/yolo_layer.c:668:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int b, \u001b[01;35m\u001b[Kn\u001b[m\u001b[K;\n",
            "            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/gaussian_yolo_layer.c -o obj/gaussian_yolo_layer.o\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmake_gaussian_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:72:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.output, batch*l.outputs * sizeof(float), cudaHostRegisterMapped)) l.output_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:79:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "     if (cudaSuccess == cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl.delta, batch*l.outputs * sizeof(float), cudaHostRegisterMapped)) l.delta_pinned = 1;\n",
            "                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kresize_gaussian_yolo_layer\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:111:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->output, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K./src/gaussian_yolo_layer.c:120:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[KcudaHostAlloc\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K-Wincompatible-pointer-types\u001b[m\u001b[K]\n",
            "         if (cudaSuccess != cudaHostAlloc(\u001b[01;35m\u001b[K&\u001b[m\u001b[Kl->delta, l->batch*l->outputs * sizeof(float), cudaHostRegisterMapped)) {\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime.h:96:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinclude/darknet.h:41\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K./src/gaussian_yolo_layer.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/include/cuda_runtime_api.h:4707:39:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kvoid **\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kfloat **\u001b[m\u001b[K’\n",
            " extern __host__ cudaError_t CUDARTAPI \u001b[01;36m\u001b[KcudaHostAlloc\u001b[m\u001b[K(void **pHost, size_t size, unsigned int flags);\n",
            "                                       \u001b[01;36m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/upsample_layer.c -o obj/upsample_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/lstm_layer.c -o obj/lstm_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/conv_lstm_layer.c -o obj/conv_lstm_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/scale_channels_layer.c -o obj/scale_channels_layer.o\n",
            "gcc -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -c ./src/sam_layer.c -o obj/sam_layer.o\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/convolutional_kernels.cu -o obj/convolutional_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/activation_kernels.cu -o obj/activation_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "./src/activation_kernels.cu(263): warning: variable \"MISH_THRESHOLD\" was declared but never referenced\n",
            "\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/im2col_kernels.cu -o obj/im2col_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/col2im_kernels.cu -o obj/col2im_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/blas_kernels.cu -o obj/blas_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1086): warning: variable \"out_index\" was declared but never referenced\n",
            "\n",
            "./src/blas_kernels.cu(1130): warning: variable \"step\" was set but never used\n",
            "\n",
            "./src/blas_kernels.cu(1736): warning: variable \"stage_id\" was declared but never referenced\n",
            "\n",
            "\u001b[01m\u001b[K./src/blas_kernels.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid backward_shortcut_multilayer_gpu(int, int, int, int*, float**, float*, float*, float*, float*, int, float*, float**, WEIGHTS_NORMALIZATION_T)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/blas_kernels.cu:1130:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kstep\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kint \u001b[m\u001b[Kstep = 0;\n",
            "     \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/crop_layer_kernels.cu -o obj/crop_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/dropout_layer_kernels.cu -o obj/dropout_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/maxpool_layer_kernels.cu -o obj/maxpool_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/network_kernels.cu -o obj/network_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "./src/network_kernels.cu(364): warning: variable \"l\" was declared but never referenced\n",
            "\n",
            "\u001b[01m\u001b[K./src/network_kernels.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfloat train_network_datum_gpu(network, float*, float*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K./src/network_kernels.cu:364:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kl\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[K \u001b[m\u001b[K layer l = net.layers[net.n - 1];\n",
            "       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "nvcc -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=[sm_50,compute_50] -gencode arch=compute_52,code=[sm_52,compute_52] -gencode arch=compute_61,code=[sm_61,compute_61] -gencode arch=compute_70,code=[sm_70,compute_70] -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF --compiler-options \"-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC\" -c ./src/avgpool_layer_kernels.cu -o obj/avgpool_layer_kernels.o\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC obj/image_opencv.o obj/http_stream.o obj/gemm.o obj/utils.o obj/dark_cuda.o obj/convolutional_layer.o obj/list.o obj/image.o obj/activations.o obj/im2col.o obj/col2im.o obj/blas.o obj/crop_layer.o obj/dropout_layer.o obj/maxpool_layer.o obj/softmax_layer.o obj/data.o obj/matrix.o obj/network.o obj/connected_layer.o obj/cost_layer.o obj/parser.o obj/option_list.o obj/darknet.o obj/detection_layer.o obj/captcha.o obj/route_layer.o obj/writing.o obj/box.o obj/nightmare.o obj/normalization_layer.o obj/avgpool_layer.o obj/coco.o obj/dice.o obj/yolo.o obj/detector.o obj/layer.o obj/compare.o obj/classifier.o obj/local_layer.o obj/swag.o obj/shortcut_layer.o obj/activation_layer.o obj/rnn_layer.o obj/gru_layer.o obj/rnn.o obj/rnn_vid.o obj/crnn_layer.o obj/demo.o obj/tag.o obj/cifar.o obj/go.o obj/batchnorm_layer.o obj/art.o obj/region_layer.o obj/reorg_layer.o obj/reorg_old_layer.o obj/super.o obj/voxel.o obj/tree.o obj/yolo_layer.o obj/gaussian_yolo_layer.o obj/upsample_layer.o obj/lstm_layer.o obj/conv_lstm_layer.o obj/scale_channels_layer.o obj/sam_layer.o obj/convolutional_kernels.o obj/activation_kernels.o obj/im2col_kernels.o obj/col2im_kernels.o obj/blas_kernels.o obj/crop_layer_kernels.o obj/dropout_layer_kernels.o obj/maxpool_layer_kernels.o obj/network_kernels.o obj/avgpool_layer_kernels.o -o darknet -lm -pthread `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv` -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -L/usr/local/cudnn/lib64 -lcudnn -lstdc++\n",
            "g++ -std=c++11 -shared -std=c++11 -fvisibility=hidden -DLIB_EXPORTS -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC ./obj/image_opencv.o ./obj/http_stream.o ./obj/gemm.o ./obj/utils.o ./obj/dark_cuda.o ./obj/convolutional_layer.o ./obj/list.o ./obj/image.o ./obj/activations.o ./obj/im2col.o ./obj/col2im.o ./obj/blas.o ./obj/crop_layer.o ./obj/dropout_layer.o ./obj/maxpool_layer.o ./obj/softmax_layer.o ./obj/data.o ./obj/matrix.o ./obj/network.o ./obj/connected_layer.o ./obj/cost_layer.o ./obj/parser.o ./obj/option_list.o ./obj/darknet.o ./obj/detection_layer.o ./obj/captcha.o ./obj/route_layer.o ./obj/writing.o ./obj/box.o ./obj/nightmare.o ./obj/normalization_layer.o ./obj/avgpool_layer.o ./obj/coco.o ./obj/dice.o ./obj/yolo.o ./obj/detector.o ./obj/layer.o ./obj/compare.o ./obj/classifier.o ./obj/local_layer.o ./obj/swag.o ./obj/shortcut_layer.o ./obj/activation_layer.o ./obj/rnn_layer.o ./obj/gru_layer.o ./obj/rnn.o ./obj/rnn_vid.o ./obj/crnn_layer.o ./obj/demo.o ./obj/tag.o ./obj/cifar.o ./obj/go.o ./obj/batchnorm_layer.o ./obj/art.o ./obj/region_layer.o ./obj/reorg_layer.o ./obj/reorg_old_layer.o ./obj/super.o ./obj/voxel.o ./obj/tree.o ./obj/yolo_layer.o ./obj/gaussian_yolo_layer.o ./obj/upsample_layer.o ./obj/lstm_layer.o ./obj/conv_lstm_layer.o ./obj/scale_channels_layer.o ./obj/sam_layer.o ./obj/convolutional_kernels.o ./obj/activation_kernels.o ./obj/im2col_kernels.o ./obj/col2im_kernels.o ./obj/blas_kernels.o ./obj/crop_layer_kernels.o ./obj/dropout_layer_kernels.o ./obj/maxpool_layer_kernels.o ./obj/network_kernels.o ./obj/avgpool_layer_kernels.o src/yolo_v2_class.cpp -o libdarknet.so -lm -pthread `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv` -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -L/usr/local/cudnn/lib64 -lcudnn -lstdc++\n",
            "In file included from \u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:2:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid track_kalman_t::clear_old_states()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:878:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                 if ((result_vec_pred[state_id].x > img_size.width) ||\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:879:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                     (result_vec_pred[state_id].y > img_size.height))\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ktrack_kalman_t::tst_t track_kalman_t::get_state_id(bbox_t, std::vector<bool>&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:899:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::predict()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:989:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::correct(std::vector<bbox_t>)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:1024:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> Detector::tracking_id(std::vector<bbox_t>, bool, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:370:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         if (\u001b[01;35m\u001b[Kprev_bbox_vec_deque.size() > frames_story\u001b[m\u001b[K) prev_bbox_vec_deque.pop_back();\n",
            "             \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:385:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                     if (\u001b[01;35m\u001b[Kcur_dist < max_dist\u001b[m\u001b[K && (k.track_id == 0 || dist_vec[m] > cur_dist)) {\n",
            "                         \u001b[01;35m\u001b[K~~~~~~~~~^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_v2_class.cpp:409:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         if (\u001b[01;35m\u001b[Kprev_bbox_vec_deque.size() > frames_story\u001b[m\u001b[K) prev_bbox_vec_deque.pop_back();\n",
            "             \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -std=c++11 -std=c++11 -Iinclude/ -I3rdparty/stb/include -DOPENCV `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv` -DGPU -I/usr/local/cuda/include/ -DCUDNN -DCUDNN_HALF -Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC -Ofast -DOPENCV -DGPU -DCUDNN -I/usr/local/cudnn/include -DCUDNN_HALF -fPIC -o uselib src/yolo_console_dll.cpp -lm -pthread `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv` -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand -L/usr/local/cudnn/lib64 -lcudnn -lstdc++ -L ./ -l:libdarknet.so\n",
            "In file included from \u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:23:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid track_kalman_t::clear_old_states()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:878:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                 if ((result_vec_pred[state_id].x > img_size.width) ||\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:879:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                     (result_vec_pred[state_id].y > img_size.height))\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ktrack_kalman_t::tst_t track_kalman_t::get_state_id(bbox_t, std::vector<bool>&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:899:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::predict()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:989:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kstd::vector<bbox_t> track_kalman_t::correct(std::vector<bbox_t>)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kinclude/yolo_v2_class.hpp:1024:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "         for (size_t i = 0; \u001b[01;35m\u001b[Ki < max_objects\u001b[m\u001b[K; ++i)\n",
            "                            \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid draw_boxes(cv::Mat, std::vector<bbox_t>, std::vector<std::__cxx11::basic_string<char> >, int, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:192:46:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "             int max_width = (\u001b[01;35m\u001b[Ktext_size.width > i.w + 2\u001b[m\u001b[K) ? text_size.width : (i.w + 2);\n",
            "                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:201:62:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "                 int const max_width_3d = (\u001b[01;35m\u001b[Ktext_size_3d.width > i.w + 2\u001b[m\u001b[K) ? text_size_3d.width : (i.w + 2);\n",
            "                                           \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ksrc/yolo_console_dll.cpp:183:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kcolors\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     int const \u001b[01;35m\u001b[Kcolors\u001b[m\u001b[K[6][3] = { { 1,0,1 },{ 0,0,1 },{ 0,1,1 },{ 0,1,0 },{ 1,1,0 },{ 1,0,0 } };\n",
            "               \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTn9rXjiz1Pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f155f7-079d-48c1-8b40-cf0c92d475d2"
      },
      "source": [
        "# Copies over Weights, data file, configuration file and two name files.\n",
        "%cd '/content/'\n",
        "\n",
        "# Weights File\n",
        "# Loads the file into Content\n",
        "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SFR5Hrt3LqhzPjfrG-kwPYO5Yy2fg7C8' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1SFR5Hrt3LqhzPjfrG-kwPYO5Yy2fg7C8\" -O custom-yolov4-detector_best.weights && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO\" -O custom-yolov4-detector_best.weights && rm -rf /tmp/cookies.txt\n",
        "# Moves the file from Content to Destination\n",
        "%cp custom-yolov4-detector_best.weights /content/darknet/custom-yolov4-detector_best.weights\n",
        "\n",
        "# Object Data File\n",
        "# Loads the file into Content\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mo394Soqlfyza9I-tk73D8LfzaBu2Qte' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1mo394Soqlfyza9I-tk73D8LfzaBu2Qte\" -O obj_4.data && rm -rf /tmp/cookies.txt\n",
        "# Moves the file from Content to Destination\n",
        "%cp obj_4.data /content/darknet/cfg/obj_4.data\n",
        "\n",
        "# Detection Config File\n",
        "# Loads the file into Content\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW\" -O custom-yolov4-detector.cfg && rm -rf /tmp/cookies.txt\n",
        "# Moves the file from Content to Destination\n",
        "%cp custom-yolov4-detector.cfg /content/darknet/cfg/custom-yolov4-detector.cfg\n",
        "\n",
        "# Coco Names File\n",
        "# Loads the file into Content\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG\" -O coco.names && rm -rf /tmp/cookies.txt\n",
        "# Moves the file from Content to Destination\n",
        "\n",
        "%cp coco.names darknet/data/coco.names\n",
        "%cp coco.names darknet/data/obj.names"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2021-05-04 03:34:23--  https://docs.google.com/uc?export=download&confirm=BNkn&id=1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.218.102, 172.217.218.138, 172.217.218.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.218.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-8k-docs.googleusercontent.com/docs/securesc/6jrhelu8cpn23ijsehhpae53fm9avfsh/mgsn24b1236cu5m8cg4edja7s4tvh0h5/1620099225000/15469335548275755091/10733433490125714532Z/1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO?e=download [following]\n",
            "--2021-05-04 03:34:23--  https://doc-0o-8k-docs.googleusercontent.com/docs/securesc/6jrhelu8cpn23ijsehhpae53fm9avfsh/mgsn24b1236cu5m8cg4edja7s4tvh0h5/1620099225000/15469335548275755091/10733433490125714532Z/1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO?e=download\n",
            "Resolving doc-0o-8k-docs.googleusercontent.com (doc-0o-8k-docs.googleusercontent.com)... 173.194.79.132, 2a00:1450:4013:c05::84\n",
            "Connecting to doc-0o-8k-docs.googleusercontent.com (doc-0o-8k-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=qr3gd4i3e5vf4&continue=https://doc-0o-8k-docs.googleusercontent.com/docs/securesc/6jrhelu8cpn23ijsehhpae53fm9avfsh/mgsn24b1236cu5m8cg4edja7s4tvh0h5/1620099225000/15469335548275755091/10733433490125714532Z/1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO?e%3Ddownload&hash=vc55sbsldlknge9k5fj2jai8m74cibp9 [following]\n",
            "--2021-05-04 03:34:24--  https://docs.google.com/nonceSigner?nonce=qr3gd4i3e5vf4&continue=https://doc-0o-8k-docs.googleusercontent.com/docs/securesc/6jrhelu8cpn23ijsehhpae53fm9avfsh/mgsn24b1236cu5m8cg4edja7s4tvh0h5/1620099225000/15469335548275755091/10733433490125714532Z/1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO?e%3Ddownload&hash=vc55sbsldlknge9k5fj2jai8m74cibp9\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.218.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0o-8k-docs.googleusercontent.com/docs/securesc/6jrhelu8cpn23ijsehhpae53fm9avfsh/mgsn24b1236cu5m8cg4edja7s4tvh0h5/1620099225000/15469335548275755091/10733433490125714532Z/1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO?e=download&nonce=qr3gd4i3e5vf4&user=10733433490125714532Z&hash=rrm4hcrfadc1cm263mssdce42pgjeg8e [following]\n",
            "--2021-05-04 03:34:24--  https://doc-0o-8k-docs.googleusercontent.com/docs/securesc/6jrhelu8cpn23ijsehhpae53fm9avfsh/mgsn24b1236cu5m8cg4edja7s4tvh0h5/1620099225000/15469335548275755091/10733433490125714532Z/1mxfy7AeCbNs6-u3f606eh0NBCF4BRUzO?e=download&nonce=qr3gd4i3e5vf4&user=10733433490125714532Z&hash=rrm4hcrfadc1cm263mssdce42pgjeg8e\n",
            "Connecting to doc-0o-8k-docs.googleusercontent.com (doc-0o-8k-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘custom-yolov4-detector_best.weights’\n",
            "\n",
            "custom-yolov4-detec     [         <=>        ] 244.22M   129MB/s    in 1.9s    \n",
            "\n",
            "2021-05-04 03:34:26 (129 MB/s) - ‘custom-yolov4-detector_best.weights’ saved [256080600]\n",
            "\n",
            "--2021-05-04 03:34:27--  https://docs.google.com/uc?export=download&confirm=&id=1mo394Soqlfyza9I-tk73D8LfzaBu2Qte\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.113, 108.177.119.139, 108.177.119.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-0k-docs.googleusercontent.com/docs/securesc/hviu1t2e769c3mssgll9et7up3he7c8p/drc0m6mbfidgg8fuvcq1ecq12vf3jut5/1620099225000/15469335548275755091/13912244723887370376Z/1mo394Soqlfyza9I-tk73D8LfzaBu2Qte?e=download [following]\n",
            "--2021-05-04 03:34:28--  https://doc-0c-0k-docs.googleusercontent.com/docs/securesc/hviu1t2e769c3mssgll9et7up3he7c8p/drc0m6mbfidgg8fuvcq1ecq12vf3jut5/1620099225000/15469335548275755091/13912244723887370376Z/1mo394Soqlfyza9I-tk73D8LfzaBu2Qte?e=download\n",
            "Resolving doc-0c-0k-docs.googleusercontent.com (doc-0c-0k-docs.googleusercontent.com)... 173.194.79.132, 2a00:1450:4013:c05::84\n",
            "Connecting to doc-0c-0k-docs.googleusercontent.com (doc-0c-0k-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=pdn9g3lh3loqu&continue=https://doc-0c-0k-docs.googleusercontent.com/docs/securesc/hviu1t2e769c3mssgll9et7up3he7c8p/drc0m6mbfidgg8fuvcq1ecq12vf3jut5/1620099225000/15469335548275755091/13912244723887370376Z/1mo394Soqlfyza9I-tk73D8LfzaBu2Qte?e%3Ddownload&hash=ruq33tvm127nb447k01aj7gobaejfg64 [following]\n",
            "--2021-05-04 03:34:28--  https://docs.google.com/nonceSigner?nonce=pdn9g3lh3loqu&continue=https://doc-0c-0k-docs.googleusercontent.com/docs/securesc/hviu1t2e769c3mssgll9et7up3he7c8p/drc0m6mbfidgg8fuvcq1ecq12vf3jut5/1620099225000/15469335548275755091/13912244723887370376Z/1mo394Soqlfyza9I-tk73D8LfzaBu2Qte?e%3Ddownload&hash=ruq33tvm127nb447k01aj7gobaejfg64\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0c-0k-docs.googleusercontent.com/docs/securesc/hviu1t2e769c3mssgll9et7up3he7c8p/drc0m6mbfidgg8fuvcq1ecq12vf3jut5/1620099225000/15469335548275755091/13912244723887370376Z/1mo394Soqlfyza9I-tk73D8LfzaBu2Qte?e=download&nonce=pdn9g3lh3loqu&user=13912244723887370376Z&hash=3g9pcp7u53cfvv8mlidisp63m9sqmnsg [following]\n",
            "--2021-05-04 03:34:28--  https://doc-0c-0k-docs.googleusercontent.com/docs/securesc/hviu1t2e769c3mssgll9et7up3he7c8p/drc0m6mbfidgg8fuvcq1ecq12vf3jut5/1620099225000/15469335548275755091/13912244723887370376Z/1mo394Soqlfyza9I-tk73D8LfzaBu2Qte?e=download&nonce=pdn9g3lh3loqu&user=13912244723887370376Z&hash=3g9pcp7u53cfvv8mlidisp63m9sqmnsg\n",
            "Connecting to doc-0c-0k-docs.googleusercontent.com (doc-0c-0k-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97 [application/octet-stream]\n",
            "Saving to: ‘obj_4.data’\n",
            "\n",
            "obj_4.data          100%[===================>]      97  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-04 03:34:28 (6.03 MB/s) - ‘obj_4.data’ saved [97/97]\n",
            "\n",
            "--2021-05-04 03:34:29--  https://docs.google.com/uc?export=download&confirm=&id=1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.119.113, 108.177.119.139, 108.177.119.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-bs-docs.googleusercontent.com/docs/securesc/d12emqrf5ng7hvfl90ii7kjmf12gi3s2/u32aia8lsiub671m6e0h94u5fiaor295/1620099225000/15469335548275755091/16541501825107516244Z/1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW?e=download [following]\n",
            "--2021-05-04 03:34:29--  https://doc-14-bs-docs.googleusercontent.com/docs/securesc/d12emqrf5ng7hvfl90ii7kjmf12gi3s2/u32aia8lsiub671m6e0h94u5fiaor295/1620099225000/15469335548275755091/16541501825107516244Z/1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW?e=download\n",
            "Resolving doc-14-bs-docs.googleusercontent.com (doc-14-bs-docs.googleusercontent.com)... 173.194.79.132, 2a00:1450:4013:c05::84\n",
            "Connecting to doc-14-bs-docs.googleusercontent.com (doc-14-bs-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=2r79spossgvuk&continue=https://doc-14-bs-docs.googleusercontent.com/docs/securesc/d12emqrf5ng7hvfl90ii7kjmf12gi3s2/u32aia8lsiub671m6e0h94u5fiaor295/1620099225000/15469335548275755091/16541501825107516244Z/1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW?e%3Ddownload&hash=jcoqir15k54s7o71d78ic88996bpta5c [following]\n",
            "--2021-05-04 03:34:29--  https://docs.google.com/nonceSigner?nonce=2r79spossgvuk&continue=https://doc-14-bs-docs.googleusercontent.com/docs/securesc/d12emqrf5ng7hvfl90ii7kjmf12gi3s2/u32aia8lsiub671m6e0h94u5fiaor295/1620099225000/15469335548275755091/16541501825107516244Z/1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW?e%3Ddownload&hash=jcoqir15k54s7o71d78ic88996bpta5c\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.119.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-14-bs-docs.googleusercontent.com/docs/securesc/d12emqrf5ng7hvfl90ii7kjmf12gi3s2/u32aia8lsiub671m6e0h94u5fiaor295/1620099225000/15469335548275755091/16541501825107516244Z/1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW?e=download&nonce=2r79spossgvuk&user=16541501825107516244Z&hash=l5t81ead625kcj9kccr0osmr5g1uq4cv [following]\n",
            "--2021-05-04 03:34:29--  https://doc-14-bs-docs.googleusercontent.com/docs/securesc/d12emqrf5ng7hvfl90ii7kjmf12gi3s2/u32aia8lsiub671m6e0h94u5fiaor295/1620099225000/15469335548275755091/16541501825107516244Z/1-4fEmuJ542QdQC9w33BNTw8mAbDVWjqW?e=download&nonce=2r79spossgvuk&user=16541501825107516244Z&hash=l5t81ead625kcj9kccr0osmr5g1uq4cv\n",
            "Connecting to doc-14-bs-docs.googleusercontent.com (doc-14-bs-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12183 (12K) [text/plain]\n",
            "Saving to: ‘custom-yolov4-detector.cfg’\n",
            "\n",
            "custom-yolov4-detec 100%[===================>]  11.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-04 03:34:30 (79.9 MB/s) - ‘custom-yolov4-detector.cfg’ saved [12183/12183]\n",
            "\n",
            "--2021-05-04 03:34:31--  https://docs.google.com/uc?export=download&confirm=&id=1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.218.100, 172.217.218.139, 172.217.218.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.218.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ddeo8la8o28943jhkl7jsps753cnif0j/8optkhcso3c7mb89epepngnib0n0hiha/1620099225000/15469335548275755091/05893291791617442435Z/1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG?e=download [following]\n",
            "--2021-05-04 03:34:31--  https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ddeo8la8o28943jhkl7jsps753cnif0j/8optkhcso3c7mb89epepngnib0n0hiha/1620099225000/15469335548275755091/05893291791617442435Z/1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG?e=download\n",
            "Resolving doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)... 173.194.79.132, 2a00:1450:4013:c05::84\n",
            "Connecting to doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=vjdjc5ebsedko&continue=https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ddeo8la8o28943jhkl7jsps753cnif0j/8optkhcso3c7mb89epepngnib0n0hiha/1620099225000/15469335548275755091/05893291791617442435Z/1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG?e%3Ddownload&hash=knhgifp09emgl596ehobu71urf3un54m [following]\n",
            "--2021-05-04 03:34:31--  https://docs.google.com/nonceSigner?nonce=vjdjc5ebsedko&continue=https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ddeo8la8o28943jhkl7jsps753cnif0j/8optkhcso3c7mb89epepngnib0n0hiha/1620099225000/15469335548275755091/05893291791617442435Z/1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG?e%3Ddownload&hash=knhgifp09emgl596ehobu71urf3un54m\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.218.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ddeo8la8o28943jhkl7jsps753cnif0j/8optkhcso3c7mb89epepngnib0n0hiha/1620099225000/15469335548275755091/05893291791617442435Z/1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG?e=download&nonce=vjdjc5ebsedko&user=05893291791617442435Z&hash=7qo7i1nd5ma9elrcusikmqhdsvg60v7g [following]\n",
            "--2021-05-04 03:34:32--  https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ddeo8la8o28943jhkl7jsps753cnif0j/8optkhcso3c7mb89epepngnib0n0hiha/1620099225000/15469335548275755091/05893291791617442435Z/1-3iHBsrWNuQaeSFoaaSIsb3uE4-mBRlG?e=download&nonce=vjdjc5ebsedko&user=05893291791617442435Z&hash=7qo7i1nd5ma9elrcusikmqhdsvg60v7g\n",
            "Connecting to doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29 [application/octet-stream]\n",
            "Saving to: ‘coco.names’\n",
            "\n",
            "coco.names          100%[===================>]      29  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-04 03:34:32 (2.03 MB/s) - ‘coco.names’ saved [29/29]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMhZLIZzSpuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8695f2e2-bee0-4999-9f42-0d2d4e864704"
      },
      "source": [
        "%cd darknet\n",
        "# from darknet import *\n",
        "!python3 darknet.py\n",
        "\n",
        "# import darknet functions to perform object detections\n",
        "from darknet import *\n",
        "\n",
        "# Loads YOLOv4 Architecture Network\n",
        "network, class_names, class_colors = load_network(\"cfg/custom-yolov4-detector.cfg\", \"cfg/obj_4.data\", \"custom-yolov4-detector_best.weights\")\n",
        "width = network_width(network)\n",
        "height = network_height(network)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/darknet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "ttScHtsXdr7U"
      },
      "source": [
        "#@title Hour Minute Second String\n",
        "def hms_string(sec_elapsed):\n",
        "  # Nicely formatted time string\n",
        "  h = int(sec_elapsed / (60 * 60))\n",
        "  m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "  s = int(sec_elapsed % 60)\n",
        "  ms = int(sec_elapsed * 1000 % 1000)\n",
        "  return f\"{h}:{m:>02}:{s:>02}:{ms:>03}\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Rk1r3x84LJiT"
      },
      "source": [
        "#@title Position_Linear_Approximation\n",
        "def position_linear_approximation(position, previous_certainty):\n",
        "  # Certainty is the number of times previous to current position that a point was not certain.\n",
        "  last_known_position = ((previous_certainty+2)*(-1))\n",
        "\n",
        "  # Finds the positional distance between two known boxes\n",
        "  x_delta = int((position[-1][0] - position[last_known_position][0])/(last_known_position+1))\n",
        "  y_delta = int((position[-1][1] - position[last_known_position][1])/(last_known_position+1))\n",
        "  delta = [x_delta, y_delta]\n",
        "\n",
        "  # Adjusts the previous positions, up to the previous certainty, based on a linear approximation\n",
        "  for j in range(2):\n",
        "    for i in range(previous_certainty+1):\n",
        "      position[i - (previous_certainty+1)][j] = position[i - (previous_certainty+2)][j] - delta[j]\n",
        "\n",
        "  return (position)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "vfH-PgbIdu4F"
      },
      "source": [
        "#@title Scoring_Box_Lights\n",
        "def scoring_box_lights(img, Scoring_Box_Position, scoring_box_size_average, default_color, frame, score_box_empty):\n",
        "\n",
        "  # A high max distance is less sensitive and a lower max distance is more sensitive\n",
        "  max_distance_total = 200\n",
        "  max_distance_specific_color = 100\n",
        "\n",
        "  # Defines the region of the top_left position of a 5x3 grid of the score_box, [xmin,ymin,xmax,ymax]\n",
        "  # Extends the Light Search Position outside of the detected box\n",
        "  xmin = Scoring_Box_Position[0] - int(scoring_box_size_average[0]/2) - int(scoring_box_size_average[0]/8)\n",
        "  xmax = Scoring_Box_Position[0] - int(scoring_box_size_average[0]/2) + int(scoring_box_size_average[0]/4)\n",
        "  ymin = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2)\n",
        "  ymax = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2) + int(scoring_box_size_average[1]/3)\n",
        "  left_light_position = [xmin, xmax, ymin, ymax]\n",
        "\n",
        "  # Defines the region of the top_right position of a 5x3 grid of the score_box, [xmin,ymin,xmax,ymax]\n",
        "  xmin = Scoring_Box_Position[0] + int(scoring_box_size_average[0]/2) - int(scoring_box_size_average[0]/4)\n",
        "  xmax = Scoring_Box_Position[0] + int(scoring_box_size_average[0]/2) + int(scoring_box_size_average[0]/8)\n",
        "  ymin = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2)\n",
        "  ymax = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2) + int(scoring_box_size_average[1]/3)\n",
        "  right_light_position = [xmin, xmax, ymin, ymax]\n",
        "\n",
        "  if default_color != []:\n",
        "    distance_temp, distance_specific_color_temp = [], []\n",
        "\n",
        "    width = left_light_position[1]-left_light_position[0]\n",
        "    height = left_light_position[3]-left_light_position[2]\n",
        "\n",
        "    #i is the x value of the image for the Left Side/Red\n",
        "    for i in range(width):\n",
        "      #j is y value of the image\n",
        "      for j in range(height):\n",
        "        #color channel of the image [B,G,R]\n",
        "        #image, img, is of format [y,x]\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = (img[pixel_position_y, pixel_position_x, 0] - default_color[0])\n",
        "        g = (img[pixel_position_y, pixel_position_x, 1] - default_color[1])\n",
        "        r = (img[pixel_position_y, pixel_position_x, 2] - default_color[2])\n",
        "        distance_temp.append(int((b**2 + g**2 + r**2)**(0.5)))\n",
        "        distance_specific_color_temp.append(abs(r))\n",
        "\n",
        "    #Sorts the distances and keeps the top quarter then finds the average\n",
        "    distance_temp.sort()\n",
        "    distance_temp = distance_temp[(int(len(distance_temp)/4)*-1):]\n",
        "    distance = int(sum(distance_temp)/len(distance_temp))\n",
        "    distance_specific_color_temp.sort()\n",
        "    distance_specific_color_temp = distance_specific_color_temp[(int(len(distance_specific_color_temp)/4)*-1):]\n",
        "    distance_specific_color = int(sum(distance_specific_color_temp)/len(distance_specific_color_temp))\n",
        "\n",
        "    #0 is no color change from the default color)\n",
        "    if distance > max_distance_total and distance_specific_color > max_distance_specific_color and score_box_empty == False:\n",
        "      left_light_comparison = 1\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      left_light_comparison = 0\n",
        "\n",
        "    #Resets b,g,r for the Right Side\n",
        "    distance_temp, distance_specific_color_temp= [], []\n",
        "    width = right_light_position[1]-right_light_position[0]\n",
        "    height = right_light_position[3]-right_light_position[2]\n",
        "\n",
        "    #i is the x value of the image\n",
        "    for i in range(width):\n",
        "      #j is y value of the image\n",
        "      for j in range(height):\n",
        "        #kcolor channel of the image [B,G,R]\n",
        "\n",
        "        # pixel_position = right_light_position[2] + j,right_light_position[0] + i\n",
        "        pixel_position_y = right_light_position[2] + j\n",
        "        pixel_position_x = right_light_position[0] + i\n",
        "        b = (img[pixel_position_y, pixel_position_x, 0] - default_color[0])\n",
        "        g = (img[pixel_position_y, pixel_position_x, 1] - default_color[1])\n",
        "        r = (img[pixel_position_y, pixel_position_x, 2] - default_color[2])\n",
        "        distance_temp.append(int((b**2 + g**2 + r**2)**(0.5)))\n",
        "        distance_specific_color_temp.append(abs(g))\n",
        "\n",
        "    #Sorts the distances and keeps the top sixth then finds the average\n",
        "    distance_temp.sort()\n",
        "    distance_temp = distance_temp[(int(len(distance_temp)/6)*-1):]\n",
        "    distance = int(sum(distance_temp)/len(distance_temp))\n",
        "    distance_specific_color_temp.sort()\n",
        "    distance_specific_color_temp = distance_specific_color_temp[(int(len(distance_specific_color_temp)/4)*-1):]\n",
        "    distance_specific_color = int(sum(distance_specific_color_temp)/len(distance_specific_color_temp))\n",
        "\n",
        "    #0 is no color change from the default color)\n",
        "    if (distance > max_distance_total and distance_specific_color > max_distance_specific_color):\n",
        "      right_light_comparison = 1\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      right_light_comparison = 0\n",
        "\n",
        "  #Finds the Defualt Color\n",
        "  else:\n",
        "    b, g, r = 0, 0, 0\n",
        "    # Cycles through the Left and Right Light Positions to determine a default color for the frame\n",
        "    width = left_light_position[1]-left_light_position[0]\n",
        "    height = left_light_position[3]-left_light_position[2]\n",
        "    for i in range(width):\n",
        "      for j in range(height):\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = b + img[pixel_position_y, pixel_position_x, 0]\n",
        "        g = g + img[pixel_position_y, pixel_position_x, 1]\n",
        "        r = r + img[pixel_position_y, pixel_position_x, 2]\n",
        "        default_color_left_temp = [int(b/(width*height)),int(g/(width*height)),int(r/(width*height))]\n",
        "    width = right_light_position[1]-right_light_position[0]\n",
        "    height = right_light_position[3]-right_light_position[2]\n",
        "    for i in range(width):\n",
        "      for j in range(height):\n",
        "        # pixel_position = right_light_position[2] + j,right_light_position[0] + i\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = b + img[pixel_position_y, pixel_position_x, 0]\n",
        "        g = g + img[pixel_position_y, pixel_position_x, 1]\n",
        "        r = r + img[pixel_position_y, pixel_position_x, 2]\n",
        "        default_color_right_temp = [int(b/(width*height)),int(g/(width*height)),int(r/(width*height))]\n",
        "    #Combines the Left and Right Default Colors for B,G,R\n",
        "    for i in range(3):\n",
        "      default_color.append((default_color_left_temp[i] + default_color_right_temp[i])/2)\n",
        "\n",
        "    # Assumes that the lights are off during the engarde phase.\n",
        "    left_light_comparison = 0\n",
        "    right_light_comparison = 0\n",
        "\n",
        "  return (left_light_comparison, right_light_comparison, default_color)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Kz4y3kZgPX58"
      },
      "source": [
        "#@title Motion_Difference_Tracking\n",
        "def motion_difference_tracking(frame, side, Bounding_Box, width, height, kernel_scaling, erosion_iterations, dilation_iterations, orig_img_worpt_starting_list):\n",
        "\n",
        "  # Ensures Bounding_Box is not negative\n",
        "  for i in range(len(Bounding_Box)):\n",
        "    if Bounding_Box[i] < 0:\n",
        "      Bounding_Box[i] = 0\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The original difference tracking bounding box at frame {frame - 1} is:')\n",
        "    display(Bounding_Box)\n",
        "\n",
        "  # Requires the Bounding Box to have a width and be on the image\n",
        "  if Bounding_Box[1] - Bounding_Box[0] != 0 and Bounding_Box[0] < width and Bounding_Box[0] > 0:\n",
        "    Position_y_Orig = int((Bounding_Box[3]+Bounding_Box[2])/2)\n",
        "\n",
        "    # Reads the images\n",
        "    image1 = orig_img_worpt_starting_list[frame - 1]\n",
        "    image2 = orig_img_worpt_starting_list[frame - 2]\n",
        "\n",
        "    # image1 = cv2.imread(file_name1)\n",
        "    # image2 = cv2.imread(file_name2)\n",
        "\n",
        "    # Convert to Grayscale\n",
        "    image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "    image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
        "    image_diff = cv2.absdiff(image1_gray,image2_gray)\n",
        "\n",
        "    # Creates a Cropped Image\n",
        "    crop_img = image_diff[Bounding_Box[2]:Bounding_Box[3], Bounding_Box[0]:Bounding_Box[1]]\n",
        "\n",
        "    # Kernel is affected by Kernel Scaling which gets finer if it initially fails\n",
        "    kernel_number = int(width/(100*kernel_scaling))\n",
        "    \n",
        "    # Ensures that the kernel is odd\n",
        "    if kernel_number%2 == 0:\n",
        "      kernel_number = kernel_number + 1\n",
        "    kernel = np.ones((kernel_number,kernel_number),np.uint8)\n",
        "    \n",
        "    if crop_img.shape[0] != 0 and crop_img.shape[1] != 0:\n",
        "\n",
        "      # Errodes\n",
        "      erosion = cv2.erode(crop_img,kernel,iterations = erosion_iterations)\n",
        "\n",
        "      # Dilates\n",
        "      dilation = cv2.dilate(erosion,kernel,iterations = dilation_iterations)\n",
        "\n",
        "      # Blurs Image\n",
        "      blur = cv2.GaussianBlur(dilation,kernel.shape,0)\n",
        "\n",
        "      # Threshold\n",
        "      ret,thresh = cv2.threshold(blur,0,90,cv2.THRESH_BINARY)\n",
        "\n",
        "      # Find contours\n",
        "      cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "      cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "      \n",
        "      if cnts != []:\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "        left = tuple(c[c[:, :, 0].argmin()][0])\n",
        "        right = tuple(c[c[:, :, 0].argmax()][0])\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'Left/Right is {left[0]}/{right[0]}.')\n",
        "          display(f'The int(left[0]) is {int(left[0])} and of type {type(int(left[0]))} while left[0] is {type(left[0])}.')\n",
        "          display(f'Bounding_Box[0] is {Bounding_Box[0]} and is of {type(Bounding_Box[0])}.')\n",
        "\n",
        "        if side == 'Left':\n",
        "          # Obtain outer left coordinate of the contour\n",
        "          # right = tuple(c[c[:, :, 0].argmin()][0])\n",
        "          position = [int(right[0]) + Bounding_Box[0], Position_y_Orig]\n",
        "        elif side == 'Right':\n",
        "          # left = tuple(c[c[:, :, 0].argmax()][0])\n",
        "          position = [int(left[0]) + Bounding_Box[0], Position_y_Orig]\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'Side is not given')\n",
        "\n",
        "      \n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'There is no data from difference imaging on the {side} side.')\n",
        "        position = 'None'\n",
        "\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'The crop image is null on the {side} side.')\n",
        "      position = 'None'\n",
        "      cnts = []\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The kernel number for frame {frame} is {kernel_number}, the number of errosions/dilations are {erosion_iterations}/{dilation_iterations}.')\n",
        "    if cnts != []:\n",
        "      if verbose == True:\n",
        "        display(f'The resulting position is {position} and the boundary box is {Bounding_Box}. The Left/Right limits of the contour are {int(left[0]) + Bounding_Box[0]}/{int(right[0]) + Bounding_Box[0]}.')\n",
        "  else:\n",
        "    position = 'None'\n",
        "    if verbose == True:\n",
        "      display(f'The bounding box given had a width of zero.')\n",
        "\n",
        "  return(position)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "_koxlAByNn7W"
      },
      "source": [
        "#@title Saturation_Test\n",
        "def saturation_test(box, frame, save_image_list):\n",
        "  # Test is a True/False return\n",
        "  # Takes an image and tests it for the expected saturation\n",
        "\n",
        "  img = save_image_list[frame]\n",
        "  # Converts from BGR to HSV\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "  \n",
        "  # Tests Bellguard\n",
        "  if box[2] == 1:\n",
        "    blue_range = [50, 150]\n",
        "    green_range = [50, 150]\n",
        "    red_range = [50, 160]\n",
        "    max_delta = 25\n",
        "    # saturation_range = [0, 20]\n",
        "    saturation_range = [0, 70]\n",
        "    object_tested = 'Bellguard'\n",
        "  # Tests Torso\n",
        "  elif box[2] == 3:\n",
        "    blue_range = [60, 150]\n",
        "    green_range = [60, 150]\n",
        "    red_range = [60, 160]\n",
        "    max_delta = 30\n",
        "    saturation_range = [0, 20]\n",
        "    object_tested = 'Torso'\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'The object to test does not have a color/saturation profile.')\n",
        "\n",
        "  width = (box[0][3]-box[0][1])\n",
        "  height = (box[0][2]-box[0][0])\n",
        "\n",
        "  s_temp = []\n",
        "\n",
        "  #i is the x value of the image\n",
        "  for i in range(width):\n",
        "    #j is y value of the image\n",
        "    for j in range(height):\n",
        "      s = img[box[0][0] + j, box[0][1] + i, 1]\n",
        "      s_temp.append(s)\n",
        "\n",
        "    #Sorts the distances and keeps the top quarter then finds the average\n",
        "    s_temp.sort()\n",
        "    #Truncates to the least saturated/most gray values\n",
        "    s_temp = s_temp[:(int(len(s_temp)/2)*-1)]\n",
        "    s_temp = s_temp[:(int(len(s_temp)*3/4)*-1)]\n",
        "    #Averages the saturation values\n",
        "    s_average = int(sum(s_temp)/len(s_temp))\n",
        "\n",
        "  if s_average < saturation_range[1]:\n",
        "    test_result = True\n",
        "  else:\n",
        "    test_result = False\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The test result for the {object_tested} saturation is {test_result} with a saturation of {s_average}.')\n",
        "\n",
        "  return (test_result)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7TbVcas8QGm"
      },
      "source": [
        "def exclusion_area_simplification(a, max_dist):\n",
        "  a_simplified = []\n",
        "  for i in range(len(a)):\n",
        "    for j in range(len(a) - (i+1)):\n",
        "      # a[i] - a[(i+1)+j]\n",
        "      if a[i] != 'skip' and a[(i+1)+j] != 'skip':\n",
        "        dist = int(((a[i][0] - a[(i+1)+j][0])**2 + (a[i][1] - a[(i+1)+j][1])**2)**(0.5))\n",
        "        # display(f'The distance between {a[i]} and {a[(i+1)+j]} is {dist}.')\n",
        "        if dist < max_dist:\n",
        "          a_simplified.append(a[i])\n",
        "          a[i] = 'skip'\n",
        "          a[(i+1)+j] = 'skip'\n",
        "\n",
        "  for k in range(len(a)):\n",
        "    if a[k] != 'skip':\n",
        "      a_simplified.append(a[k])\n",
        "\n",
        "  return(a_simplified)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JUqi_WR8RMk"
      },
      "source": [
        "def exclusion_area_simplification_recursion(a, max_dist):\n",
        "  a_simplified = exclusion_area_simplification(a, max_dist)\n",
        "  a_simplified_temp = 0\n",
        "\n",
        "  while a_simplified != a_simplified_temp:\n",
        "    if verbose == True:\n",
        "      display(a_simplified)\n",
        "    a_simplified_temp = exclusion_area_simplification(a_simplified, max_dist)\n",
        "    if a_simplified_temp != a_simplified:\n",
        "      a_simplified = a_simplified_temp\n",
        "      a_simplified_temp = 0\n",
        "\n",
        "  if a_simplified_temp == a_simplified:\n",
        "    if verbose == True:\n",
        "      display(f'The two are equal.')\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'The two are NOT equal.')\n",
        "\n",
        "  return(a_simplified)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "KNPGfzrydJ9M"
      },
      "source": [
        "#@title Box_Size_Finder\n",
        "def box_size_finder(bbox, capture_width, capture_height, object_to_size):\n",
        "\n",
        "  Box_Size = [[],[]]\n",
        "  sum_of_boxes = [[],[]]\n",
        "  frame_multiplier = 1\n",
        "\n",
        "  if object_to_size == 'score_box':\n",
        "    x_min = int(capture_width/4)\n",
        "    x_max = int(capture_width*3/4)\n",
        "    bbox_category = 2\n",
        "  elif object_to_size == 'left':\n",
        "    x_min = 0\n",
        "    x_max = int(capture_width/2)\n",
        "    bbox_category = 3\n",
        "  elif object_to_size == 'right':\n",
        "    x_min = int(capture_width/2)\n",
        "    x_max = int(capture_width)\n",
        "    bbox_category = 3\n",
        "\n",
        "  # i represents the frame, minimum of 50 frames or len(bbox)\n",
        "  for i in range(min(50*frame_multiplier, len(bbox))):\n",
        "    # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "    for j in range(len(bbox[i])):\n",
        "      if (bbox[i][j][1] > 0.90 and bbox[i][j][0][1] > x_min and bbox[i][j][0][1] < x_max and bbox[i][j][2] == bbox_category):\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[0].append(bbox[i][j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[1].append(bbox[i][j][0][0])  \n",
        "        #Appends x width value:\n",
        "        Box_Size[0].append(bbox[i][j][0][3] - bbox[i][j][0][1])\n",
        "        #Appends y width value:\n",
        "        Box_Size[1].append(bbox[i][j][0][2] - bbox[i][j][0][0])\n",
        "\n",
        "  x_average = average_list(sum_of_boxes[0])\n",
        "  y_average = average_list(sum_of_boxes[1])\n",
        "\n",
        "  # scoring_box_size_average [Width, Height]\n",
        "  box_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  box_size_average.append(int(average_list(Box_Size[0])))\n",
        "  # Appends the average scoring box height\n",
        "  box_size_average.append(int(average_list(Box_Size[1])))\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Average Box Size for {object_to_size} is {box_size_average}')\n",
        "\n",
        "  return (box_size_average)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "7q00swZNxhP5"
      },
      "source": [
        "#@title Tracking_Box_Default\n",
        "def tracking_box_default(Left, Right, Score_Box, x_padding, y_padding, engarde_length):\n",
        "  # Creates a default tracking box\n",
        "\n",
        "  Tracking_Bounding_Boxes_Temp = [[],[],[]]\n",
        "  Tracking_Bounding_Boxes = []\n",
        "\n",
        "  for i in range(engarde_length):\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[1] + y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[1] + y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[1]+ y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "\n",
        "  return (Tracking_Bounding_Boxes)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Cy0ESby5beUc"
      },
      "source": [
        "#@title Bell_Guard_Position_Finding\n",
        "def Bell_Guard_Position_Finding(bbox, capture_width, capture_height, positions, frame_count, left_torso_size_average, right_torso_size_average, engarde_length, previous_certainty, camera_steady, camera_motion_threshold, Exclusion_Areas, orig_img_worpt_starting_list):\n",
        "  # Format positions = [Left_Position, Right_Position, Score_Box_Position, Left_Torso_Position, Right_Torso_Position]\n",
        "\n",
        "  x_min = []\n",
        "  x_max = []\n",
        "  y_min = []\n",
        "  y_max = []\n",
        "\n",
        "  Left_Position = positions[0]\n",
        "  Right_Position = positions[1]\n",
        "  Scoring_Box_Position = positions[2]\n",
        "  Left_Torso_Position = positions[3]\n",
        "  Right_Torso_Position = positions[4]\n",
        "  Left_Foot_Position = positions[5]\n",
        "  Right_Foot_Position = positions[6]\n",
        "\n",
        "  # Any of the First engarde_length position can be used since the engarde position is an averaged constant\n",
        "  # Certainty is used here as a counter for how many times a bounding box does not fall in the tracking box\n",
        "  # And increases the size of the bounding box based on each miss\n",
        "\n",
        "  certainty = [0,0,0,0,0,0,0]\n",
        "  if verbose == True:\n",
        "    display(f'Previous Certainty at frame {frame_count - 1} is {previous_certainty}.')\n",
        "\n",
        "  #Establishes Previous Positions to determine speed and expected positions\n",
        "  previous_position_Left = Left_Position[-1]\n",
        "  twice_previous_position_Left = Left_Position[-2]\n",
        "  previous_position_Right = Right_Position[-1]\n",
        "  twice_previous_position_Right = Right_Position[-2]\n",
        "  previous_position_Scoring_Box = Scoring_Box_Position[-1]\n",
        "  twice_previous_position_Scoring_Box = Scoring_Box_Position[-2]\n",
        "  previous_position_Left_Torso = Left_Torso_Position[-1]\n",
        "  twice_previous_position_Left_Torso = Left_Torso_Position[-2]\n",
        "  previous_position_Right_Torso = Right_Torso_Position[-1]\n",
        "  twice_previous_position_Right_Torso = Right_Torso_Position[-2]\n",
        "  previous_position_Left_Foot = Left_Foot_Position[-1]\n",
        "  twice_previous_position_Left_Foot = Left_Foot_Position[-2]\n",
        "  previous_position_Right_Foot = Right_Foot_Position[-1]\n",
        "  twice_previous_position_Right_Foot = Right_Foot_Position[-2]\n",
        "\n",
        "  #Boxes are the bounding boxes for the current frame, passes less data to tracking function\n",
        "  boxes = bbox\n",
        "\n",
        "  # Tracking_Bounding_Boxes_Temp = [[],[],[],[],[]]\n",
        "  Tracking_Bounding_Boxes_Temp = [[],[],[],[],[],[],[]]\n",
        "\n",
        "  # Torso Positions are calculated prior to the BellGuard because they are an input to the bellguard position\n",
        "\n",
        "  # Bellguard Position Tracking focuses on Tracking as opposed to detection\n",
        "  \n",
        "  # Left_Torso Position\n",
        "  [current_position, certainty[3], Tracking_Bounding_Boxes_Left_Torso] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left_Torso, \\\n",
        "    twice_previous_position_Left_Torso, previous_certainty[3], 'Left_Torso', \\\n",
        "    frame_count, 'None', left_torso_size_average, capture_width, capture_height, \\\n",
        "    engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    'None', orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[3] = Tracking_Bounding_Boxes_Left_Torso\n",
        "  Left_Torso_Position = current_position\n",
        "\n",
        "  # Right_Torso Position\n",
        "  [current_position, certainty[4], Tracking_Bounding_Boxes_Right_Torso] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Right_Torso, \\\n",
        "    twice_previous_position_Right_Torso, previous_certainty[4], \"Right_Torso\", \\\n",
        "    frame_count, 'None', right_torso_size_average, capture_width, capture_height, \\\n",
        "    engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    'None', orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[4] = Tracking_Bounding_Boxes_Right_Torso\n",
        "  Right_Torso_Position = current_position\n",
        "\n",
        "  # Left Position\n",
        "  [current_position, certainty[0], Tracking_Bounding_Boxes_Left] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left, \\\n",
        "    twice_previous_position_Left, previous_certainty[0], 'Left_BellGuard', \\\n",
        "    frame_count, Left_Torso_Position, left_torso_size_average, capture_width, \\\n",
        "    capture_height, engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    Exclusion_Areas, orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[0] = Tracking_Bounding_Boxes_Left\n",
        "  Left_Position = current_position\n",
        "\n",
        "  #  Right Position\n",
        "  [current_position, certainty[1], Tracking_Bounding_Boxes_Right] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Right, \\\n",
        "    twice_previous_position_Right, previous_certainty[1], 'Right_BellGuard', \\\n",
        "    frame_count, Right_Torso_Position, right_torso_size_average, capture_width, \\\n",
        "    capture_height, engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    Exclusion_Areas, orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[1] = Tracking_Bounding_Boxes_Right\n",
        "  Right_Position = current_position\n",
        "\n",
        "  # Scoring_Box Position\n",
        "  [current_position, certainty[2], Tracking_Bounding_Boxes_Scoring_Box] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Scoring_Box, \\\n",
        "    twice_previous_position_Scoring_Box, previous_certainty[2], 'Scoring_Box', \\\n",
        "    frame_count, 'None', left_torso_size_average, capture_width, capture_height, \\\n",
        "    engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    'None', orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[2] = Tracking_Bounding_Boxes_Scoring_Box\n",
        "  Scoring_Box_Position = current_position\n",
        "\n",
        "  # Left Foot Position\n",
        "  [current_position, certainty[5], Tracking_Bounding_Boxes_Left_Foot] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left_Foot, \\\n",
        "    twice_previous_position_Left_Foot, previous_certainty[5], 'Left_Foot', \\\n",
        "    frame_count, Left_Torso_Position, left_torso_size_average, capture_width, \\\n",
        "    capture_height, engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    'None', orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[5] = Tracking_Bounding_Boxes_Left_Foot\n",
        "  Left_Foot_Position = current_position\n",
        "\n",
        "  # Left Foot Position\n",
        "  [current_position, certainty[5], Tracking_Bounding_Boxes_Left_Foot] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left_Foot, \\\n",
        "    twice_previous_position_Left_Foot, previous_certainty[5], 'Left_Foot', \\\n",
        "    frame_count, Left_Torso_Position, left_torso_size_average, capture_width, \\\n",
        "    capture_height, engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    'None', orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[5] = Tracking_Bounding_Boxes_Left_Foot\n",
        "  Left_Foot_Position = current_position\n",
        "\n",
        "  # Right Foot Position\n",
        "  [current_position, certainty[6], Tracking_Bounding_Boxes_Right_Foot] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Right_Foot, \\\n",
        "    twice_previous_position_Right_Foot, previous_certainty[6], 'Right_Foot', \\\n",
        "    frame_count, Right_Torso_Position, right_torso_size_average, capture_width, \\\n",
        "    capture_height, engarde_length, camera_steady, camera_motion_threshold, \\\n",
        "    'None', orig_img_worpt_starting_list)\n",
        "  Tracking_Bounding_Boxes_Temp[6] = Tracking_Bounding_Boxes_Right_Foot\n",
        "  Right_Foot_Position = current_position\n",
        "\n",
        "  Tracking_Bounding_Boxes = Tracking_Bounding_Boxes_Temp\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Length of the Left and Right Positions after the Position Finding are: {len(Left_Position)} and {len(Right_Position)}.')\n",
        "    display(f'At frame {frame_count} the certainty and previous certainty before linear approx analysis is:')\n",
        "    display(f'{certainty} and {previous_certainty}')\n",
        "\n",
        "  return (Left_Position, Right_Position, Scoring_Box_Position, Tracking_Bounding_Boxes, Left_Torso_Position, Right_Torso_Position, Left_Foot_Position, Right_Foot_Position, certainty)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "qY9eLcBIMerm"
      },
      "source": [
        "#@title Bell_Guard_Position_Tracking\n",
        "def Bell_Guard_Position_Tracking(boxes, previous_position, twice_previous_position, certainty, tracked_item, frame, Torso_Position, Torso_Size, capture_width, capture_height, engarde_length, camera_steady, camera_motion_threshold, Exclusion_Areas, orig_img_worpt_starting_list):\n",
        "  # Tracks the position of items\n",
        "  # tracked_item is needed since boxes only have the class of the item tracked, not the Left or Right\n",
        "  # tracked_item Format: [0,1,2,3] = [Background, Bell_Guard, Score_Box, Torso]\n",
        "\n",
        "  #Assumed inherent uncertainty\n",
        "  # certainty_default = int(capture_width/8)\n",
        "  certainty_default = int(capture_width/12)\n",
        "  certainty_multiplier = int(capture_width/80)\n",
        "\n",
        "  #Reduces the max value of y as compared to x\n",
        "  y_limiter = 24\n",
        "\n",
        "  # Max allowed speed of a bellguard in a single frame\n",
        "  # Accounts for a position jump following the engarde positioning\n",
        "  if frame < engarde_length + 3:\n",
        "    max_speed = int(capture_width/64)\n",
        "  else:\n",
        "    max_speed = int(capture_width/24)\n",
        "\n",
        "  # Converts previous position into a speed\n",
        "  x_pos = int(previous_position[0])\n",
        "  if verbose == True:\n",
        "    display(f'previous_position is {previous_position} and twice_previous_position is {twice_previous_position}.')\n",
        "  # x_speed = int(min(previous_position[0] - twice_previous_position[0], max_speed))\n",
        "\n",
        "  x_speed = previous_position[0] - twice_previous_position[0]\n",
        "  if x_speed > 0:\n",
        "    x_speed = min(x_speed, max_speed)\n",
        "  else:\n",
        "    x_speed = max(x_speed, max_speed*-1)\n",
        "\n",
        "  y_pos = int(previous_position[1])\n",
        "  y_speed = int(min(previous_position[1] - twice_previous_position[1], int(max_speed/y_limiter)))\n",
        "  y_speed = int(max(y_speed, int(max_speed*(-1)/y_limiter)))\n",
        "\n",
        "  if (frame - 1)  == engarde_length and verbose == True:\n",
        "    display(f'THe x_speed is {x_speed} and the y_speed is {y_speed} at the engarde length, frame {frame - 1}.')\n",
        "\n",
        "  # Flips the tracking box to be between the two fencers\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Left_Torso' or tracked_item == 'Left_Foot':\n",
        "    horiz_flip = False\n",
        "    if verbose == True:\n",
        "      display(f'The horizontal flip is {horiz_flip} for the {tracked_item} at frame {frame - 1}.')\n",
        "  elif tracked_item == 'Right_BellGuard' or tracked_item == 'Right_Torso' or tracked_item == 'Right_Foot':\n",
        "    horiz_flip = True\n",
        "    if verbose == True:\n",
        "      display(f'The horizontal flip is {horiz_flip} for the {tracked_item} at frame {frame - 1}.')\n",
        "  else:\n",
        "    horiz_flip = False\n",
        "\n",
        "  # Defines the tracking box\n",
        "  expected_position = [(x_pos + x_speed),(y_pos + y_speed)]\n",
        "\n",
        "  # Allows for more lenient box following engarde positioning\n",
        "  if frame < (engarde_length + 3):\n",
        "    padding = int(certainty*certainty_multiplier + certainty_default*1.5)\n",
        "  else:\n",
        "    padding = int(certainty*certainty_multiplier + certainty_default)\n",
        "  # boundary_box_for_tracking = [int(padding*7/8), padding, padding, padding]\n",
        "  boundary_box_for_tracking = [int(padding*16/8), padding, padding, padding]\n",
        "  if verbose == True:\n",
        "    display(f'The Boundary Box for the {tracked_item} is {boundary_box_for_tracking} using a certainty of {certainty} with an expected position of {expected_position} at frame {frame - 1}')\n",
        "  tracking_box = create_boundary_box(expected_position, boundary_box_for_tracking, horiz_flip)\n",
        "  positions = []\n",
        "\n",
        "  boxes_temp = []\n",
        "  #Filters out potential boxes based on Tracked Item, Confidence and Saturation of the Box\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard'):\n",
        "    bell_certainty = certainty\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The boxes being tested for exclusion are:')\n",
        "      display(boxes)\n",
        "\n",
        "    within_exclusion_box = False\n",
        "    # Only Tests if Bell Guard confidence is not extra very high  \n",
        "    for j in range(len(boxes)):\n",
        "      if boxes[j][1] < bellguard_confidence_extra_very_high and use_Exclusion_Areas == True:\n",
        "        # Tests if the box is in an exlusion area\n",
        "        padding = capture_width/48\n",
        "        exclusion_padding = [padding,padding,padding,padding]\n",
        "        # Cycles through the Exclusion Areas\n",
        "        for k in range(len(Exclusion_Areas)):\n",
        "          if verbose == True:\n",
        "            display(f'Exclusion_Areas[k] is {Exclusion_Areas[k]}.')\n",
        "          Exculsion_Box = create_boundary_box(Exclusion_Areas[k],exclusion_padding, False)\n",
        "          if verbose == True:\n",
        "            display(f'boxes[j][0] is {boxes[j][0]}, boxes[j][1] is {boxes[j][1]}.')\n",
        "          test_point = [int((boxes[j][0][3]+boxes[j][0][1])/2), int((boxes[j][0][2]+boxes[j][0][0])/2)]\n",
        "          if verbose == True:\n",
        "            display(f'The test_point is {test_point}.')\n",
        "          within_tested_box = boundary_box_test(test_point, Exculsion_Box)\n",
        "          if within_tested_box == True:\n",
        "            within_exclusion_box = True\n",
        "            if verbose == True:\n",
        "              display(f'Within the exclusion box is {within_exclusion_box}.')\n",
        "          else:\n",
        "            if verbose == True:\n",
        "              display(f'Within the exclusion box is {within_exclusion_box}.')\n",
        "\n",
        "      # Allows Extra Very High Confidence regardless of exclusion areas\n",
        "      if boxes[j][1] >= bellguard_confidence_extra_very_high:\n",
        "         within_exclusion_box = False\n",
        "\n",
        "      # for k in range(len(boxes)):\n",
        "        #The minimum required certainty for a bellguard box\n",
        "      # Appends the Appropriate boxes to the boxes list to become potential positions \n",
        "      if (boxes[j][2] == 1) and (boxes[j][1] > (bellguard_confidence - bellguard_tracking_det_offset)) and within_exclusion_box == False:\n",
        "        boxes_temp.append(boxes[j])\n",
        "\n",
        "  elif (tracked_item == 'Left_Torso' or tracked_item == 'Right_Torso'):\n",
        "    for j in range(len(boxes)):\n",
        "      # Tests the torso for a drastic change in height from engarde positioning\n",
        "      torso_height = boxes[j][0][2] - boxes[j][0][0]\n",
        "      if verbose == True:\n",
        "        display(f'The Torso Height at frame {frame - 1}, region of interest {j} is {torso_height} with initial height of {Torso_Size[1]}.')\n",
        "\n",
        "      if ((boxes[j][2] == 3) and (boxes[j][1] > min_torso_confidence) and torso_height > Torso_Size[1]*(2/3)):\n",
        "\n",
        "        boundary_box = [int(Torso_Size[0]*4),int(Torso_Size[0]*4),int(Torso_Size[1]/4),int(Torso_Size[1]/4)]\n",
        "\n",
        "        # boundary_box = [int(Torso_Size[0]),int(Torso_Size[0]*2),int(Torso_Size[1]/4),int(Torso_Size[1]/4)]                                 \n",
        "        torso_box = create_boundary_box(expected_position, boundary_box, horiz_flip)                             \n",
        "        # torso_box = create_boundary_box(Torso_Position, boundary_box_for_torso, horiz_flip)\n",
        "\n",
        "        # Allows for more lenient box following engarde positioning\n",
        "        if frame < (engarde_length + 3):\n",
        "          [x_min, x_max, y_min, y_max] = tracking_box\n",
        "        else:\n",
        "          [x_min, x_max, y_min, y_max] = boundary_box_overlap(tracking_box, torso_box)\n",
        "        x_center = int((boxes[j][0][1] + boxes[j][0][3])/2)\n",
        "        y_center = int((boxes[j][0][0] + boxes[j][0][2])/2)\n",
        "        torso_boundary_test = boundary_box_test([x_center,y_center], [x_min, x_max, y_min, y_max])\n",
        "        if torso_boundary_test == True:\n",
        "          boxes_temp.append(boxes[j])\n",
        "\n",
        "  elif (tracked_item == 'Scoring_Box'):\n",
        "    for j in range(len(boxes)):\n",
        "      if (boxes[j][2] == 2):\n",
        "        boxes_temp.append(boxes[j])\n",
        "\n",
        "  elif (tracked_item == 'Left_Foot' or tracked_item == 'Right_Foot'):\n",
        "    for j in range(len(boxes)):\n",
        "      if ((boxes[j][2] == 4) and (boxes[j][1] > foot_confidence)):\n",
        "\n",
        "        # Uses the Torso as a horizontal and vertical minimum\n",
        "        boundary_box_for_torso = [0, int(Torso_Size[0]*2), -1*int(Torso_Size[1]), int(Torso_Size[1]*2.0)]\n",
        "        torso_box = create_boundary_box(Torso_Position, boundary_box_for_torso, horiz_flip)\n",
        "        [x_min, x_max, y_min, y_max] = boundary_box_overlap(tracking_box, torso_box)\n",
        "        x_center = int((boxes[j][0][1] + boxes[j][0][3])/2)\n",
        "        y_center = int((boxes[j][0][0] + boxes[j][0][2])/2)\n",
        "        foot_boundary_test = boundary_box_test([x_center,y_center], [x_min, x_max, y_min, y_max])\n",
        "        if foot_boundary_test == True:\n",
        "          boxes_temp.append(boxes[j])\n",
        "\n",
        "  # Assigns boxes_temp to boxes\n",
        "  boxes = boxes_temp\n",
        "\n",
        "  # Creates points at the centers of the bounding boxes that are in this frame\n",
        "  x_center = []\n",
        "  y_center = []\n",
        "  for i in range(len(boxes)):\n",
        "    x_center.append(int((boxes[i][0][1] + boxes[i][0][3])/2))\n",
        "    y_center.append(int((boxes[i][0][0] + boxes[i][0][2])/2))\n",
        "\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    # Sets the values for the torso boundary box, limits Bellguard distance from Torso center\n",
        "    boundary_box_for_torso = [int(Torso_Size[0]*0.20), int(Torso_Size[0]*3.25), int(Torso_Size[1]*.75), int(Torso_Size[1]*1.0)]\n",
        "    # Uses the boundary box to create a box based on Left/Right and expected/previous position\n",
        "    torso_box = create_boundary_box(Torso_Position, boundary_box_for_torso, horiz_flip)\n",
        "    # Finds the overlap of multiple boxes to satisy multiple restrictions\n",
        "    [x_min, x_max, y_min, y_max] = boundary_box_overlap(tracking_box, torso_box)\n",
        "    if verbose == True:\n",
        "      display(f'The Torso_Size[0] is {Torso_Size[0]}, the Horizontal Flip is {horiz_flip} and Torso_Position is {Torso_Position}.')\n",
        "  elif tracked_item == 'Left_Foot' or tracked_item == 'Right_Foot':\n",
        "    # Uses the Torso as a horizontal and vertical minimum\n",
        "    boundary_box_for_torso = [0, int(Torso_Size[0]*2), -1*int(Torso_Size[1]), int(Torso_Size[1]*2.0)]\n",
        "    torso_box = create_boundary_box(Torso_Position, boundary_box_for_torso, horiz_flip)\n",
        "    [x_min, x_max, y_min, y_max] = boundary_box_overlap(tracking_box, torso_box)\n",
        "    if verbose == True:\n",
        "      display(f'The Boundary Box for the {tracked_item} with the horiz_flip as {horiz_flip} at frame {frame -1} is:')\n",
        "      display([x_min, x_max, y_min, y_max])\n",
        "      display(f'The Torso Position is {Torso_Position}.')\n",
        "      display(f'The Torso Box is {torso_box}.')\n",
        "  else:\n",
        "    [x_min, x_max, y_min, y_max] = tracking_box\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The tracking box for the {tracked_item} at frame {frame - 1} is: {tracking_box}.')\n",
        "\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard'):\n",
        "    if verbose == True:\n",
        "      display(f'The torso box for the {tracked_item} at frame {frame - 1} is: {torso_box}.')\n",
        "      display(f'The overlapping tracking box for the {tracked_item} at frame {frame - 1} is: {[x_min, x_max, y_min, y_max]}.')\n",
        "\n",
        "  # Creates a list of positions within the bounding boxes\n",
        "  for i in range(len(boxes)):\n",
        "    center = [x_center[i], y_center[i]]\n",
        "    tracking_result = boundary_box_test(center,tracking_box)\n",
        "    # If the center point is within both boxes for Bellguards or tracking box for other items, then it is appended to positions\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      torso_result = boundary_box_test(center,torso_box)\n",
        "      # if (frame - 1) == 49:\n",
        "      #   display(f'For ({x_center[i]},{y_center[i]}), {boxes[i][1]}%, the tracking_result is {tracking_result} and the torso_result is {torso_result}.')\n",
        "      # Allows for an incorrect engarde position for the bellguard\n",
        "      if (frame - 1) > engarde_length + 3:\n",
        "        if tracking_result == True and torso_result == True:\n",
        "          positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "      else:\n",
        "        # Only the torso results is required for the engarde positioning\n",
        "        if torso_result == True:\n",
        "          positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "    else:\n",
        "      if tracking_result == True:\n",
        "        positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "\n",
        "  # Maximum distance only applies if there are multiple bounding boxes within the tracking box\n",
        "  maximum_distance_from_expected = int(capture_width/24)\n",
        "  # Expected Position [x,y], Limits expected position in front of the fencer\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    if verbose == True:\n",
        "      display(f'The expected position is {expected_position} and Torso Position and size is {Torso_Position[0]} and {Torso_Size[0]}.')\n",
        "    if (expected_position[0] > Torso_Position[0] + Torso_Size[0]*2.50) and tracked_item == 'Left_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was too far in front of the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0] + Torso_Size[0]*2.50), y_pos]\n",
        "    if (expected_position[0] < Torso_Position[0]) and tracked_item == 'Left_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was behind the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0]), y_pos]\n",
        "    if expected_position[0] < Torso_Position[0] - Torso_Size[0]*2.50 and tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was too far from the Torso, adjusting expected.')\n",
        "        display(f'Torso_Position[0] is {Torso_Position[0]}, Torso_Size[0] is {Torso_Size[0]}, y_pos is {y_pos}.')\n",
        "      expected_position = [int(Torso_Position[0] - Torso_Size[0]*2.50), y_pos]\n",
        "    if (expected_position[0] > Torso_Position[0]) and tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was behind the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0]), y_pos]\n",
        "\n",
        "  #Assumed maximum distance from wrist to bellguard\n",
        "  wrist_to_bellguard_max = int(Torso_Size[0]/8)\n",
        "\n",
        "  #Sets Initial Conditions for Type of Tracking\n",
        "  using_human_pose = False\n",
        "  using_difference_images = False\n",
        "  using_difference_images_normal_kernel = False\n",
        "  using_expected = False\n",
        "  using_position = False\n",
        "\n",
        "  # fencer_data\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The camera steady value for frame {frame - 1} is {camera_steady[frame - 1]}.')\n",
        "    if camera_steady[frame - 1] >= camera_motion_threshold:\n",
        "      if verbose == True:\n",
        "        display(f'The camera is in motion and motion detection is less reliable.')\n",
        "\n",
        "  # Determines the Bellguard Position based on number of detections, confidence, box location and motion\n",
        "  if (len(positions)) == 0:\n",
        "    if verbose == True:\n",
        "      display(f'There where no positions found for the {tracked_item} at frame {frame - 1}.')\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      motion_difference_boundary = [int(Torso_Size[0]*5/8), int(Torso_Size[0] + 1), int(Torso_Size[0]/3), int(Torso_Size[0]/3)]\n",
        "      if tracked_item == 'Left_BellGuard' and camera_steady[frame - 1] < camera_motion_threshold:\n",
        "        boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "        position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, (capture_width/2560), 3, 4, orig_img_worpt_starting_list)\n",
        "        box_test = boundary_box_test(position, boundary_box)\n",
        "        if position != 'None' and box_test == True:\n",
        "          using_difference_images_normal_kernel = True\n",
        "        if position == 'None' or box_test == False:\n",
        "          if verbose == True:\n",
        "            display(f'Attempting to use a smaller kernel for motion difference tracking.')\n",
        "          position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, (capture_width/640), 3, 4, orig_img_worpt_starting_list)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          # if position != 'None' and box_test == True:\n",
        "          #   using_difference_images_normal_kernel = True\n",
        "          if position == 'None' or box_test == False:\n",
        "            if verbose == True:\n",
        "              display(f'Attempting to use a smallest kernel for motion difference tracking.')\n",
        "            position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, (capture_width/320), 3, 4, orig_img_worpt_starting_list)\n",
        "            if position == 'None':\n",
        "              if verbose == True:\n",
        "                display(f'Using the far Left Portion of the tracking Box')\n",
        "              position = [expected_position[0] - motion_difference_boundary[0], expected_position[1]]\n",
        "        # Adjusts the position if motion detection is too far out from the Torso\n",
        "        if position[0] > Torso_Position[0] + Torso_Size[0]*2.50:\n",
        "          if verbose == True:\n",
        "            display(f'The motion detected position was too far from the torso and was adjusted')\n",
        "          position[0] = int(Torso_Position[0] + Torso_Size[0]*2.50)\n",
        "          # position = [Torso_Position[0] + Torso_Size[0]*2.25, position[1]]\n",
        "        if verbose == True:\n",
        "          display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "          display(f'The boundary box test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "        # boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "        box_test = boundary_box_test(position, boundary_box)\n",
        "        #Uses the Expected position if the motion difference is out of bounds\n",
        "        if box_test == False:\n",
        "          if verbose == True:\n",
        "            display(f'Motion difference failed, using the Expected Position for the {tracked_item} for frame {frame - 1}.')\n",
        "          position = expected_position\n",
        "          using_expected = True\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'The motion difference position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "          using_difference_images = True\n",
        "      elif tracked_item == 'Right_BellGuard' and camera_steady[frame - 1] < camera_motion_threshold:\n",
        "        boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "        position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, (capture_width/2560), 3, 4, orig_img_worpt_starting_list)\n",
        "        box_test = boundary_box_test(position, boundary_box)\n",
        "        if position != 'None' and box_test == True:\n",
        "          using_difference_images_normal_kernel = True\n",
        "        if position == 'None' or box_test == False:\n",
        "          if verbose == True:\n",
        "            display(f'Attempting to use a smaller kernel for motion difference tracking.')\n",
        "          position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, (capture_width/640), 3, 4, orig_img_worpt_starting_list)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          # if position != 'None' and box_test == True:\n",
        "          #   using_difference_images_normal_kernel = True\n",
        "          if position == 'None' or box_test == False:\n",
        "            if verbose == True:\n",
        "              display(f'Attempting to use the smallest kernel for motion difference tracking.')\n",
        "            position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, (capture_width/320), 3, 4, orig_img_worpt_starting_list)\n",
        "            # Uses the Right Portion of the Motion Tracking Box if no motion tracking is found\n",
        "            if position == 'None' or box_test == False:\n",
        "              if verbose == True:\n",
        "                display(f'Using the far Right Portion of the tracking Box, ({expected_position[0] + motion_difference_boundary[0]},{expected_position[1]})')\n",
        "              position = [expected_position[0] + motion_difference_boundary[0], expected_position[1]]\n",
        "        if position[0] < Torso_Position[0] - Torso_Size[0]*2.50:\n",
        "          if verbose == True:\n",
        "            display(f'The motion detected position ({position[0]},{position[1]}) was too far from the torso ({Torso_Position[0]},{Torso_Position[1]}), with a max of {Torso_Position[0] - Torso_Size[0]*2.50} and was adjusted')\n",
        "          position[0] = int(Torso_Position[0] - Torso_Size[0]*2.50)\n",
        "          # position = [Torso_Position[0] - Torso_Size[0]*2.25, position[1]]\n",
        "        if verbose == True:\n",
        "          display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "          display(f'The boundary box test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "        boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "        box_test = boundary_box_test(position, boundary_box)\n",
        "        # box_test = False\n",
        "        if box_test == False:\n",
        "          if verbose == True:\n",
        "            display(f'Motion difference failed, using the Expected Position for the {tracked_item} for frame {frame - 1}.')\n",
        "          position = expected_position\n",
        "          using_expected = True\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'The motion difference position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "          using_difference_images = True\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'Too much camera motion, using expected position')\n",
        "        position = expected_position\n",
        "        using_expected = True    \n",
        "    else:\n",
        "      position = expected_position    \n",
        "\n",
        "    # Criteria for Setting Certainty to zero preventing a linear appoximation adjustment of this point\n",
        "    # Allows for motion difference to be certain if it uses the largest kernel\n",
        "    if (using_difference_images_normal_kernel == True and position[1] < Torso_Position[1] + Torso_Size[1] and \\\n",
        "        abs(expected_position[0] - position[0]) < Torso_Size[0]/2 and camera_steady[frame - 1] < camera_motion_threshold):\n",
        "      if using_difference_images == True:\n",
        "        if verbose == True:\n",
        "          display(f'Using difference images for frame {frame - 1} with no detected positions')\n",
        "    # if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high):\n",
        "      certainty = 0\n",
        "    else:\n",
        "      certainty = certainty + 1\n",
        "\n",
        "  # For a single detected Bellguard Position\n",
        "  elif (len(positions)) == 1:\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'There is one possible position, {positions[0]} for {tracked_item} in the tracking box for frame {frame - 1}.')\n",
        "      # Allows for a large bounding box if bell guard confidence is very high\n",
        "      if positions[0][2] > bellguard_confidence_extra_very_high:\n",
        "        if verbose == True:\n",
        "          display(f'Using Bell Guard Extra Very High confidence.')\n",
        "        single_position_box = [int(Torso_Size[0]*9/8*(1+bell_certainty/4)), int(Torso_Size[0]*9/8*(1+bell_certainty/4)), int(Torso_Size[0]*12/8), int(Torso_Size[0]*12/8)]\n",
        "      elif positions[0][2] > bellguard_confidence_very_high and positions[0][2] <= bellguard_confidence_extra_very_high:\n",
        "        if verbose == True:\n",
        "          display(f'Using Bell Guard Very High confidence.')\n",
        "        single_position_box = [int(Torso_Size[0]*6/8*(1+bell_certainty/4)), int(Torso_Size[0]*6/8*(1+bell_certainty/4)), int(Torso_Size[0]*12/8), int(Torso_Size[0]*12/8)]\n",
        "      else:\n",
        "        single_position_box = [int(Torso_Size[0]*4/8*(1+bell_certainty/4)), int(Torso_Size[0]*5/8*(1+bell_certainty/4)), int(Torso_Size[0]*8/8), int(Torso_Size[0]*8/8)]\n",
        "      if tracked_item == 'Left_BellGuard':\n",
        "        boundary_box = create_boundary_box(expected_position, single_position_box, False)\n",
        "      else:\n",
        "        boundary_box = create_boundary_box(expected_position, single_position_box, True)\n",
        "      box_test = boundary_box_test(positions[0], boundary_box)\n",
        "      if verbose == True:\n",
        "        display(f'The expected position for frame {frame - 1} is {expected_position}.')\n",
        "        display(f'The single_position_box is {single_position_box} and the boundary box is {boundary_box}.')\n",
        "      if box_test == True and positions[0][2] > bellguard_confidence_high:\n",
        "        if verbose == True:\n",
        "          display(f'The detected position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "        position = positions[0]\n",
        "        using_position = True\n",
        "      else:\n",
        "        #Human Pose\n",
        "        if verbose == True:\n",
        "          display(f'Attempting to use Human Pose for the {tracked_item} at frame {frame - 1}')\n",
        "        #Image Difference\n",
        "        if verbose == True:\n",
        "          display(f'Attempting to use Image Difference for the {tracked_item} at frame {frame - 1}')\n",
        "        motion_difference_boundary = [int(Torso_Size[0]/8), int(Torso_Size[0]/2), int(Torso_Size[0]/4), int(Torso_Size[0]/4)]\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "          diff_position = motion_difference_tracking(frame, 'Left', [x_min, x_max, y_min, y_max], capture_width, capture_height, 1, 1, 2, orig_img_worpt_starting_list)\n",
        "          if diff_position == 'None':\n",
        "            diff_position = motion_difference_tracking(frame, 'Left', [x_min, x_max, y_min, y_max], capture_width, capture_height, 2, 1, 2, orig_img_worpt_starting_list)\n",
        "        else:\n",
        "          #Right Bellguard is assumed\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "          diff_position = motion_difference_tracking(frame, 'Right', [x_min, x_max, y_min, y_max], capture_width, capture_height, 1, 1, 2, orig_img_worpt_starting_list)\n",
        "          if diff_position == 'None':\n",
        "            diff_position = motion_difference_tracking(frame, 'Right', [x_min, x_max, y_min, y_max], capture_width, capture_height, 2, 1, 2, orig_img_worpt_starting_list)\n",
        "        box_test = boundary_box_test(diff_position, motion_difference_boundary)\n",
        "        if box_test == True and diff_position != 'None':\n",
        "          position = diff_position\n",
        "          using_difference_images = True\n",
        "        else:\n",
        "          #Expected Position\n",
        "          position = expected_position\n",
        "          using_expected = True\n",
        "        if verbose == True:\n",
        "          display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "          display(f'The motion_difference_boundary test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "\n",
        "      # Designed to catch an engarde position that is outside the tracking box\n",
        "      if frame < (engarde_length + 3) and position == twice_previous_position:\n",
        "        position = positions[0]\n",
        "\n",
        "      #Sets Certainty Box\n",
        "      # if (using_human_pose == True and fencer_data_side[0][2] > wrist_conf_high) or (using_position == True):\n",
        "      if using_position == True:\n",
        "        certainty = 0\n",
        "        if verbose == True:\n",
        "          display(f'Certainty set to zero for frame {frame - 1} for the {tracked_item}.')\n",
        "      else:\n",
        "        certainty = certainty + 1\n",
        "\n",
        "    else:\n",
        "      position = positions[0]\n",
        "\n",
        "  # Multiple bounding boxes within the tracking box\n",
        "  elif (len(positions)) > 1:\n",
        "    if verbose == True:\n",
        "      display(f'Multiple Bounding Boxes Detected for the {tracked_item} at frame {frame - 1}')\n",
        "    # One set of conditions is used for Bell_Guards and another for all else\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      if positions[0][2] > bellguard_confidence_high:\n",
        "        if verbose == True:\n",
        "          display(f'Using Multiple Box Determination for the {tracked_item} at frame {frame - 1}.')\n",
        "        human_pose_boundary = [int(Torso_Size[0]*3/4), int(Torso_Size[0]), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "        position = multiple_box_determination(expected_position, positions, [human_pose_boundary[0], human_pose_boundary[1]], bellguard_confidence, horiz_flip)\n",
        "        using_position = True\n",
        "      else:\n",
        "        position = expected_position\n",
        "        using_expected = True\n",
        "        if verbose == True:\n",
        "          display(f'The Human Pose Box Test failed for the {tracked_item} at frame {frame - 1}, using expected position.')\n",
        "      within_distance_from_expected = []\n",
        "      for i in range(len(positions)):\n",
        "        expected_box = [int(Torso_Size[0]/2*(1+bell_certainty/4)), int(Torso_Size[0]*(1+bell_certainty/4)), int(Torso_Size[0]/2*(1+bell_certainty/4)), int(Torso_Size[0]/6)]\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          boundary_box = create_boundary_box(expected_position, expected_box, False)\n",
        "        else:\n",
        "          boundary_box = create_boundary_box(expected_position, expected_box, True)\n",
        "        box_test = boundary_box_test(positions[i], boundary_box)\n",
        "        if box_test:\n",
        "          within_distance_from_expected.append(positions[i])\n",
        "\n",
        "        # Uses the most confident, i.e. the first position in the list\n",
        "        if len(within_distance_from_expected) > 0:\n",
        "          position_boundary = [int(Torso_Size[0]/4), int(Torso_Size[0]/2), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "          position = multiple_box_determination(expected_position, positions, [position_boundary[0], position_boundary[1]], bellguard_confidence, horiz_flip)\n",
        "          certainty = 0\n",
        "          using_position = True\n",
        "        else:\n",
        "          # If the length of within_distance_from_expected is zero\n",
        "          if verbose == True:\n",
        "            display(f'Error occured finding a position within the required distance and the {tracked_item} set to expected position at frame {frame - 1}.')\n",
        "            display(f'The expected position is {expected_position}, while the expected box is {expected_box}.')\n",
        "          position = [(x_pos + x_speed),(y_pos + y_speed)]\n",
        "          using_expected = True\n",
        "\n",
        "      #Sets Certainty Box\n",
        "      # if (using_human_pose == True and fencer_data_side[0][2] > wrist_conf_high) or (using_position == True):\n",
        "      if using_position == True:\n",
        "        if verbose == True:\n",
        "          display(f'Confidence for the {tracked_item} is High so the certainty is set to zero.')\n",
        "        certainty = 0\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'Confidence for the {tracked_item} is Low so the certainty is incremented higher.')\n",
        "        certainty = certainty + 1\n",
        "\n",
        "    elif tracked_item == 'Left_Foot' or tracked_item == 'Right_Foot':\n",
        "      # Max and Min are based on the first value of the set therefore in this case max and min refer to the x position\n",
        "      if horiz_flip == False:\n",
        "        position = max(positions)\n",
        "      else:\n",
        "        position = min(positions)\n",
        "      certainty = 0\n",
        "\n",
        "    # If the tracked item is not a bell_guard\n",
        "    else:\n",
        "      # Uses the most confident position within the tracking box\n",
        "      position = positions[0]\n",
        "      # Sets Certainty for Torso and Box back to Zero if detected.\n",
        "      certainty = 0\n",
        "\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    if verbose == True:\n",
        "      display(f'The position of the {tracked_item} at frame {frame - 1} is {position}.')\n",
        "\n",
        "  return (position, certainty, [x_min, x_max, y_min, y_max])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "0RRV4tGfTVF2"
      },
      "source": [
        "#@title Weight_Average_List\n",
        "def weight_average_list(List):\n",
        "  # Finds the Weight Average of a List\n",
        "\n",
        "  # Prevents division by zero\n",
        "  try:\n",
        "    value_sum = 0\n",
        "    value_weight = 0\n",
        "    for i in range(len(List)):\n",
        "      value_sum = value_sum + List[i][0] * List[i][1]\n",
        "      value_weight = value_weight + List[i][1]\n",
        "    weighted_average = value_sum/value_weight\n",
        "  except:\n",
        "    weighted_average = 0\n",
        "\n",
        "  return (weighted_average)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "LAkHbcyRCmQk"
      },
      "source": [
        "#@title Average_List\n",
        "def average_list(List):\n",
        "  # Finds the Average of a List\n",
        "  try:\n",
        "    average = sum(List) / len(List)\n",
        "  except:\n",
        "    average = 0\n",
        "  return (average)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViVEUhN0WqUu"
      },
      "source": [
        "def average_list_without_null(List):\n",
        "  # Takes a List of x,y values and null values. Removes the null values and returns the average x and y values.\n",
        "\n",
        "  List_temp = []\n",
        "  for i in range(len(List)):\n",
        "    if List[i] != []:\n",
        "      List_temp.append(List[i])\n",
        "    else:\n",
        "      pass\n",
        "  \n",
        "  x_sum = 0\n",
        "  y_sum = 0\n",
        "\n",
        "  # display(f'List_Temp is {List_temp}')\n",
        "\n",
        "  for j in range(len(List_temp)):\n",
        "    x_sum = x_sum + List_temp[j][0]\n",
        "    y_sum = y_sum + List_temp[j][1]\n",
        "\n",
        "  x_average = int(x_sum/len(List_temp))\n",
        "  y_average = int(y_sum/len(List_temp))\n",
        "\n",
        "  average = [x_average, y_average]\n",
        "\n",
        "  return (average)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "FZCO-GE5eqUl"
      },
      "source": [
        "#@title Color_Tester\n",
        "def color_tester(box, frame):\n",
        "  #Takes a given box and tests for a specific color range\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "  file_name = str(frame) + '.jpg'\n",
        "  name = os.path.join(path, file_name)\n",
        "  img = cv2.imread(name)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The file names to be color tested is {name}.')\n",
        "  # box[0] are the coordinates ([y1,x1,y2,x2]), box[1] is confidence and box[2] is object\n",
        "  # Tests if Bellguard is the correct color\n",
        "  if box[2] == 1:\n",
        "    blue_range = [50, 150]\n",
        "    green_range = [50, 150]\n",
        "    red_range = [50, 160]\n",
        "    max_delta = 25\n",
        "  elif box[2] == 3:\n",
        "    blue_range = [60, 150]\n",
        "    green_range = [60, 150]\n",
        "    red_range = [60, 160]\n",
        "    max_delta = 30\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'The object to test does not have a color profile.')\n",
        "\n",
        "  # OpenCV uses Blue, Green, Red order\n",
        "  b, g, r = 0, 0, 0\n",
        "\n",
        "  width = (box[0][3]-box[0][1])\n",
        "  height = (box[0][2]-box[0][0])\n",
        "\n",
        "  #i is the x value of the image\n",
        "  for i in range(width):\n",
        "    #j is y value of the image\n",
        "    for j in range(height):\n",
        "      #color channel of the image [B,G,R]\n",
        "      #image, img, is of format [y,x] \n",
        "      b = b + img[box[0][0] + j, box[0][1] + i, 0]\n",
        "      g = g + img[box[0][0] + j, box[0][1] + i, 1]\n",
        "      r = r + img[box[0][0] + j, box[0][1] + i, 2]\n",
        "\n",
        "  # Finds the Color Averages\n",
        "  b_average = int(b/(width*height))\n",
        "  g_average = int(g/(width*height))\n",
        "  r_average = int(r/(width*height))\n",
        "\n",
        "  # Finds maximum differences between colors\n",
        "  max_1 = abs(b_average - g_average)\n",
        "  max_2 = abs(b_average - r_average)\n",
        "  max_3 = abs(g_average - r_average)\n",
        "  max_delta = max(max_1, max_2, max_3)\n",
        "\n",
        "  if test_result == False:\n",
        "    if verbose == True:\n",
        "      display(f'The Color Test Result Failed for object {box[2]}.')\n",
        "\n",
        "  return (test_result)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "GPntbZz1PTB-"
      },
      "source": [
        "#@title Symmetry_Test\n",
        "def symmetry_test(width, height, left_x, left_y, right_x, right_y):\n",
        "\n",
        "  # Tests the potential left and right positions for left/right symmetry and removes outlier points\n",
        "  display(f'Commencing Symmetry Test...')\n",
        "\n",
        "  # Sets how large the allowable band is with respect to height or width\n",
        "  band_width_ratio_x = 8\n",
        "  band_width_ratio_y = 8\n",
        "\n",
        "  all_positions_x = left_x + right_x\n",
        "  all_positions_y = left_y +right_y\n",
        "  if len(all_positions_x) != len(all_positions_y):\n",
        "    display(f'ERROR...The length of the x and y positions are different.')\n",
        "\n",
        "\n",
        "  # Keeps track of which positions are most in line with the other positions\n",
        "  # Finds the X Band\n",
        "  x_distances_from_center = []\n",
        "  x_distances_from_other_points_score = []\n",
        "  for i in range(len(all_positions_x)):\n",
        "    #Determines the x_min band for each position by distance from center\n",
        "    x_distances_from_center.append(abs(int((width/2)-all_positions_x[i])))\n",
        "  #Creates an iterator that determines which x_point is close to the most other points and finds its index\n",
        "  for j in range(len(x_distances_from_center)):\n",
        "    score = 0\n",
        "    for k in range(len(x_distances_from_center) - 1):\n",
        "      if abs(x_distances_from_center[j] - x_distances_from_center[k+1]) < width/band_width_ratio_x:\n",
        "        score = score + 1\n",
        "      else:\n",
        "        pass\n",
        "    x_distances_from_other_points_score.append(score)\n",
        "  x_index_band = x_distances_from_other_points_score.index(max(x_distances_from_other_points_score))\n",
        "\n",
        "  x_min = abs(int(all_positions_x[x_index_band] - width/band_width_ratio_x))\n",
        "  x_max = abs(int(all_positions_x[x_index_band] + width/band_width_ratio_x))\n",
        "\n",
        "  # Finds the Y Band\n",
        "  y_distances_from_center = []\n",
        "  y_distances_from_other_points_score = []\n",
        "  for i in range(len(all_positions_y)):\n",
        "    y_distances_from_center.append(abs(int((height/2)-all_positions_y[i])))\n",
        "  for j in range(len(y_distances_from_center)):\n",
        "    score = 0\n",
        "    for k in range(len(y_distances_from_center) - 1):\n",
        "      if abs(y_distances_from_center[j] - y_distances_from_center[k+1]) < width/band_width_ratio_y:\n",
        "        score = score + 1\n",
        "      else:\n",
        "        pass\n",
        "    y_distances_from_other_points_score.append(score)\n",
        "  y_index_band = y_distances_from_other_points_score.index(max(y_distances_from_other_points_score))\n",
        "\n",
        "  y_min = abs(int(all_positions_y[y_index_band] - width/band_width_ratio_y))\n",
        "  y_max = abs(int(all_positions_y[y_index_band] + width/band_width_ratio_y))\n",
        "\n",
        "  # Cycles through the positions and keeps values that are in the horizontal x band\n",
        "  positionsx_temp = []\n",
        "  positionsy_temp = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The x_min/max is {x_min}/{x_max}, the band width is {width/band_width_ratio_x} and the center is {width/2}.')\n",
        "\n",
        "  for i in range(len(all_positions_x)):\n",
        "    if ((all_positions_x[i] < (width/2 - x_min)) and (all_positions_x[i] > (width/2 - x_max))) or ((all_positions_x[i] < (width/2 + x_max)) and (all_positions_x[i] > (width/2 + x_min))):\n",
        "      positionsx_temp.append(all_positions_x[i])\n",
        "      positionsy_temp.append(all_positions_y[i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  # Replaces the all position x and y lists with the temp list limited by the bands\n",
        "  all_positions_x = positionsx_temp\n",
        "  all_positions_y = positionsy_temp\n",
        "\n",
        "  #Cycles through the positions and keeps values that are in the vertical y band\n",
        "  positionsx_temp = []\n",
        "  positionsy_temp = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The y_min/max is {y_min}/{y_max}, the band width is {height/band_width_ratio_y} and the center is {height/2}.')\n",
        "\n",
        "  for i in range(len(all_positions_y)):\n",
        "    if ((all_positions_y[i] > (y_min)) and (all_positions_y[i] < (y_max))):\n",
        "      positionsx_temp.append(all_positions_x[i])\n",
        "      positionsy_temp.append(all_positions_y[i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  # Replaces the all position x and y lists with the temp list limited by the bands\n",
        "  all_positions_x = positionsx_temp\n",
        "  all_positions_y = positionsy_temp\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'There were originaly {len(left_x) + len(right_x)} values and {len(all_positions_x) - (len(left_x) + len(right_x))} were removed.')\n",
        "\n",
        "  # Returns the x and y values to left and right positions\n",
        "  ret_left_x, ret_left_y, ret_right_x, ret_right_y = [],[],[],[]\n",
        "\n",
        "  \n",
        "  for i in range(len(all_positions_x)):\n",
        "    # Tests if the x value is on the left or right side\n",
        "    if all_positions_x[i] < width/2:\n",
        "      ret_left_x.append(all_positions_x[i])\n",
        "      ret_left_y.append(all_positions_y[i])\n",
        "    else:\n",
        "      ret_right_x.append(all_positions_x[i])\n",
        "      ret_right_y.append(all_positions_y[i])\n",
        "  # Prevents an off center camera from removing all engarde points\n",
        "  if (len(ret_left_x) == 0) or (len(ret_left_y) == 0) or (len(ret_right_x) == 0) or (len(ret_right_y) == 0):\n",
        "    ret_left_x = left_x\n",
        "    ret_left_y = left_y\n",
        "    ret_right_x = right_x\n",
        "    ret_right_y = right_y\n",
        "\n",
        "  return (ret_left_x, ret_left_y, ret_right_x, ret_right_y)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "YR6IATJKmVDz"
      },
      "source": [
        "#@title List_Threshold_Test\n",
        "def list_threshold_test(threshold, list_to_test):\n",
        "  #Determines if a list meets a minimum threshold\n",
        "  threshold_met = False\n",
        "\n",
        "  for k in range(len(list_to_test)):\n",
        "    if list_to_test[k][1] > threshold:\n",
        "      threshold_met = True\n",
        "      break\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return (threshold_met)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "FnsMsig-fR4M"
      },
      "source": [
        "#@title Multiple_Box_Determination\n",
        "def multiple_box_determination(expected_position, positions, x_boundaries, min_conf, horiz_flip):\n",
        "\n",
        "  confidence_weighting = .9\n",
        "\n",
        "  delta_x_forward = x_boundaries[1]\n",
        "  delta_x_backward = x_boundaries[0]\n",
        "\n",
        "  if horiz_flip == True:\n",
        "    delta_temp = delta_x_forward\n",
        "    delta_x_forward = delta_x_backward\n",
        "    delta_x_backward = delta_temp\n",
        "\n",
        "  position_ratings = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'There are {len(positions)} positions available.')\n",
        "    display(f'The positions are:')\n",
        "    display(positions)  \n",
        "\n",
        "  for i in range(len(positions)):\n",
        "    delta_position = positions[i][0] - expected_position[0]\n",
        "    if verbose == True:\n",
        "      display(f'The positions{i}[0] is {positions[i][0]} and the expected_position[0] is {expected_position[0]} therefore delta position is {delta_position}.')\n",
        "    if delta_position > 0:\n",
        "      if verbose == True:\n",
        "        display(f'Position {i} is forward of the expected position.')\n",
        "      position_ratings.append(abs((delta_position/delta_x_forward)*(1-positions[i][2])**confidence_weighting))\n",
        "      if verbose == True:\n",
        "        display(f'delta_position is {delta_position}.')\n",
        "        display(f'delta_x_forward is {delta_x_forward}.')\n",
        "        display(f'positions[i][2] is {positions[i][2]}.')\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'Position {i} is behind the expected position.')\n",
        "      position_ratings.append(abs((delta_position/delta_x_backward)*(1-positions[i][2])**confidence_weighting))\n",
        "      if verbose == True:\n",
        "        display(f'delta_position is {delta_position}.')\n",
        "        display(f'delta_x_backward is {delta_x_backward}.')\n",
        "        display(f'positions[i][2] is {positions[i][2]}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(position_ratings)\n",
        "\n",
        "  position = positions[position_ratings.index(min(position_ratings))]\n",
        "\n",
        "  return (position)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "DtyzedR9Aief"
      },
      "source": [
        "#@title Boundary_Box_Overlap\n",
        "def boundary_box_overlap(box1, box2):\n",
        "  #Finds the overlap of two boxes assume (x_min, x_max, y_min, y_max)\n",
        "  \n",
        "  box_overlap = [max(box1[0], box2[0]), min(box1[1], box2[1]), max(box1[2], box2[2]), min(box1[3], box2[3])]\n",
        "\n",
        "  return (box_overlap)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "C8uvTRrIxL3T"
      },
      "source": [
        "#@title Create_Boundary_Box\n",
        "def create_boundary_box(center, padding, horiz_flip):\n",
        "  # Creates a Boundary Box based on Center Padding and if the Left and Right Boundaries should be flipped.\n",
        "  # Center is [x,y]\n",
        "  # Padding is [Behind, Front, Top, Bottom]\n",
        "  # horiz_flip is True or False\n",
        "\n",
        "  if horiz_flip == False:\n",
        "    left = center[0] - padding[0]\n",
        "    right = center[0] + padding[1]\n",
        "  elif horiz_flip == True:\n",
        "    left = center[0] - padding[1]\n",
        "    right = center[0] + padding[0]\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'ERROR Horiz Flip not True or False.')\n",
        "\n",
        "  top = center[1] - padding[2]\n",
        "  bottom = center[1] + padding[3]\n",
        "\n",
        "  return ([left, right, top, bottom])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "cnDPSNJoQINX"
      },
      "source": [
        "#@title Boundary_Box_Test\n",
        "def boundary_box_test(test_point, boundary):\n",
        "  # Tests if a give point is in a Boundary Box.\n",
        "  #Format Test_Point is of the form (x,y)\n",
        "  #Format Boundary is of the form (x_min, x_max, y_min, y_max)\n",
        "  #Format Boundary is of the form (behind the fencer, in front of the fencer, above the fencer, below the fencer)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(test_point)\n",
        "    display(boundary)\n",
        "\n",
        "  if test_point != 'None':\n",
        "    if test_point[0] > boundary[0] and test_point[0] < boundary[1] and test_point[1] > boundary[2] and test_point[1] < boundary[3]:\n",
        "      box_test = True\n",
        "    else:\n",
        "      box_test = False\n",
        "  else:\n",
        "    box_test = False\n",
        "\n",
        "  return (box_test)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "HPik4RGlWPYB"
      },
      "source": [
        "#@title Engarde_Failure_Test\n",
        "def engarde_failure_test(bbox, bellguard_confidence, x_max, y_max, side):\n",
        "  # Tests for reasons the engarde positioning failed to detect a BellGuard\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The {side} engarde position failed due to...')\n",
        "\n",
        "  if side == 'Left':\n",
        "    oppside = 'Right'\n",
        "    k = 0\n",
        "  else:\n",
        "    oppside = 'Left'\n",
        "    k = 1\n",
        "\n",
        "\n",
        "  for j in range(len(bbox)):\n",
        "    if bbox[j][1] < bellguard_confidence:\n",
        "      if verbose == True:\n",
        "        display(f'The confidence in the {side} bellguard is too low at {bellguard_confidence}.')\n",
        "    else: \n",
        "      pass\n",
        "    if side == 'Left':\n",
        "      if bbox[j][k] > x_max:\n",
        "        if verbose == True:\n",
        "          display(f'The {side} bellguard was too far {oppside} at {bbox[j][0]} while the maximum is {x_max}.')\n",
        "      else:\n",
        "        pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'bbox at this point is: {bbox}. J is {j} and k is {k}.')\n",
        "        display(bbox[j])\n",
        "        display(bbox[j][k])\n",
        "      if bbox[j][k] < x_max:\n",
        "        if verbose == True:\n",
        "          display(f'The {side} bellguard was too far {oppside} at {bbox[j][0]} while the maximum is {x_max}.')\n",
        "    if bbox[j][k] > y_max:\n",
        "      if verbose == True:\n",
        "        display(f'The {side} bellguard was too low at {bbox[j][0]} while the maximum allowed is {y_max}.')\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "rzdGZ-f52Ucw"
      },
      "source": [
        "#@title Torso_Failure_Test\n",
        "def torso_failure_test(bbox, capture_width, capture_height, y_average, Bell_Guard_Size_average, side, frame_count, min_torso_confidence):\n",
        "  # Tests for reasons the engarde positioning failed to detect a Torso\n",
        "  # Is tested at finding tracking boxes\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The {side} Torso failed due to...') \n",
        "  for j in range(len(bbox)):\n",
        "    if bbox[j][1] > min_torso_confidence:\n",
        "      pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'The confidence is of the box is too low at only {int(bbox[j][1]*100)}% at frame {frame_count}.')\n",
        "    if bbox[j][0][2] > y_average:\n",
        "      pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'The Torso was not lower than the Bell Guard with a lower height of {bbox[j][0][2]} with a max value of {y_average} at frame {frame_count}.')\n",
        "    if bbox[j][0][2] < (y_average + 3*Bell_Guard_Size_average[1]):\n",
        "      pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'The bottom of the torso box was too low at {bbox[j][0][2]} with a max value of {int(y_average + 3*Bell_Guard_Size_average[1])} at frame {frame_count}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'y_average is {y_average}.')\n",
        "    display(f'Bell_Guard_Size_average[1] is {Bell_Guard_Size_average[1]}.')\n",
        "\n",
        "  return"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "SrP957nvDFvz"
      },
      "source": [
        "#@title Torso_Position_Failure_Test\n",
        "def torso_position_failure_test(bbox, engarde_length, x_min_torso, x_max_torso, y_min_torso, y_max_torso, y_average, side, frame_count):\n",
        "  # Tests for reasons the engarde positioning failed to detect a Torso\n",
        "  # Is tested at torso positions\n",
        "\n",
        "  confidence = min_torso_confidence\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Analyzing the Torso Position Failure at frame {frame_count} for the {side} side...')\n",
        "  count = 0\n",
        "  \n",
        "  for k in range(len(bbox)):\n",
        "\n",
        "    if bbox[k][2] == 3 and bbox[k][1] > confidence:\n",
        "      count = count + 1\n",
        "  if verbose == True:\n",
        "    display(f'There are {len(bbox)} ROIs, {count} of them are Torsos with greater than {confidence}%.')\n",
        "\n",
        "  for j in range(len(bbox)):\n",
        "    y_center = int((bbox[j][0][0] + bbox[j][0][2])/2)\n",
        "    x_center = int((bbox[j][0][1] + bbox[j][0][3])/2)\n",
        "    if bbox[j][2] == 3 and bbox[j][1] > confidence:\n",
        "      if x_center > x_min_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {x_center} is to the Left of the Box side at {x_min_torso} at frame {frame_count}.')\n",
        "      if x_center < x_max_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {x_center} is to the Right of the Box side at {x_max_torso} at frame {frame_count}.')\n",
        "      if y_center > y_min_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {y_center} is Above the Box at {y_min_torso} at frame {frame_count}.')\n",
        "      if y_center < y_max_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {y_center} is Below the Box at {y_max_torso} at frame {frame_count}.')\n",
        "      if bbox[j][0][2] > y_average:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center is Below the Bell Guard at frame {frame_count}.')\n",
        "      if bbox[j][2] == 3:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso is not labelled as a Torso at frame {frame_count}.')\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "YrY_K5mZafoc"
      },
      "source": [
        "#@title Engarde_Position\n",
        "def engarde_position(bbox, capture_width, capture_height, engarde_length, frame_count, save_image_list):\n",
        "  #Finds the initial positions to start tracking\n",
        "  #Format of bbox[frame][roi], ([y1,x1,y2,x2], percent certainty, type)\n",
        "\n",
        "  # Initializes the Bell Guard Positions\n",
        "  # Position format [x,y]\n",
        "  # Size format [[Width],[Height]]\n",
        "  Left_Position = []\n",
        "  Right_Position = []\n",
        "  Bell_Guard_Size = [[],[]]\n",
        "  Scoring_Box_Position = []\n",
        "  Scoring_Box_Size = [[],[]]\n",
        "  Left_Torso_Position = []\n",
        "  Left_Torso_Size = [[],[]]\n",
        "  Right_Torso_Position = []\n",
        "  Right_Torso_Size = [[],[]]\n",
        "  Left_Foot_Position = []\n",
        "  Right_Foot_Position = []\n",
        "  Foot_Size = [[],[]]\n",
        "  All_Bell_Guard_Positions = []\n",
        "\n",
        "  # sum_of_boxes is used to average the Left (x,y)(0), Right (x,y)(1), ScoreBox (x,y)(2), Left_Torso (x,y)(3), Right_Torso(x,y)(4),\n",
        "  # Left_Foot(x,y)(5), Right_Foot(x,y),(6) values\n",
        "  sum_of_boxes = [[[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]]\n",
        "\n",
        "  # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "  for j in range(len(bbox)):\n",
        "    # The percent confidence for each region of interest (roi) is [i][j][1]\n",
        "    # This uses the minimum value of the bbox (top-left) to determine Left, Right, Scorebox\n",
        "    # The Bellguards must be centered within the frame, classified as Bellguards with a minimum confidence and have the correct color saturation\n",
        "    # Adds values to the Left engarde box\n",
        "    if (bbox[j][1] > bellguard_confidence and bbox[j][0][1] < int(capture_width*2/5) and bbox[j][0][0] < int(capture_height*3/4) and bbox[j][0][0] > int(capture_height*1/4) and bbox[j][2] == 1):\n",
        "      test_result = saturation_test(bbox[j], frame_count, save_image_list)\n",
        "      if verbose == True:\n",
        "        display(f'The result of the saturation test for the Left Engarde Position is {test_result} at frame {frame_count}.')\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[0][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[0][1].append([bbox[j][0][0], bbox[j][1]])\n",
        "        #Appends x width value:\n",
        "        Bell_Guard_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Bell_Guard_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    #Adds values to the Right engarde box\n",
        "    elif (bbox[j][1] > bellguard_confidence and bbox[j][0][1] > int(capture_width*3/5) and bbox[j][0][0] < int(capture_height*3/4) and bbox[j][0][0] > int(capture_height*1/4) and bbox[j][2] == 1):\n",
        "      # test_result = color_tester(bbox[i][j], i)\n",
        "      test_result = saturation_test(bbox[j], frame_count, save_image_list)\n",
        "      if verbose == True:\n",
        "        display(f'The result of the saturation test for the Right Engarde Position is {test_result} at frame {frame_count}.')\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[1][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[1][1].append([bbox[j][0][0], bbox[j][1]])\n",
        "        #Appends x width value:\n",
        "        Bell_Guard_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Bell_Guard_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    #Adds values to the ScoreBox Position\n",
        "    elif (bbox[j][1] > 0.50 and bbox[j][0][1] > int(capture_width/3) and bbox[j][0][1] < int(capture_width*(2/3)) and bbox[j][2] == 2):\n",
        "      #Appends x value:\n",
        "      sum_of_boxes[2][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "      #Appends y value:\n",
        "      sum_of_boxes[2][1].append([bbox[j][0][0], bbox[j][1]])  \n",
        "      #Appends x width value:\n",
        "      Scoring_Box_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "      #Appends y width value:\n",
        "      Scoring_Box_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    # Adds values to the Left Foot Position\n",
        "    elif (bbox[j][1] > foot_confidence and bbox[j][0][1] < int(capture_width*2/5) and bbox[j][0][0] > int(capture_height*2/4) and bbox[j][2] == 4):\n",
        "      sum_of_boxes[5][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "      sum_of_boxes[5][1].append([bbox[j][0][0], bbox[j][1]])  \n",
        "      Foot_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "      Foot_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    # Adds values to the Right Foot Position\n",
        "    elif (bbox[j][1] > foot_confidence and bbox[j][0][1] > int(capture_width*2/5) and bbox[j][0][0] > int(capture_height*2/4) and bbox[j][2] == 4):\n",
        "      sum_of_boxes[6][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "      sum_of_boxes[6][1].append([bbox[j][0][0], bbox[j][1]])  \n",
        "      Foot_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "      Foot_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    else:\n",
        "      pass\n",
        "    \n",
        "    if bbox[j][2] == 1:\n",
        "      All_Bell_Guard_Positions.append([int((bbox[j][0][3] + bbox[j][0][1])/2), int((bbox[j][0][2] + bbox[j][0][0])/2)])\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Left Foot Sum of Boxes at frame {frame_count} is:')\n",
        "    display(sum_of_boxes[5])\n",
        "\n",
        "  try:\n",
        "    # Tests for cause of Left Engarde Position Failure\n",
        "    if len(sum_of_boxes[0][0]) == 0:\n",
        "      engarde_failure_test(bbox[j], bellguard_confidence, int(capture_width*2/5), int(capture_height*2/3), 'Left')\n",
        "    # Tests for cause of Right Engarde Position Failure\n",
        "    if len(sum_of_boxes[1][0]) == 0:\n",
        "      engarde_failure_test(bbox[j], bellguard_confidence, int(capture_width*3/5), int(capture_height*3/4), 'Right')\n",
        "  except:\n",
        "    if verbose == True:\n",
        "      display(f'There was an error in the engarde failure test and it was skipped.')\n",
        "\n",
        "  # Finds the center points\n",
        "  x_average_left = weight_average_list(sum_of_boxes[0][0])\n",
        "  y_average_left = weight_average_list(sum_of_boxes[0][1])\n",
        "  x_average_right = weight_average_list(sum_of_boxes[1][0])\n",
        "  y_average_right = weight_average_list(sum_of_boxes[1][1])\n",
        "  x_average_scorebox = weight_average_list(sum_of_boxes[2][0])\n",
        "  y_average_scorebox = weight_average_list(sum_of_boxes[2][1])\n",
        "  x_average_left_foot = weight_average_list(sum_of_boxes[5][0])\n",
        "  y_average_left_foot = weight_average_list(sum_of_boxes[5][1])\n",
        "  x_average_right_foot = weight_average_list(sum_of_boxes[6][0])\n",
        "  y_average_right_foot = weight_average_list(sum_of_boxes[6][1])\n",
        "\n",
        "  # Prevents a failure to detect the bellguard from failing to detect the torso\n",
        "  # If the bellguard is unusually high or low then it is set to the height of the opposing BellGuard\n",
        "  if (y_average_left < capture_height/5) or (y_average_left > capture_height*4/5):\n",
        "    if verbose == True:\n",
        "      display(f'The y_average_left was too high or low and was set to y_average_right.')\n",
        "    y_average_left = y_average_right\n",
        "  if (y_average_right < capture_height/5) or (y_average_right > capture_height*4/5):\n",
        "    if verbose == True:\n",
        "      display(f'The y_average_right was too high or low and was set to y_average_left.')\n",
        "    y_average_right = y_average_left\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average left position is ({x_average_left},{y_average_left}).')\n",
        "    display(f'The average right position is ({x_average_right},{y_average_right}).')\n",
        "\n",
        "  # Bell_Guard_Size_average [Width, Height]\n",
        "  Bell_Guard_Size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  Bell_Guard_Size_average.append(average_list(Bell_Guard_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  Bell_Guard_Size_average.append(average_list(Bell_Guard_Size[1]))\n",
        "\n",
        "  # Finds the Torso Position After the Bell_Guard Position because the Bell_Guard is used as a constraint\n",
        "  # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "  for j in range(len(bbox)):\n",
        "    # Adds values to the Left_Torso Position, similar requirements to Left guard\n",
        "    # Minimum Torso confidence, on the left half of the screen, bottom of the box is below the bellguard, but also above 3 three times the bellguard height and is labeled torso\n",
        "    if (bbox[j][1] > min_torso_confidence and bbox[j][0][1] < int(capture_width/2) and bbox[j][0][2] > (y_average_left - Bell_Guard_Size_average[1] * 2) \\\n",
        "        and bbox[j][0][2] < (y_average_left + 3*Bell_Guard_Size_average[1]) and bbox[j][2] == 3):\n",
        "      test_result = True\n",
        "      if test_result == True:\n",
        "        # Appends x value:\n",
        "        sum_of_boxes[3][0].append(bbox[j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[3][1].append(bbox[j][0][0])\n",
        "        #Appends x width value:\n",
        "        Left_Torso_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Left_Torso_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The saturation test failed at frame {frame_count}.')\n",
        "        else:\n",
        "          pass\n",
        "    # Adds values to the Right_Torso Position, similar requirements to Right guard\n",
        "    # display(f'y_average_right-Bell_Guard_Size[1] * 2 is {(y_average_right-Bell_Guard_Size[1]*2)}.')\n",
        "\n",
        "    if Bell_Guard_Size[1] == []:\n",
        "      if verbose == True:\n",
        "        display(f'The Bell Guard Height was not defined so it is set to a default of zero at frame {frame_count}.')\n",
        "      Bell_Guard_Size[1] = 0\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'bbox[j][1] is {bbox[j][1]}.')\n",
        "      display(f'bbox[j][0][1] is {bbox[j][0][1]}.')\n",
        "      display(f'bbox[j][0][2] is {bbox[j][0][2]}.')\n",
        "      display(f'y_average_right is {y_average_right}')\n",
        "      display(f'Bell_Guard_Size[1] is {Bell_Guard_Size[1]}')\n",
        "      display(f'(y_average_right-2*Bell_Guard_Size[1]) is {(y_average_right-2*Bell_Guard_Size[1])}.')\n",
        "      display(f'bbox[j][1] is {bbox[j][1]}.')\n",
        "\n",
        "    if (bbox[j][1] > min_torso_confidence and bbox[j][0][1] > int(capture_width/2) and bbox[j][0][2] > (y_average_right-2*Bell_Guard_Size_average[1]) and bbox[j][0][2] < (y_average_right + 3*Bell_Guard_Size_average[1]) and bbox[j][2] == 3):\n",
        "      \n",
        "      #Appends x value:\n",
        "      sum_of_boxes[4][0].append(bbox[j][0][1])\n",
        "      #Appends y value:\n",
        "      sum_of_boxes[4][1].append(bbox[j][0][0])\n",
        "      #Appends x width value:\n",
        "      Right_Torso_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "      #Appends y width value:\n",
        "      Right_Torso_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "\n",
        "  # Checks for a Failure to Detect the Left Torso\n",
        "  if len(sum_of_boxes[3][0]) == 0:\n",
        "    torso_failure_test(bbox, capture_width, capture_height, y_average_left, Bell_Guard_Size_average, 'Left', frame_count, min_torso_confidence)\n",
        "  if verbose == True:\n",
        "    display(f'Prior to torso failure test for right torso the y_average_left is {y_average_left}.')\n",
        "\n",
        "  # Checks for a Failure to Detect the Right Torso\n",
        "  if len(sum_of_boxes[4][0]) == 0:\n",
        "    torso_failure_test(bbox, capture_width, capture_height, y_average_right, Bell_Guard_Size_average, 'Right', frame_count, min_torso_confidence) \n",
        "  if verbose == True:\n",
        "    display(f'Prior to torso failure test for left torso the y_average_right is {y_average_right}.')\n",
        "\n",
        "  #Finds the top left corner then moves the average point to the center\n",
        "  x_average_left_torso = average_list(sum_of_boxes[3][0]) + average_list(Left_Torso_Size[0])/2\n",
        "  y_average_left_torso = average_list(sum_of_boxes[3][1]) + average_list(Left_Torso_Size[1])/2\n",
        "  x_average_right_torso = average_list(sum_of_boxes[4][0]) + average_list(Right_Torso_Size[0])/2\n",
        "  y_average_right_torso = average_list(sum_of_boxes[4][1]) + average_list(Right_Torso_Size[1])/2\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average left engarde position is:({x_average_left},{y_average_left})')\n",
        "    display(f'The average right engarde position is:({x_average_right},{y_average_right})')\n",
        "    display(f'The average left torso is:({int(x_average_left_torso)},{int(y_average_left_torso)})')\n",
        "    display(f'The average right torso is:({int(x_average_right_torso)},{int(y_average_right_torso)})')\n",
        "\n",
        "  # scoring_box_size_average [Width, Height]\n",
        "  scoring_box_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  scoring_box_size_average.append(average_list(Scoring_Box_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  scoring_box_size_average.append(average_list(Scoring_Box_Size[1]))\n",
        "\n",
        "  # left_torso_size_average [Width, Height]\n",
        "  left_torso_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  left_torso_size_average.append(average_list(Left_Torso_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  left_torso_size_average.append(average_list(Left_Torso_Size[1]))\n",
        "\n",
        "  # right_torso_size_average [Width, Height]\n",
        "  right_torso_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  right_torso_size_average.append(average_list(Right_Torso_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  right_torso_size_average.append(average_list(Right_Torso_Size[1]))\n",
        "\n",
        "  #Creates Padding for the EnGarde Tracking Box\n",
        "  engarde_box_padding = int(capture_width/15)\n",
        "  torso_padding = int(capture_width/20)\n",
        "  foot_padding = int(capture_width/15)\n",
        "\n",
        "  x_min_engardeL = int(x_average_left - engarde_box_padding)\n",
        "  x_max_engardeL = int(x_average_left + engarde_box_padding)\n",
        "  y_min_engardeL = int(y_average_left - engarde_box_padding)\n",
        "  y_max_engardeL = int(y_average_left + engarde_box_padding)\n",
        "\n",
        "  x_min_engardeR = int(x_average_right - engarde_box_padding)\n",
        "  x_max_engardeR = int(x_average_right + engarde_box_padding)\n",
        "  y_min_engardeR = int(y_average_right - engarde_box_padding)\n",
        "  y_max_engardeR = int(y_average_right + engarde_box_padding)\n",
        "\n",
        "  x_min_engardeScore = int(x_average_scorebox - engarde_box_padding)\n",
        "  x_max_engardeScore = int(x_average_scorebox + engarde_box_padding)\n",
        "  y_min_engardeScore = int(y_average_scorebox - engarde_box_padding)\n",
        "  y_max_engardeScore = int(y_average_scorebox + engarde_box_padding)\n",
        "\n",
        "  x_min_torsoL = int(x_average_left_torso - torso_padding)\n",
        "  x_max_torsoL = int(x_average_left_torso + torso_padding)\n",
        "  y_min_torsoL = int(y_average_left_torso - torso_padding*3/2)\n",
        "  y_max_torsoL = int(y_average_left_torso + torso_padding*3/2)\n",
        "\n",
        "  x_min_torsoR = int(x_average_right_torso - torso_padding)\n",
        "  x_max_torsoR = int(x_average_right_torso + torso_padding)\n",
        "  y_min_torsoR = int(y_average_right_torso - torso_padding*3/2)\n",
        "  y_max_torsoR = int(y_average_right_torso + torso_padding*3/2)\n",
        "\n",
        "  x_min_footL = int(x_average_left_foot)\n",
        "  x_max_footL = int(x_average_left_foot + foot_padding*2)\n",
        "  y_min_footL = int(y_average_left_foot - foot_padding)\n",
        "  y_max_footL = int(y_average_left_foot + foot_padding)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Left Foot xmin,xmax,ymin,ymax are {x_min_footL},{x_max_footL},{y_min_footL},{y_max_footL} at frame {frame_count}.')\n",
        "\n",
        "  # x Foot Padding is large because both feet are detected and differentiated in the subsequent steps\n",
        "  x_min_footR = int(x_average_right_foot - foot_padding*2)\n",
        "  x_max_footR = int(x_average_right_foot)\n",
        "  y_min_footR = int(y_average_right_foot - foot_padding)\n",
        "  y_max_footR = int(y_average_right_foot + foot_padding)\n",
        "\n",
        "  #Iterates through the first engarde_length frames and checks if there are rois in the expected engarde position\n",
        "  for j in range(len(bbox)):\n",
        "    y_center = int((bbox[j][0][0] + bbox[j][0][2])/2)\n",
        "    x_center = int((bbox[j][0][1] + bbox[j][0][3])/2)\n",
        "    # Checks for rois in the Left Engarde Position\n",
        "    if (x_center > x_min_engardeL and x_center < x_max_engardeL and y_center > y_min_engardeL and y_center < y_max_engardeL and bbox[j][2] == 1):\n",
        "      # display(f'The roi is in the left en garde position')\n",
        "      Left_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Right Engarde Position\n",
        "    if (x_center > x_min_engardeR and x_center < x_max_engardeR and y_center > y_min_engardeR and y_center < y_max_engardeR and bbox[j][2] == 1):\n",
        "      # display(f'The roi is in the right en garde position')\n",
        "      Right_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Scoring Box Position\n",
        "    if (x_center > x_min_engardeScore and x_center < x_max_engardeScore and y_center > y_min_engardeScore and y_center < y_max_engardeScore and bbox[j][2] == 2):\n",
        "      Scoring_Box_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Left Torso Position\n",
        "    if (x_center > x_min_torsoL and x_center < x_max_torsoL and y_center > y_min_torsoL and y_center < y_max_torsoL and bbox[j][0][2] > y_average_left and bbox[j][2] == 3):\n",
        "      Left_Torso_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Right Torso Position \n",
        "    if (x_center > x_min_torsoR and x_center < x_max_torsoR and y_center > y_min_torsoR and y_center < y_max_torsoR and bbox[j][0][2] > y_average_right and bbox[j][2] == 3):\n",
        "      Right_Torso_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Left Foot Position \n",
        "    if (x_center > x_min_footL and x_center < x_max_footL and y_center > y_min_footL and y_center < y_max_footL and bbox[j][0][2] > y_average_left and bbox[j][0][1] > x_average_left_torso and bbox[j][2] == 4):\n",
        "      Left_Foot_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Right Foot Position \n",
        "    if (x_center > x_min_footR and x_center < x_max_footR and y_center > y_min_footR and y_center < y_max_footR and bbox[j][0][2] > y_average_right and bbox[j][0][1] < x_average_right_torso and bbox[j][2] == 4):\n",
        "      Right_Foot_Position.append([x_center, y_center])\n",
        "\n",
        "    # Tracking Bounding Box has unused brackets for Left and Right Torso\n",
        "    Tracking_Bounding_Boxes_Temp = [[],[],[],[],[],[],[]]\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(x_min_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(x_max_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(y_min_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(y_max_engardeL)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(x_min_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(x_max_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(y_min_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(y_max_engardeR)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(x_min_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(x_max_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(y_min_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(y_max_engardeScore)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[5].append(x_max_footL)\n",
        "    Tracking_Bounding_Boxes_Temp[5].append(x_max_footL)\n",
        "    Tracking_Bounding_Boxes_Temp[5].append(y_min_footL)\n",
        "    Tracking_Bounding_Boxes_Temp[5].append(y_max_footL)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[6].append(x_max_footR)\n",
        "    Tracking_Bounding_Boxes_Temp[6].append(x_max_footR)\n",
        "    Tracking_Bounding_Boxes_Temp[6].append(y_min_footR)\n",
        "    Tracking_Bounding_Boxes_Temp[6].append(y_max_footR)\n",
        "\n",
        "    Tracking_Bounding_Boxes = Tracking_Bounding_Boxes_Temp\n",
        "\n",
        "  # Creates a Tracking Bounding Boxes Variable if there are no Bounding Box detections\n",
        "  if len(bbox) == 0:\n",
        "    Tracking_Bounding_Boxes = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[],[],[0,0,0,0],[0,0,0,0]]\n",
        "\n",
        "  # Tests for why a Torso Position is not Found\n",
        "  if (len(Left_Torso_Position) == 0):\n",
        "    torso_position_failure_test(bbox, engarde_length, x_min_torsoL, x_max_torsoL, y_min_torsoL, y_max_torsoL, y_average_left, 'Left', frame_count)    \n",
        "  if (len(Right_Torso_Position) == 0):\n",
        "    torso_position_failure_test(bbox, engarde_length, x_min_torsoR, x_max_torsoR, y_min_torsoR, y_max_torsoR, y_average_right, 'Right', frame_count)\n",
        "\n",
        "\n",
        "  # Averages the Left and Right x,y positions for engarde\n",
        "  # Left Bell Guard engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Left_Position) > 0:\n",
        "    for i in range(len(Left_Position)):\n",
        "      x = x + Left_Position[i][0]\n",
        "      y = y + Left_Position[i][1]\n",
        "    x = int(x/(len(Left_Position)))\n",
        "    y = int(y/(len(Left_Position)))\n",
        "    Left_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Left_Position at Engarde is:')\n",
        "    display(Left_Position)\n",
        "\n",
        "  # Right Bell Guard engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Right_Position) > 0:\n",
        "    for i in range(len(Right_Position)):\n",
        "      x = x + Right_Position[i][0]\n",
        "      y = y + Right_Position[i][1]\n",
        "    x = int(x/(len(Right_Position)))\n",
        "    y = int(y/(len(Right_Position)))\n",
        "    Right_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Right_Position at Engarde is:')\n",
        "    display(Right_Position)\n",
        "\n",
        "  # Scoring_Box engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Scoring_Box_Position) > 0:\n",
        "    for i in range(len(Scoring_Box_Position)):\n",
        "      x = x + Scoring_Box_Position[i][0]\n",
        "      y = y + Scoring_Box_Position[i][1]\n",
        "    x = int(x/(len(Scoring_Box_Position)))\n",
        "    y = int(y/(len(Scoring_Box_Position)))\n",
        "    Scoring_Box_Position = [x,y]\n",
        "\n",
        "  if Scoring_Box_Position == [0,0]:\n",
        "    Tracking_Bounding_Boxes_Temp[2] = [0,0,0,0]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Scoring_Box_Position at Engarde is:')\n",
        "    display(Scoring_Box_Position)\n",
        "\n",
        "  # Left_Torso engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Left_Torso_Position) > 0:\n",
        "    for i in range(len(Left_Torso_Position)):\n",
        "      x = x + Left_Torso_Position[i][0]\n",
        "      y = y + Left_Torso_Position[i][1]\n",
        "    x = int(x/(len(Left_Torso_Position)))\n",
        "    y = int(y/(len(Left_Torso_Position)))\n",
        "    Left_Torso_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Left_Torso_Position at Engarde is:')\n",
        "    display(Left_Torso_Position)\n",
        "\n",
        "  # Right_Torso engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Right_Torso_Position) > 0:\n",
        "    for i in range(len(Right_Torso_Position)):\n",
        "      x = x + Right_Torso_Position[i][0]\n",
        "      y = y + Right_Torso_Position[i][1]\n",
        "    x = int(x/(len(Right_Torso_Position)))\n",
        "    y = int(y/(len(Right_Torso_Position)))\n",
        "    Right_Torso_Position = [x,y]\n",
        "\n",
        "  # Left_Foot engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Left_Foot_Position) > 0:\n",
        "    for i in range(len(Left_Foot_Position)):\n",
        "      x = x + Left_Foot_Position[i][0]\n",
        "      y = y + Left_Foot_Position[i][1]\n",
        "    x = int(x/(len(Left_Foot_Position)))\n",
        "    y = int(y/(len(Left_Foot_Position)))\n",
        "    Left_Foot_Position = [x,y]\n",
        "\n",
        "  # Right_Foot engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Right_Foot_Position) > 0:\n",
        "    for i in range(len(Right_Foot_Position)):\n",
        "      x = x + Right_Foot_Position[i][0]\n",
        "      y = y + Right_Foot_Position[i][1]\n",
        "    x = int(x/(len(Right_Foot_Position)))\n",
        "    y = int(y/(len(Right_Foot_Position)))\n",
        "    Right_Foot_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Right_Torso_Position at Engarde is:')\n",
        "    display(Right_Torso_Position)\n",
        "\n",
        "  return (Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Tracking_Bounding_Boxes, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average, All_Bell_Guard_Positions, Left_Foot_Position, Right_Foot_Position, Foot_Size)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "c_0ssV1qoSLh"
      },
      "source": [
        "#@title Draw_Bell_Guard_Position\n",
        "def draw_Bell_Guard_Position(Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Left_Torso_Position, Right_Torso_Position, Left_Foot_Position, Right_Foot_Position, frame_count, Tracking_Bounding_Boxes, video_filename, capture_width, capture_height, engarde_length, score_box_empty, camera_steady, camera_motion_threshold, Exclusion_Areas, simple_clip_vector, save_image_list):\n",
        "  #Adds an overlay on the image to visualize the location of tracked objects\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/'\n",
        "  capture = cv2.VideoCapture(os.path.join(path, video_filename))\n",
        "\n",
        "  capture.set(cv2.CAP_PROP_FRAME_WIDTH, capture_width)\n",
        "  capture.set(cv2.CAP_PROP_FRAME_HEIGHT, capture_height)\n",
        "\n",
        "  #Color format is [B,G,R]\n",
        "  left_light_color_default = [[],[],[]]\n",
        "  right_light_color_default = [[],[],[]]\n",
        "  left_light_color = []\n",
        "  right_light_color = []\n",
        "\n",
        "  # Creates a list of Files from a Directory\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "\n",
        "\n",
        "  left_light_comparison, right_light_comparison, default_color = [], [], []\n",
        "\n",
        "  # for i, file in enumerate(files):\n",
        "  for i in range(len(save_image_list)):\n",
        "    # Reads the image\n",
        "    # name = os.path.join(path, file)\n",
        "    img = save_image_list[i]\n",
        "\n",
        "    # OpenCV uses Blue, Green, Red order\n",
        "    # Light_Color is of the format [[[B0],[G0],[R0]],[[B1],[G1],[R1]],[[B2],[G2],[R2]],...]\n",
        "    \n",
        "    if i <= engarde_length:\n",
        "      if scoring_box_size_average == [0,0]:\n",
        "        scoring_box_size_average = [int(capture_width/5), int(capture_height/5)]\n",
        "      if verbose == True:\n",
        "        display(f'The average scoring box size is {scoring_box_size_average}.')\n",
        "      # Uses a comparison of frames and scoring box position to determine the light off colors\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'The index i is {i}.')\n",
        "        display(f'The Score Box Position is: {Scoring_Box_Position}.')\n",
        "        display(f'The Score Box Position at i is {Scoring_Box_Position[i]}.')\n",
        "      [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = scoring_box_lights(img, Scoring_Box_Position[i], scoring_box_size_average, [], i, score_box_empty)\n",
        "      left_light_comparison.append(left_light_comparison_temp)\n",
        "      right_light_comparison.append(right_light_comparison_temp)\n",
        "      default_color.append(defualt_color_temp)\n",
        "      # Averages the Default Color on the Last iteration\n",
        "      if i == engarde_length:\n",
        "        b_temp = int(sum(default_color[0])/len(default_color[0]))\n",
        "        g_temp = int(sum(default_color[1])/len(default_color[1]))\n",
        "        r_temp = int(sum(default_color[2])/len(default_color[2]))\n",
        "        default_color = [b_temp,g_temp,r_temp]\n",
        "    elif i > engarde_length:\n",
        "      try:\n",
        "        [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = scoring_box_lights(img, Scoring_Box_Position[i], scoring_box_size_average, default_color, i, score_box_empty)\n",
        "      except:\n",
        "        if verbose == True:\n",
        "          display(f'Light Comparison Failed due to Error at frame {i}.')\n",
        "        [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = [0,0,[]]\n",
        "      left_light_comparison.append(left_light_comparison_temp)\n",
        "      right_light_comparison.append(right_light_comparison_temp)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The Left Position is: {Left_Position}.')\n",
        "      display(f'The iterator i is {i}.')\n",
        "\n",
        "    #Creates the dots on the Bell Guards\n",
        "    frame = cv2.circle(img, (Left_Position[i][0], Left_Position[i][1]), int(4*(capture_width/1280)), (118, 37, 217), -1)\n",
        "    frame = cv2.circle(frame, (Right_Position[i][0], Right_Position[i][1]), int(4*(capture_width/1280)), (157, 212, 19), -1)\n",
        "    frame = cv2.circle(frame, (Scoring_Box_Position[i][0], Scoring_Box_Position[i][1]), int(4*(capture_width/1280)), (255, 255, 0), -1)\n",
        "    frame = cv2.circle(frame, (Left_Torso_Position[i][0], Left_Torso_Position[i][1]), int(4*(capture_width/1280)), (0, 255, 0), -1)\n",
        "    frame = cv2.circle(frame, (Right_Torso_Position[i][0], Right_Torso_Position[i][1]), int(4*(capture_width/1280)), (255, 255, 0), -1)\n",
        "    # frame = cv2.circle(frame, (Left_Foot_Position[i][0], Left_Foot_Position[i][1]), int(4*(capture_width/1280)), (118, 37, 217), -1)\n",
        "    # frame = cv2.circle(frame, (Right_Foot_Position[i][0], Right_Foot_Position[i][1]), int(4*(capture_width/1280)), (157, 212, 19), -1)\n",
        "    \n",
        "    # Creates the Representative Bell Guard Position\n",
        "    frame = cv2.circle(img, (Left_Position[i][0], int(capture_height/2)), int(20*(capture_width/1280)), (118, 37, 217), -1)\n",
        "    frame = cv2.circle(frame, (Right_Position[i][0], int(capture_height/2)), int(20*(capture_width/1280)), (157, 212, 19), -1)\n",
        "    # frame = cv2.circle(frame, (Left_Foot_Position[i][0], int(capture_height*5/8)), int(10*(capture_width/1280)), (118, 37, 217), -1)\n",
        "    # frame = cv2.circle(frame, (Right_Foot_Position[i][0], int(capture_height*5/8)), int(10*(capture_width/1280)), (157, 212, 19), -1)\n",
        "\n",
        "    # Creates the Light Indicators\n",
        "    rect_size = int(capture_width/40)\n",
        "    if (simple_clip_vector[i][2] == 1):\n",
        "      #Creates the Left Score Light\n",
        "      frame = cv2.rectangle(frame, (rect_size, int(rect_size*1.5)), (rect_size*5, int(rect_size*4.5)), (0, 0, 255), -1)\n",
        "    if (simple_clip_vector[i][3] == 1):\n",
        "      #Creates the Right Score Light\n",
        "      frame = cv2.rectangle(frame, (capture_width - rect_size, int(rect_size*1.5)), (capture_width - rect_size*5, int(rect_size*4.5)), (0, 255, 0), -1)\n",
        "\n",
        "    # Adds Frame Number to the Image\n",
        "    text = 'Frame' + str(i)\n",
        "    frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*1/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, )\n",
        "    # Adds if the Camera Motion is detected by difference images\n",
        "    if camera_steady[i] > camera_motion_threshold:\n",
        "      text = 'Camera'\n",
        "      frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*3/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4, )\n",
        "      text = 'Motion'\n",
        "      frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*4/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4, )\n",
        "    \n",
        "    # Sets BellGuard Position Colors\n",
        "    left_color = (255, 0, 0)\n",
        "    right_color = (0, 255, 0)\n",
        "\n",
        "    #Creates the Tracking Boxes\n",
        "    frame = cv2.putText(frame, 'Tracking Box', (Tracking_Bounding_Boxes[i][0][0], Tracking_Bounding_Boxes[i][0][2]), cv2.FONT_HERSHEY_COMPLEX, 0.7, left_color, 2)\n",
        "    frame = cv2.rectangle(frame, (Tracking_Bounding_Boxes[i][0][0], Tracking_Bounding_Boxes[i][0][2]),(Tracking_Bounding_Boxes[i][0][1], Tracking_Bounding_Boxes[i][0][3]),left_color, 2)\n",
        "    frame = cv2.putText(frame, 'Tracking Box', (Tracking_Bounding_Boxes[i][1][0], Tracking_Bounding_Boxes[i][1][2]), cv2.FONT_HERSHEY_COMPLEX, 0.7, right_color, 2)\n",
        "    frame = cv2.rectangle(frame, (Tracking_Bounding_Boxes[i][1][0], Tracking_Bounding_Boxes[i][1][2]),(Tracking_Bounding_Boxes[i][1][1], Tracking_Bounding_Boxes[i][1][3]),right_color, 2)\n",
        "\n",
        "    # [frame, none] = overlay_keypoints(frame, keypoints[i][0], keypoints[i][1], True)\n",
        "\n",
        "    #Draws the Exclusion Areas\n",
        "    for j in range(len(Exclusion_Areas)):\n",
        "      frame = cv2.circle(frame, (Exclusion_Areas[j][0],Exclusion_Areas[j][1]), int(capture_width/80), (144,238,144), 2)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The Tracking Box for the Left Fencer at frame {i} is:')\n",
        "      display(f'{Tracking_Bounding_Boxes[i][0][0]},{Tracking_Bounding_Boxes[i][0][2]}')\n",
        "      display(f'The Tracking Box for the Right Fencer at frame {i} is:')\n",
        "      display(f'{Tracking_Bounding_Boxes[i][1][0]},{Tracking_Bounding_Boxes[i][1][2]}')\n",
        "\n",
        "    #Saves the image frame overwriting the original image\n",
        "    # name = os.path.join(path, file)\n",
        "    file = str(i) + '.jpg'\n",
        "    name = os.path.join(path, file)\n",
        "    # cv2.imwrite(name, frame)\n",
        "    save_image_list[i] = frame\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The Draw Bell Guard frame {i} is being saved at {name}.')\n",
        "\n",
        "  #Releases capture so that other files can be used\n",
        "  capture.release()\n",
        "\n",
        "  return (left_light_comparison, right_light_comparison, save_image_list)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPU_ZjXvkfla"
      },
      "source": [
        "def create_title_card(width, height, results, pred_total, name, img_directory):\n",
        "\n",
        "  # Tests for and creates if needed, a folder for the font\n",
        "  os.chdir('/content/')\n",
        "  if not exists('/content/font'):\n",
        "    os.mkdir('font')\n",
        "  os.chdir('/content/font')\n",
        "  if not exists('NotoSerif-hinted.zip'):\n",
        "    !wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSerif-hinted.zip\n",
        "    !unzip \"NotoSerif-hinted.zip\"\n",
        "  os.chdir('/content/')\n",
        "\n",
        "  # create same size image of background color\n",
        "  bg_color = (0,0,0)\n",
        "  # bg = np.full((img.shape), bg_color, dtype=np.uint8)\n",
        "  image = np.full((height, width,3), bg_color, dtype=np.uint8)\n",
        "\n",
        "  # draw text on bg\n",
        "  text_color = (255,255,255)\n",
        "  font = cv2.FONT_HERSHEY_TRIPLEX\n",
        "\n",
        "  scaling = width/1280\n",
        "  fontScale_header= 3.8*scaling\n",
        "  fontScale = 3.6*scaling\n",
        "  thickness = int(4*scaling)\n",
        "  certainty_cutoff = 0.1\n",
        "  \n",
        "\n",
        "  from PIL import Image\n",
        "\n",
        "  if (pred_total * 100) > certainty_cutoff:\n",
        "    # Creates the Text for the Title Card\n",
        "    text1 = \"Predicted Result\"\n",
        "    text2 = 'Left: ' + str(results[0]) + '%'\n",
        "    text3 = 'Right: ' + str(results[1]) + '%'\n",
        "    text4 = 'Simul: ' + str(results[2]) + '%'\n",
        "\n",
        "    # Gets the boundary of the text. It is approximate since it uses Hershey Text\n",
        "    text1size = cv2.getTextSize(text1, font, fontScale_header, thickness)[0]\n",
        "    text2size = cv2.getTextSize(text2, font, fontScale, thickness)[0]\n",
        "    text3size = cv2.getTextSize(text3, font, fontScale, thickness)[0]\n",
        "    text4size = cv2.getTextSize(text4, font, fontScale, thickness)[0]\n",
        "\n",
        "    # Defines the x and y values of the Text Blocks\n",
        "    text1x = int((image.shape[1] - text1size[0]) / 2)\n",
        "    text1y = int(height*0/10)\n",
        "    text2x = int((image.shape[1] - text2size[0]) / 2)\n",
        "    text2y = int(height*2/10)\n",
        "    text3x = int((image.shape[1] - text3size[0]) / 2)\n",
        "    text3y = int(height*4/10)\n",
        "    text4x = int((image.shape[1] - text4size[0]) / 2)\n",
        "    text4y = int(height*6/10)\n",
        "\n",
        "    # Uses PIL for a so that a custom font can be used\n",
        "    im = Image.fromarray(np.uint8(image))\n",
        "    font_pil_header = PIL.ImageFont.truetype('/content/font/NotoSerif-SemiBold.ttf', int(scaling*140))\n",
        "    font_pil = PIL.ImageFont.truetype('/content/font/NotoSerif-SemiBold.ttf', int(scaling*130))\n",
        "    draw  = PIL.ImageDraw.Draw(im)\n",
        "\n",
        "    # Draws the Text\n",
        "    draw.text((text1x, text1y), text1, fill=(255,255,255,255), font=font_pil_header)\n",
        "    draw.text((text2x, text2y), text2, fill=(255,255,255,255), font=font_pil)\n",
        "    draw.text((text3x, text3y), text3, fill=(255,255,255,255), font=font_pil)\n",
        "    draw.text((text4x, text4y), text4, fill=(255,255,255,255), font=font_pil)\n",
        "\n",
        "  else:\n",
        "    # Creates the Text for an Uncertain Result\n",
        "    text1 = \"Certainty is\"\n",
        "    text2 =  \"too Low\"\n",
        "    # Gets the boundary of the text. It is approximate since it uses Hershey Text\n",
        "    text1size = cv2.getTextSize(text1, font, fontScale, thickness)[0]\n",
        "    text2size = cv2.getTextSize(text2, font, fontScale, thickness)[0]\n",
        "    text1x = int((image.shape[1] - text1size[0]) / 2)\n",
        "    text1y = int(height*0/10)\n",
        "    text2x = int((image.shape[1] - text2size[0]) / 2)\n",
        "    text2y = int(height*2/10)\n",
        "\n",
        "    im = Image.fromarray(np.uint8(image))\n",
        "    font_pil_header = PIL.ImageFont.truetype('/content/font/NotoSerif-SemiBold.ttf', int(scaling*140))\n",
        "    font_pil = PIL.ImageFont.truetype('/content/font/NotoSerif-SemiBold.ttf', int(scaling*130))\n",
        "    draw  = PIL.ImageDraw.Draw(im)\n",
        "    draw.text((text1x, text1y), text1, fill=(255,255,255,255), font=font_pil)\n",
        "    draw.text((text2x, text2y), text2, fill=(255,255,255,255), font=font_pil)\n",
        "\n",
        "  # Converts the PIL image back into Numpy\n",
        "  image = np.array(im)\n",
        "\n",
        "  # Finds the Number of the Last frame\n",
        "  ROOT_DIR = '/content/Mask_RCNN/'\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, img_directory)\n",
        "  images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
        "  # Sorts the images by integer index\n",
        "  images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "  last_iterator = int(images[-1].split(img_directory + '/')[1][:-4])\n",
        "  display(f'The last image is {last_iterator}.')\n",
        "\n",
        "  # Saves the image\n",
        "  for i in range(25):\n",
        "    save_name = name + str(i + last_iterator + 1) + '.jpg'\n",
        "    save_name = os.path.join(VIDEO_SAVE_DIR, save_name)\n",
        "    cv2.imwrite(save_name, image)\n",
        "\n",
        "  return"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "YZNUlGxZAMYv"
      },
      "source": [
        "#@title Mask_Image\n",
        "def mask_image(frame, width, height, masking_box):\n",
        "  # Used to Mask parts of the image that are not of interest\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The masking box is:')\n",
        "    display(masking_box)\n",
        "\n",
        "  #Create the Mask\n",
        "  mask = np.zeros((height, width, 3), dtype = np.uint8);\n",
        "  for i in range(len(masking_box)):\n",
        "    mask = cv2.rectangle(mask, (masking_box[i][0], masking_box[i][2]) ,(masking_box[i][1], masking_box[i][3]), (255,255,255), -1)\n",
        "\n",
        "  #Applies the mask to Frame\n",
        "  frame = cv2.bitwise_and(mask, frame)\n",
        "\n",
        "  return (frame)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "NkcK4BxFc-lJ"
      },
      "source": [
        "#@title Create_Representative_Image\n",
        "def create_representative_image(clip_vector, capture_width, capture_height):\n",
        "  # Allows for an overlay that represents the bellguard horizontal motion and box lights\n",
        "\n",
        "  #Creates a Folder to save the images and removes previous version\n",
        "  os.chdir('/content/Mask_RCNN/videos')\n",
        "  # Removes and Recreates the Save_White_Dot to ensure the directory is empty\n",
        "  try:\n",
        "    shutil.rmtree('save_white_dot')\n",
        "    if verbose == True:\n",
        "      display(f'Removed the Save_White_Dot folder.')\n",
        "  except:\n",
        "    if verbose == True:\n",
        "      display(f'ERROR removing the Save_White_Dot folder.')\n",
        "  os.mkdir('save_white_dot')\n",
        "\n",
        "  rect_size = int(capture_width/40)\n",
        "\n",
        "  #Defines the File Path\n",
        "  path = r'/content/Mask_RCNN/videos/save_white_dot/'\n",
        "  \n",
        "  for i in range(len(clip_vector)):\n",
        "    img = np.zeros((capture_height,capture_width,3), np.uint8)\n",
        "\n",
        "    #Creates the Left Bell_Guard\n",
        "    img = cv2.circle(img, (clip_vector[i][0], int(capture_height/2)), 20, (118, 37, 217), -1)\n",
        "    #Creates the Right Bell_Guard\n",
        "    img = cv2.circle(img, (clip_vector[i][1], int(capture_height/2)), 20, (157, 212, 19), -1)\n",
        "\n",
        "    if (clip_vector[i][2] == 1):\n",
        "      #Creates the Left Score Light\n",
        "      img = cv2.rectangle(img, (rect_size, rect_size), (rect_size*5, rect_size*3), (0, 0, 255), -1)\n",
        "    if (clip_vector[i][3] == 1):\n",
        "      #Creates the Right Score Light\n",
        "      img = cv2.rectangle(img, (capture_width - rect_size, rect_size), (capture_width - rect_size*5, rect_size*3), (0, 255, 0), -1)\n",
        "\n",
        "    name = str(i) + '.jpg'\n",
        "    name = os.path.join(path, name)\n",
        "\n",
        "    cv2.imwrite(name, img)\n",
        "\n",
        "  return"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "kCrXhYq2f3sb"
      },
      "source": [
        "#@title Create_Overlay_Image\n",
        "def create_overlay_image(frame_count):\n",
        "  # Allows for an overlay that represents the bellguard horizontal motion and box lights\n",
        "\n",
        "  #Creates a Folder to save the images and removes previous version\n",
        "  os.chdir('/content/Mask_RCNN/videos/')\n",
        "  # !rm -r /content/Mask_RCNN/videos/overlay\n",
        "  # Attempts to remove the Overlay folder and recreate it to ensure that it is empty\n",
        "  try:\n",
        "    shutil.rmtree('overlay')\n",
        "  except:\n",
        "    display(f'ERROR removing the Overlay folder.')\n",
        "  # !mkdir overlay\n",
        "  ch\n",
        "\n",
        "\n",
        "  #Defines the File Path\n",
        "  path = r'/content/Mask_RCNN/videos/overlay/'\n",
        "  path_background = r'/content/Mask_RCNN/videos/save/'\n",
        "  path_foreground = r'/content/Mask_RCNN/videos/save_white_dot/'\n",
        "  for i in range(frame_count):\n",
        "    background_name = str(i) + '.jpg'\n",
        "    background_name = os.path.join(path_background, background_name)\n",
        "\n",
        "    foreground_name = str(i) + '.jpg'\n",
        "    foreground_name = os.path.join(path_foreground, foreground_name)\n",
        "    \n",
        "    background = cv2.imread(background_name)\n",
        "    foreground = cv2.imread(foreground_name)\n",
        "\n",
        "    added_image = cv2.addWeighted(background,0.8,foreground,1.0,0)\n",
        "\n",
        "    combined_name = str(i) + '.jpg'\n",
        "    combined_name = os.path.join(path, combined_name)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The file added image is saved at {combined_name}.')\n",
        "\n",
        "    cv2.imwrite(combined_name, added_image)\n",
        "\n",
        "  return"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "6RQ1thQAPxbc"
      },
      "source": [
        "#@title Light_Color_Comparison\n",
        "def light_color_comparison(light_color, light_color_default, color):\n",
        "  # Deterines if a light turned on based on a default color, an input color and expected color\n",
        "\n",
        "  light_comparison = []\n",
        "  # A high max distance is less sensitive and a lower max distance is more sensitive\n",
        "  max_distance_total = 180\n",
        "  max_distance_specific_color = 90\n",
        "\n",
        "  if color == 'Red':\n",
        "    color_specific = 2\n",
        "  elif color == 'Green':\n",
        "    color_specific = 1\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Color being analyzed is {color}.')\n",
        "    display(f'The default color is:')\n",
        "    display(light_color_default)\n",
        "    display(f'With the specific color being {light_color_default[color_specific]}')\n",
        "    display(f'The max distance total is {max_distance_total}.')\n",
        "    display(f'The max distance for a specific color is {max_distance_specific_color}.')\n",
        "\n",
        "  #i cycles through each light value corresponding to each frame\n",
        "  for i in range(len(light_color)):\n",
        "    distance = 0\n",
        "    for j in range(3):\n",
        "      distance = distance + (light_color[i][j] - light_color_default[j])**2\n",
        "\n",
        "    distance_specific_color = abs(light_color[i][color_specific] - light_color_default[color_specific])\n",
        "\n",
        "    distance = int((distance)**(0.5))\n",
        "    if vebose == True:\n",
        "      display(f'The distance is {distance} and the color specific distance is {distance_specific_color} for frame {i}.')\n",
        "    #0 is no color change from the default color)\n",
        "    if (distance > max_distance_total and distance_specific_color > max_distance_specific_color):\n",
        "      light_comparison.append(1)\n",
        "      if verbose == True:\n",
        "        display(f'The light is ON.')\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      light_comparison.append(0)\n",
        "      if verbose == True:\n",
        "        display(f'The light is OFF.')\n",
        "\n",
        "  return (light_comparison)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "IU0k7BCzVbOD"
      },
      "source": [
        "#@title Clip_Vector_Generator\n",
        "def clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous, width):\n",
        "  #Compiles the clip_vector that is used for the action analysis\n",
        "\n",
        "  # Allows for the assumption that both lights are on if the positions are close to each other.\n",
        "  # Useful if there is difficulty detecting the scoring box.\n",
        "  close_bellguards = False\n",
        "  # Once lights turn on it is assumed the lights stay on for the rest of the action\n",
        "  light_assumption = False\n",
        "\n",
        "  if len(Left_Position) != len(Right_Position):\n",
        "    display(f'The Left and Right Positions do not match up')\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  # This is either [] or the Previously saved Clip_Vector\n",
        "  clip_vector = clip_vector_previous\n",
        "\n",
        "  for i in range(len(Left_Position)):  \n",
        "    # Checks if the lights should be assumed on if they are not already\n",
        "    # Determines if the bellguards are close to each other\n",
        "    # if (abs(Left_Position[i][0] - Right_Position[i][0]) < width*.050) and (light_assumption == False):\n",
        "    if ((Right_Position[i][0] - Left_Position[i][0]) < width*position_difference_ratio) and (light_assumption == False):\n",
        "      close_bellguards = True\n",
        "\n",
        "    # Adjusts the clip vector to reflect scoring box light assumptions\n",
        "    clip_vector_temp = [[],[],[],[]]\n",
        "    clip_vector_temp[0] = Left_Position[i][0]\n",
        "    clip_vector_temp[1] = Right_Position[i][0]\n",
        "    if (assume_lights == True and close_bellguards == True) or light_assumption == True:\n",
        "      clip_vector_temp[2] = 1\n",
        "      clip_vector_temp[3] = 1\n",
        "      light_assumption = True\n",
        "    else:\n",
        "      if ignore_box_lights == True:\n",
        "        clip_vector_temp[2] = 0\n",
        "        clip_vector_temp[3] = 0\n",
        "      else:\n",
        "        clip_vector_temp[2] = left_light_comparison[i]\n",
        "        clip_vector_temp[3] = right_light_comparison[i]\n",
        "\n",
        "    clip_vector.append(clip_vector_temp)\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtuA8ocWoV9l"
      },
      "source": [
        "def simple_clip_vector_generator(Left_Position, Right_Position, width):\n",
        "  #Compiles the clip_vector that is used for the action analysis\n",
        "\n",
        "  # Allows for the assumption that both lights are on if the positions are close to each other.\n",
        "  # Useful if there is difficulty detecting the scoring box.\n",
        "  close_bellguards = False\n",
        "  # Once lights turn on it is assumed the lights stay on for the rest of the action\n",
        "  light_assumption = False\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Left Position is {Left_Position}.')\n",
        "    display(f'The Right Position is {Right_Position}.')\n",
        "\n",
        "  if len(Left_Position) != len(Right_Position):\n",
        "    display(f'The Left and Right Positions do not match up')\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  clip_vector = []\n",
        "\n",
        "  for i in range(len(Left_Position)):  \n",
        "    # Checks the lights should be assumed on if they are not already\n",
        "    # Determines if the bellguards are close to each other\n",
        "    if ((Right_Position[i][0] - Left_Position[i][0]) < width*position_difference_ratio) and (light_assumption == False):\n",
        "      close_bellguards = True\n",
        "\n",
        "    # Adjusts the clip vector to reflect scoring box light assumptions\n",
        "    clip_vector_temp = [[],[],[],[]]\n",
        "    clip_vector_temp[0] = Left_Position[i][0]\n",
        "    clip_vector_temp[1] = Right_Position[i][0]\n",
        "    if (assume_lights == True and close_bellguards == True) or light_assumption == True:\n",
        "      clip_vector_temp[2] = 1\n",
        "      clip_vector_temp[3] = 1\n",
        "      light_assumption = True\n",
        "    else:\n",
        "      clip_vector_temp[2] = 0\n",
        "      clip_vector_temp[3] = 0\n",
        "\n",
        "    clip_vector.append(clip_vector_temp)\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "5zNZqZbF3AKY"
      },
      "source": [
        "#@title Clip_Vector_Np_Save\n",
        "def clip_vector_np_save(clip_call, file_number, clip_vector, file_title):\n",
        "  # Saves the clip vector for future use\n",
        "  # Clip_Call Left_Touch, Right_Touch, Simul\n",
        "\n",
        "  # Generates the clip_vector speed based on the clip_vector\n",
        "  clip_vector_speed = []\n",
        "  for i in range(len(clip_vector)-1):\n",
        "    clip_vector_speed.append([])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][0]-clip_vector[i][0])\n",
        "    # Reverses the Right Fencers position so that positive is towards the opponent\n",
        "    clip_vector_speed[i].append(clip_vector[i][1]-clip_vector[i+1][1])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][2])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][3])\n",
        "\n",
        "  # Generates the clip_vector acceleration based on the clip_vector\n",
        "  clip_vector_acceleration = []\n",
        "  for i in range(len(clip_vector_speed)-1):\n",
        "    clip_vector_acceleration.append([])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][0]-clip_vector[i][0])\n",
        "    # Reverses the Right Fencers position so that positive is towards the opponent\n",
        "    clip_vector_acceleration[i].append(clip_vector[i][1]-clip_vector[i+1][1])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][2])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][3])\n",
        "\n",
        "  path = '/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "\n",
        "  # # Saves the clip_vector_acceleration\n",
        "  clip_vector_acceleration_np = np.asarray(clip_vector_acceleration)\n",
        "\n",
        "  # Saves a Copy to the Sync Folder\n",
        "  # if run_sync_folder == True or use_sync_folder == True:\n",
        "  if run_entire_sync_folder == True or run_most_recent_clip == True:\n",
        "    file_title = file_title[:-4]\n",
        "    clip_vector_acceleration_np_name = file_title + '_acc.csv'\n",
        "    sync_folder_acc = '/content/drive/My Drive/Sync/Acceleration'\n",
        "    os.chdir(sync_folder_acc)\n",
        "    np.savetxt(clip_vector_acceleration_np_name, clip_vector_acceleration_np, delimiter=',')\n",
        "    # os.chdir(name)\n",
        "\n",
        "  return"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "M1WK5HCkX_eX"
      },
      "source": [
        "#@title Left_Right_Test\n",
        "def Left_Right_Test(Left_Position, Right_Position):\n",
        "  # Requires that the Left and Right BellGuards be on the Left and Right sides respectively\n",
        "\n",
        "  #Left_Position is chosen arbitrarily for length\n",
        "  for i in range(len(Left_Position)):\n",
        "    if Left_Position[i][0] > Right_Position[i][0]:\n",
        "      if verbose == True:\n",
        "        display(f'The Left and Right were swapped on frame {i} and are now corrected.')\n",
        "      position_temp = Left_Position[i]\n",
        "      Left_Position[i] = Right_Position[i]\n",
        "      Right_Position[i] = position_temp\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return (Left_Position, Right_Position)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "uHKEKonnjipI"
      },
      "source": [
        "#@title Camera_Motion_Adjustment\n",
        "def camera_motion_adjustment(Position, Score_Box_Position):\n",
        "  # Takes a Position as an input and adjusts the position to compensate for camera motion\n",
        "  # Used solely the x position of the scoring box to calculate motion\n",
        "  # Ignores the change in angle as the camera is rotated\n",
        "  # This is only used when it is assumed that the Scoring Box is well detected and tracked\n",
        "\n",
        "  Score_Box_Position_Temp = []\n",
        "  #Converts Scoring Box Positions to solely x value\n",
        "  #Scoring Box Position is of the format [x0,x1,x2...]\n",
        "  for i in range(len(Score_Box_Position)):\n",
        "    Score_Box_Position_Temp.append(Score_Box_Position[i][0])\n",
        "\n",
        "  for j in range(len(Position)):\n",
        "    score_box_delta = Score_Box_Position_Temp[j] - Score_Box_Position_Temp[0]\n",
        "    Position[j][0] = Position[j][0] - score_box_delta\n",
        "\n",
        "  return (Position)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "YAKPWXQz0R4l"
      },
      "source": [
        "#@title Position_Down_Scale\n",
        "def position_down_scale(Position1, Position2, capture_width, capture_height):\n",
        "  # Scales the Position Down to the Capture Width in the x axis if required for visualization convenience\n",
        "  # Does not alter the Clip Vector Data\n",
        "  \n",
        "  position_temp = []\n",
        "\n",
        "  for i in range(len(Position1)):\n",
        "    position_temp.append(Position1[i][0])\n",
        "\n",
        "  for j in range(len(Position2)):\n",
        "    position_temp.append(Position2[j][0])\n",
        "\n",
        "  min_x_position = min(position_temp)\n",
        "  max_x_position = max(position_temp)\n",
        "\n",
        "  if min_x_position < 0:\n",
        "    #Shifts the bellguards to the right for the camera moving to the left\n",
        "    for i in range(len(Position1)):\n",
        "      Position1[i][0] = int(Position1[i][0] - min_x_position)\n",
        "\n",
        "    for j in range(len(Position2)):\n",
        "      Position2[j][0] = int(Position2[j][0] - min_x_position)\n",
        "\n",
        "  # Absolute Pixel\n",
        "  if max_x_position > capture_width:\n",
        "    #Scales the max x position if greater than the screen\n",
        "    for i in range(len(Position1)):\n",
        "      Position1[i][0] = int(Position1[i][0] * capture_width / max_x_position)\n",
        "\n",
        "    for j in range(len(Position2)):\n",
        "      Position2[j][0] = int(Position2[j][0] * capture_width / max_x_position)\n",
        "\n",
        "  return (Position1, Position2)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "qbuXubXE4XRf"
      },
      "source": [
        "#@title Random_Colors\n",
        "def random_colors(N):\n",
        "    np.random.seed(1)\n",
        "    colors = [tuple(255 * np.random.rand(3)) for _ in range(N)]\n",
        "    return colors"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "XFVyAiia4ZcB"
      },
      "source": [
        "#@title Apply_Mask\n",
        "def apply_mask(image, mask, color, alpha=0.5):\n",
        "    \"\"\"apply mask to image\"\"\"\n",
        "    for n, c in enumerate(color):\n",
        "        image[:, :, n] = np.where(\n",
        "            mask == 1,\n",
        "            image[:, :, n] * (1 - alpha) + alpha * c,\n",
        "            image[:, :, n]\n",
        "        )\n",
        "    return image"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "urekTB1e4bf4"
      },
      "source": [
        "#@title Display_Instances\n",
        "def display_instances(image, boxes, masks, ids, names, scores, file_name):\n",
        "    \"\"\"\n",
        "        take the image and results and apply the mask, box, and Label\n",
        "    \"\"\"\n",
        "    n_instances = boxes.shape[0]\n",
        "    colors = random_colors(n_instances)\n",
        "\n",
        "    if not n_instances:\n",
        "        print('NO INSTANCES TO DISPLAY')\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        if not np.any(boxes[i]):\n",
        "            continue\n",
        "\n",
        "        y1, x1, y2, x2 = boxes[i]\n",
        "        # label = boxes[i][4]\n",
        "        label = names[ids[i]]\n",
        "        score = scores[i] if scores is not None else None\n",
        "        caption = '{} {:.2f}'.format(label, score) if score else label\n",
        "        mask = masks[:, :, i]\n",
        "        # display(f'The mask is: {mask}')\n",
        "        # image = apply_mask(image, mask, color)\n",
        "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "        image = cv2.putText(\n",
        "            image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2\n",
        "        )\n",
        "\n",
        "    return image"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "3UhmJ-_-kyG3"
      },
      "source": [
        "#@title Smooth_Clip_Vector\n",
        "def smooth_clip_vector(clip_vector, engarde_length):\n",
        "  # Allows for smoothing the clip_vector\n",
        "\n",
        "  a = []\n",
        "  b = []\n",
        "  for i in range(engarde_length, len(clip_vector)):\n",
        "    a.append(clip_vector[i][0])\n",
        "    b.append(clip_vector[i][1])\n",
        "\n",
        "  x = np.linspace(engarde_length,len(clip_vector), len(clip_vector) - engarde_length)\n",
        "\n",
        "  # sos = signal.ellip(13, 0.009, 80, 0.05, output='sos')\n",
        "  # yhata = signal.sosfilt(sos, a)\n",
        "  if len(a)%2 == 1:\n",
        "    yhata = signal.savgol_filter(a, len(a), 11)\n",
        "    yhatb = signal.savgol_filter(b, len(b), 11)\n",
        "  else:\n",
        "    yhata = signal.savgol_filter(a, len(a) - 1, 11)\n",
        "    yhatb = signal.savgol_filter(b, len(b) - 1, 11)    \n",
        "\n",
        "  # plt.plot(x,a, color='black')\n",
        "  # plt.plot(x,yhata, color='red')\n",
        "  plt.plot(x,b, color='black')\n",
        "  plt.plot(x,yhatb, color='blue')\n",
        "  plt.show()\n",
        "\n",
        "  vector_clip_smooth = []\n",
        "\n",
        "  for j in range(len(clip_vector)):\n",
        "    if j <= engarde_length:\n",
        "      clip_vector_smooth_temp = [clip_vector[j][0], clip_vector[j][1], clip_vector[j][2], clip_vector[j][3]]\n",
        "    else:\n",
        "      clip_vector_smooth_temp = [int(yhata[j - engarde_length]), int(yhatb[j - engarde_length]), clip_vector[j][2], clip_vector[j][3]]\n",
        "    vector_clip_smooth.append(clip_vector_smooth_temp)\n",
        "\n",
        "  return (vector_clip_smooth)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Ug5H_4QojaEk"
      },
      "source": [
        "#@title Load_Clip_Vector\n",
        "def load_clip_vector():\n",
        "  # Only used for large clips\n",
        "\n",
        "  display(f'Loading the Clip Vector...')\n",
        "  filename = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Temp_Clip_Vector/Temp_Clip_Vector_Clips/clip_vector_np1.csv'\n",
        "\n",
        "  display(f'Attempting to load:')\n",
        "  display(filename)\n",
        "  try:\n",
        "    vector_data = pd.read_csv(filename, header=None)\n",
        "    arr = vector_data.to_numpy(dtype = np.int32)\n",
        "    clip_vector = arr.tolist()\n",
        "  except:\n",
        "    display(f'Load Failure...')\n",
        "    display(f'The clip_vector did not exist so it is set to []')\n",
        "    clip_vector = []\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "22PKpGGUnmKK"
      },
      "source": [
        "#@title Create_Tracking_Masks\n",
        "def create_tracking_masks(previous_positions, certainty, frame_count, torso_size, width, height):\n",
        "  #Creates Tracking Boxes that can be used to mask the image, ignoring parts that are not of interest\n",
        "  #Format, Tracking_Boxes = [Left, Right, Scorebox], Left = [x_min, x_max, y_min, y_max]\n",
        "  #Format, Previous Positions\n",
        "          # previous_positions  = [[Left_Position[-1], Left_Position[-2]], \\\n",
        "          #                      [Right_Position[-1], Right_Position[-2]], \\\n",
        "          #                      [Scoring_Box_Position[-1], Scoring_Box_Position[-2]], \\\n",
        "          #                      [Left_Torso_Position[-1], Left_Torso_Position[-2]], \\\n",
        "          #                      [Right_Torso_Position[-1], Right_Torso_Position[-2]]]\n",
        "\n",
        "          #Format, positions are [x,y]\n",
        "\n",
        "  #Format, torso_position = [[Left_x,Lefty],[Right_x,Right_y]]\n",
        "  #Format, torso_size = [[Lw,Lh], [Rw,Rh]]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Creating Tracking Masks...')\n",
        "    display(f'The Previous Positions are:')\n",
        "    display(previous_positions)\n",
        "    display(f'The torso sizes are:')\n",
        "    display(torso_size)\n",
        "\n",
        "  frame_mask = []\n",
        "\n",
        "  #Certainty is the number of times the bellguard has not been detected in previous frames\n",
        "  #certainty_default is the minimum size of the tracking box\n",
        "  certainty_default = int(width/16)\n",
        "  #certainty_multiplier is how much the tracking box enlarges following a missed\n",
        "  certainty_multiplier = int(width/80)\n",
        "  y_limiter = 24\n",
        "\n",
        "  #Max allowed speed of a bellguard in a single frame\n",
        "  max_speed = int(width/48)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The length of the previous positions is: {len(previous_positions)}.')\n",
        "\n",
        "  for i in range(len(previous_positions)):\n",
        "    if verbose == True:\n",
        "      display(f'The masking iteration for frame {frame_count} is {i}.')\n",
        "    #FINDS THE LEFT MASKING BOX\n",
        "    x_pos = previous_positions[i][0][0]\n",
        "    y_pos = previous_positions[i][0][1]\n",
        "    #Converts previous position into a speed\n",
        "    x_speed = min(previous_positions[i][0][0] - previous_positions[i][1][0], max_speed)\n",
        "    # Limits the maximum vertical speed with relation to x\n",
        "    y_speed = min(previous_positions[i][0][1] - previous_positions[i][1][1], int(max_speed/y_limiter))\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'x and y position is ({x_pos},{y_pos}) and the speeds are ({x_speed},{y_speed}).')\n",
        "\n",
        "    x_min = x_pos + (x_speed) - (certainty[i]*certainty_multiplier) - certainty_default\n",
        "    x_max = x_pos + (x_speed) + (certainty[i]*certainty_multiplier) + certainty_default\n",
        "    y_min = y_pos + (y_speed) - (certainty[i]*certainty_multiplier) - certainty_default\n",
        "    y_max = y_pos + (y_speed) + (certainty[i]*certainty_multiplier) + certainty_default\n",
        "\n",
        "    #Appends the mask to collection of tracked areas\n",
        "    frame_mask.append([x_min, x_max, y_min, y_max])\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Frame Mask for frame {frame_count} is:')\n",
        "    display(frame_mask)\n",
        "\n",
        "  return(frame_mask)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Wrtkw3MVw-H4"
      },
      "source": [
        "def make_video(outvid, images=None, fps=25, size=None,\n",
        "               is_color=True, format=\"FMP4\"):\n",
        "  \n",
        "    \"\"\"\n",
        "    Create a video from a list of images.\n",
        " \n",
        "    @param      outvid      output video\n",
        "    @param      images      list of images to use in the video\n",
        "    @param      fps         frame per second\n",
        "    @param      size        size of each frame\n",
        "    @param      is_color    color\n",
        "    @param      format      see http://www.fourcc.org/codecs.php\n",
        "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
        " \n",
        "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
        "    By default, the video will have the size of the first image.\n",
        "    It will resize every image to this size before adding them to the video.\n",
        "    \"\"\"\n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*format)\n",
        "    vid = None\n",
        "    for image in images:\n",
        "        if not os.path.exists(image):\n",
        "            raise FileNotFoundError(image)\n",
        "        img = imread(image)\n",
        "        if vid is None:\n",
        "            if size is None:\n",
        "                size = img.shape[1], img.shape[0]\n",
        "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
        "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
        "            img = resize(img, size)\n",
        "        vid.write(img)\n",
        "    vid.release()\n",
        "    return vid"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "VZZzHy8XnxaE"
      },
      "source": [
        "def mean_of_a_numpy_percentile(arr, percentile_cutoff):\n",
        "  # Returns a percentile value of a numpy array\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average of arr is {np.average(arr)}.')\n",
        "\n",
        "  percentile_value = np.percentile(arr, percentile_cutoff)\n",
        "\n",
        "  # Uses just the percentile without averaging\n",
        "  array_percentile_mean = percentile_value\n",
        "\n",
        "  return (array_percentile_mean)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "D5E4dnezv1Mf"
      },
      "source": [
        "#@title Frame_Comparison_SSIM\n",
        "def frame_comparison_ssim(frame_number, frame, width, height, engarde_length):\n",
        "\n",
        "  # Determines if the subsequent frame is identical to the current or if there was camera motion\n",
        "  # Uses Mean Square Error and Structural Similarity Index\n",
        "\n",
        "  save_path = r'/content/Mask_RCNN/videos/original/'\n",
        "  # image_num = frame_number\n",
        "  image_name1 = str(frame_number-1) + '.jpg'\n",
        "  file_name1 = os.path.join(save_path, image_name1)\n",
        "  # file_name2 = os.path.join(save_path, image_name2)\n",
        "\n",
        "  image1 = cv2.imread(file_name1)\n",
        "  image2 = frame\n",
        "\n",
        "  # Uses a tighter crop for engarde positioning to minimize motion outside the bout\n",
        "  if frame_number <= engarde_length:\n",
        "    crop_image1 = image1[int(height*1/5):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*1/5):int(height*3/4), 0:width]\n",
        "  else:\n",
        "    # Removes the bottom of the frame to minimize the effect of overlays and shadowing in the foreground\n",
        "    crop_image1 = image1[int(height*0):int(height*2/4), 0:width]\n",
        "    crop_image2 = image2[int(height*0):int(height*2/4), 0:width]\n",
        "\n",
        "  # Calculate MSE\n",
        "  m = np.linalg.norm(image1 - image2)\n",
        "  \n",
        "  # # If GrayScale\n",
        "  # s = ssim(imageA, imageB)\n",
        "  # If Color\n",
        "  s = ssim(crop_image1, crop_image2, multichannel=True)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Mean Square Error of frame {frame_number} is {m}.')\n",
        "    display(f'The Structural Similarity Index of frame {frame_number} is {s}.')\n",
        "\n",
        "  return(m, s)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "-jpMiJc_pD9c"
      },
      "source": [
        "\n",
        "#@title Frame_Comparison\n",
        "def frame_comparison(frame_number, frame, width, height, engarde_length, original_image_list):\n",
        "  # Determines if the subsequent frame is identical to the current or if there was camera motion\n",
        "  # By calculating an average Hue from an HSV image. The Hue is then correlated to an average \n",
        "  # color difference between frames.\n",
        "\n",
        "  image1 = original_image_list[frame_number - 1]\n",
        "  image2 = frame\n",
        "\n",
        "  # Uses a tighter crop for engarde positioning to minimize motion outside the bout\n",
        "  if frame_number <= engarde_length:\n",
        "    crop_image1 = image1[int(height*1/5):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*1/5):int(height*3/4), 0:width]\n",
        "  else:\n",
        "    # Removes the bottom of the frame to minimize the effect of overlays and shadowing in the foreground\n",
        "    crop_image1 = image1[int(height*0):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*0):int(height*3/4), 0:width]\n",
        "\n",
        "  #Convert to Grayscale and find the Difference\n",
        "  image1_gray = cv2.cvtColor(crop_image1, cv2.COLOR_BGR2GRAY)\n",
        "  image2_gray = cv2.cvtColor(crop_image2, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "\n",
        "  # Finds the HSV of image2\n",
        "  image2_HSV = cv2.cvtColor(image2, cv2.COLOR_BGR2HSV)\n",
        "  h_average = np.average(image2_HSV[0])\n",
        "\n",
        "  # Uses Uncropped Frames\n",
        "  # image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "  # image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
        "  # image_diff = cv2.absdiff(image1_gray,image2_gray)\n",
        "  image_diff = cv2.absdiff(crop_image1,crop_image2)\n",
        "  # image_diff_color = cv2.absdiff(image1,image2)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The max for the difference of frame {frame_number} is {np.amax(image_diff)}.')\n",
        "    display(f'The average/median for the difference of frame {frame_number} is {np.average(image_diff)}/{np.median(image_diff)}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The shape of image_diff is {image_diff.shape}.')\n",
        "\n",
        "  average_image_diff = np.average(image_diff)\n",
        "  if verbose == True:\n",
        "    display(f'The image difference average is {np.average(image_diff)} for frame {frame_number}.')\n",
        "\n",
        "  # average_diff = np.average(image_diff)\n",
        "  # average_diff = np.average(image_diff_color)\n",
        "\n",
        "  return(average_image_diff, h_average)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awUaS8HiKKIK"
      },
      "source": [
        "def read_video(file_name, folder):\n",
        "  # Reads a video file and returns the video as a list of arrays for each frame and width, height, fps\n",
        "\n",
        "  display(f'The file to be read is {os.path.join(folder, file_name)}')\n",
        "\n",
        "  video_array_list = []\n",
        "  capture = cv2.VideoCapture(os.path.join(folder, file_name))\n",
        "  \n",
        "  fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
        "  # Prevents unusually large FPS\n",
        "  if fps > 100:\n",
        "    fps = 30\n",
        "\n",
        "  # capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
        "  # capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "\n",
        "  while True:\n",
        "    ret, frame = capture.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    video_array_list.append(frame)\n",
        "\n",
        "  capture.release()\n",
        "\n",
        "  return(video_array_list, fps)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "fop2haomlTNr"
      },
      "source": [
        "#@title Test_and_Remove_Duplicate_Frames\n",
        "def test_and_remove_duplicate_frames(original_image_list, engarde_length, remove_duplicate_frames):\n",
        "  # Creates a List of unique frames with by comparing the previous and current frames\n",
        "  # This compensates for video compression that may give duplicate frames when FPS is changed\n",
        "\n",
        "  camera_steady = []\n",
        "  engarde_diff_average_arr = np.array([])\n",
        "  engarde_hue_average_arr = np.array([])\n",
        "\n",
        "  # original_image_list = []\n",
        "  original_image_worpt_list = []\n",
        "\n",
        "  #Tests for the Same Frames and removes identical frames\n",
        "  # frame_count = 0\n",
        "  num_of_deleted_frames = 0\n",
        "  frame_test = True\n",
        "\n",
        "  # while True:\n",
        "  #   ret, frame = capture.read()\n",
        "  #   if not ret:\n",
        "  #     break\n",
        "  #   if verbose == True:\n",
        "  #     display(f'frame_count is {frame_count}.')\n",
        "\n",
        "  for i in range(len(original_image_list)):\n",
        "    # #Saves an original version of the frame without Regions of Interest\n",
        "\n",
        "    if i > 0:\n",
        "      if verbose == True:\n",
        "        display(f'Performing Difference Check for Frame {i}')\n",
        "      # True implies a unique frame while False is a repeat\n",
        "      # [average_diff, h_average] = frame_comparison(frame_count, frame, width, height, frame_count, engarde_length, original_image_list)\n",
        "      [average_diff, h_average] = frame_comparison(i, original_image_list[i], width, height, engarde_length, original_image_list)\n",
        "\n",
        "      # Uses the Engarde Positioning to determine a baseline difference level between frames\n",
        "      if i < engarde_length:\n",
        "        # engarde_diff_average_list.append(average_diff)\n",
        "        engarde_diff_average_arr = np.append(engarde_diff_average_arr, average_diff)\n",
        "        engarde_hue_average_arr = np.append(engarde_hue_average_arr, h_average)\n",
        "\n",
        "      elif i == engarde_length:\n",
        "\n",
        "        engarde_diff_average_arr = np.append(engarde_diff_average_arr, average_diff)\n",
        "        engarde_hue_average_arr = np.append(engarde_hue_average_arr, h_average)\n",
        "\n",
        "        # Emperically Derived Threshold Based on Hue\n",
        "        duplicate_threshold = np.average(engarde_hue_average_arr) * -.037+3.8663\n",
        "        # duplicate_threshold = np.average(engarde_hue_average_arr) * -.037+ 1.5663\n",
        "        camera_motion_threshold = np.percentile(engarde_diff_average_arr, 40) * camera_motion_threshold_factor\n",
        "                \n",
        "        if verbose == True:\n",
        "          display(f'The Duplicate Threshold at the Engarde Length is {duplicate_threshold}.')\n",
        "          display(f'The Camera Motion Threshold at the Engarde Length is {camera_motion_threshold}.')\n",
        "\n",
        "      elif i > engarde_length:\n",
        "        if average_diff < duplicate_threshold:\n",
        "          if verbose == True:\n",
        "            display(f'The frame {i} is identical to frame {i - 1}.')\n",
        "          frame_test = False\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'Frame {i} is unique.')\n",
        "          frame_test = True\n",
        "        # display(f'{frame_count} greater than engarde length')\n",
        "\n",
        "    # Saves the Image in Either Original or Original and Without Repeat\n",
        "    # Excludes frames that are Part of the Engarde Positioning\n",
        "    if (frame_test == True) or (i <= engarde_length) or (remove_duplicate_frames == False):\n",
        "      # cv2.imwrite(name_orig, frame)\n",
        "      # cv2.imwrite(name_orig_worpt, frame)\n",
        "\n",
        "      # original_image_list.append(frame)\n",
        "      original_image_worpt_list.append(original_image_list[i])\n",
        "\n",
        "      if i > 0:\n",
        "        camera_steady.append(average_diff)\n",
        "      else:\n",
        "        camera_steady.append(0)\n",
        "    else:\n",
        "      # cv2.imwrite(name_orig, frame)\n",
        "      # original_image_list.append(frame)\n",
        "      num_of_deleted_frames += 1\n",
        "\n",
        "  return (camera_steady, duplicate_threshold, camera_motion_threshold, original_image_list, original_image_worpt_list)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTGdGUgMVaBG"
      },
      "source": [
        "def setup_clip_directories():\n",
        "  # Creates the Save, Original and Original without Repeats Folders\n",
        "\n",
        "  os.chdir('/content/')\n",
        "\n",
        "  if not exists('/content/Mask_RCNN'):\n",
        "    os.mkdir('Mask_RCNN')\n",
        "\n",
        "  os.chdir('/content/Mask_RCNN/')\n",
        "\n",
        "  display(f'os.getcwd() is: {os.getcwd()}')\n",
        "  ROOT_DIR = os.getcwd()\n",
        "  MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"save\")\n",
        "  VIDEO_ORIG_DIR = os.path.join(VIDEO_DIR, \"original\")\n",
        "  VIDEO_ORIGWORPT_DIR = os.path.join(VIDEO_DIR, \"original_without_repeats\")\n",
        "  display(f'The ROOT_DIR is: {ROOT_DIR}')\n",
        "\n",
        "  # Removes and recreates the save directory effectively emptying the folder\n",
        "  # Attempts to remove folders to ensure folders are empty\n",
        "  try:\n",
        "    shutil.rmtree('videos')\n",
        "  except:\n",
        "    display(f'The video directory did not exist.')\n",
        "  os.mkdir('videos')\n",
        "  os.chdir('/content/Mask_RCNN/videos')\n",
        "  os.mkdir('save')\n",
        "  os.mkdir('original')\n",
        "  os.mkdir('original_without_repeats')\n",
        "\n",
        "\n",
        "  return(ROOT_DIR, VIDEO_DIR, VIDEO_SAVE_DIR, VIDEO_ORIG_DIR)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne8g4EqqmR92"
      },
      "source": [
        "def run_human_pose(image, fencer_data, keypoints, already_run):\n",
        "  # Runs Human Pose Analysis on an Image and returns fencer data and keypoints\n",
        "  # Already Run ensures that it runs at most one time for a frame\n",
        "\n",
        "  if already_run == False:\n",
        "    # Uses Human Pose Analysis for Keypoints\n",
        "    [fencer_data_temp, keypoints_temp] = human_pose_analysis(image)\n",
        "    display(f'The fencer data is:')\n",
        "    display(fencer_data)\n",
        "    fencer_data.append(fencer_data_temp)\n",
        "    keypoints.append(keypoints_temp)\n",
        "    already_run = True\n",
        "\n",
        "  return(fencer_data, keypoints, already_run)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "mw5PNoa-gmJa"
      },
      "source": [
        "#@title Process_Video_Clip\n",
        "def process_video_clip(file_name, touch_folder, remove_duplicate_frames):\n",
        "  # Processes the video\n",
        "\n",
        "  display(f'The video file_name is: {file_name}')\n",
        "\n",
        "  # Initiates Conditions\n",
        "  score_box_empty = False\n",
        "  right_torso_empty = False\n",
        "  left_torso_empty = False\n",
        "  left_position_empty = False\n",
        "  right_position_empty = False\n",
        "  left_foot_empty = False\n",
        "  right_foot_empty = False\n",
        "\n",
        "  save_image_list = []\n",
        "  Left_Position = []\n",
        "  Right_Position = []\n",
        "  Scoring_Box_Position = []\n",
        "  Left_Torso_Position = []\n",
        "  Right_Torso_Position = []\n",
        "  Left_Foot_Position = []\n",
        "  Right_Foot_Position = []\n",
        "  left_torso_size_average = []\n",
        "  right_torso_size_average = []\n",
        "  Tracking_Bounding_Boxes = []\n",
        "  All_Bell_Guard_Positions = []\n",
        "  Exclusion_Areas = []\n",
        "  bbox_clip = []\n",
        "\n",
        "  skip_frames = False\n",
        "  skip_frame_counter = 0\n",
        "  frames_to_skip = 300\n",
        "  number_of_frames_skipped = 0\n",
        "\n",
        "  clip_vector_previous = []\n",
        "\n",
        "  # Removes and Recreates Save, Original and Original without Repeats folders to ensure empty and moves the clip to the appropriate directory\n",
        "  [ROOT_DIR, VIDEO_DIR, VIDEO_SAVE_DIR, VIDEO_ORIG_DIR] = setup_clip_directories()\n",
        "\n",
        "  engarde_length = 10\n",
        "\n",
        "  video_clip_dir = '/content/drive/My Drive/Sync/'\n",
        "  [original_image_list, fps] = read_video(file_name, video_clip_dir)\n",
        "\n",
        "  # Removes Duplicates and Detects Camera motion in Frames\n",
        "  [camera_steady, duplicate_threshold, camera_motion_threshold, original_image_list, original_image_worpt_list] \\\n",
        "  = test_and_remove_duplicate_frames(original_image_list, engarde_length, remove_duplicate_frames)\n",
        "\n",
        "  width = original_image_worpt_list[0].shape[1]\n",
        "  height = original_image_worpt_list[0].shape[0]\n",
        "\n",
        "  # Determines the Starting Frame of the file\n",
        "  [starting_frame, total_frames] = determine_starting_frame(original_image_worpt_list, width, height)\n",
        "\n",
        "  start_time = starting_frame/fps\n",
        "  end_time = total_frames/fps\n",
        "\n",
        "  orig_img_worpt_starting_list = original_image_worpt_list[starting_frame:]\n",
        "\n",
        "  if total_frames == 0:\n",
        "    display(f'ERROR: The Video Clip selected has no frames.')\n",
        "  display(f'The total number of frames in the video are: {total_frames}')\n",
        "\n",
        "  try:\n",
        "    if not os.path.exists(VIDEO_SAVE_DIR):\n",
        "          os.makedirs(VIDEO_SAVE_DIR)\n",
        "  except OSError:\n",
        "    print ('Error: Creating directory of data')\n",
        "\n",
        "  frame_count = 0\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The capture width is: {width}')\n",
        "    display(f'The capture height is: {height}')\n",
        "  \n",
        "  # Continues until it breaks by not finding a return from attempting to read a frame capture\n",
        "\n",
        "  # Creates a variable that allows looping if engarde positioning is not effective\n",
        "  perform_engarde_positioning = True\n",
        "  engarde_offset = 0\n",
        "  # The number of frames that is incremented when an engarde positioning fails\n",
        "  offset_increment = 10\n",
        "\n",
        "  close_bellguards = False\n",
        "  too_few_frames = False\n",
        "  frames_after_light_iterator = 0\n",
        "  frames_after_light_max = 15\n",
        "  minimum_frames_to_run = 20\n",
        "\n",
        "  if (total_frames - starting_frame) > minimum_frames_to_run:\n",
        "    display(f'The remaining frames are {(total_frames - starting_frame)}, which is greater than {minimum_frames_to_run}.')\n",
        "  else:\n",
        "    display(f'The remaining frames are {(total_frames - starting_frame)}, which is fewer than {minimum_frames_to_run}.')\n",
        "\n",
        "  # if ((total_frames - starting_frame) =< minimum_frames_to_run):\n",
        "  if ((total_frames - starting_frame - engarde_offset) <= minimum_frames_to_run):\n",
        "    display(f'Too Few Frames to run {file_name}')\n",
        "    too_few_frames = True\n",
        "\n",
        "\n",
        "  # Runs for 5 frames after the lights turn on and required at least 30 frames total\n",
        "  while (total_frames - starting_frame) > minimum_frames_to_run and (frame_count) < len(orig_img_worpt_starting_list):\n",
        "    # Iterator\n",
        "    frame_count += 1\n",
        "    #Creates Bounding Box List\n",
        "    frame = orig_img_worpt_starting_list[frame_count - 1]\n",
        "    frames = [frame]\n",
        "\n",
        "    # Runs the Detection Model for Bellguard, Torso, Scoring_Box\n",
        "    [bbox, frame] = yolov4_run_image(frame, True)\n",
        "    save_image_list.append(frame)\n",
        "\n",
        "    bbox_clip.append(bbox)\n",
        "\n",
        "    if (frame_count <= (engarde_length + engarde_offset)):\n",
        "\n",
        "      # Uses a frame in the middle of positioning to determine the time for frame analysis\n",
        "\n",
        "      # Performs Engarde Positioning for a frame\n",
        "      [Left_Position_Temp, Right_Position_Temp, Scoring_Box_Position_Temp, scoring_box_size_average, Tracking_Bounding_Boxes_Temp, \\\n",
        "        Left_Torso_Position_Temp, Right_Torso_Position_Temp, left_torso_size_average_Temp, right_torso_size_average_Temp, \\\n",
        "       All_Bell_Guard_Positions_Temp, Left_Foot_Position, Right_Foot_Position, Foot_Size] \\\n",
        "        = engarde_position(bbox, width, height, engarde_length + engarde_offset, frame_count-1, save_image_list)\n",
        "\n",
        "      Left_Position.append(Left_Position_Temp)\n",
        "      Right_Position.append(Right_Position_Temp)\n",
        "\n",
        "      Scoring_Box_Position.append(Scoring_Box_Position_Temp)\n",
        "      Left_Torso_Position.append(Left_Torso_Position_Temp)\n",
        "      Right_Torso_Position.append(Right_Torso_Position_Temp)\n",
        "      left_torso_size_average.append(left_torso_size_average_Temp)\n",
        "      right_torso_size_average.append(right_torso_size_average_Temp)\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame_count} the tracking Bounding Boxes Temp is:')\n",
        "        display(Tracking_Bounding_Boxes_Temp)\n",
        "      Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "      # Creates a List of each Bell Guard Position in each engarde positioning frame. Used for excluding false positives\n",
        "      All_Bell_Guard_Positions.append(All_Bell_Guard_Positions_Temp)\n",
        "\n",
        "    if (frame_count == engarde_length + engarde_offset):\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'Commencing the Engarde Length Processing.')\n",
        "      # tracked_items = [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average]\n",
        "      tracked_items = [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average, Left_Foot_Position, Right_Foot_Position]\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'The Scoring Box Position is:')\n",
        "        display(Scoring_Box_Position)\n",
        "      if max(Scoring_Box_Position) == []:\n",
        "        if verbose == True:\n",
        "          display(f'The Scoring Box Position was empty so a default was used.')\n",
        "        score_box_empty = True\n",
        "        tracked_items[2] = [[width/2, height/2], [width/2, height/2], [width/2, height/2]]\n",
        "\n",
        "      # Tests for empty Positions\n",
        "      if max(Right_Torso_Position) == []:\n",
        "        right_torso_empty = True\n",
        "\n",
        "      if max(Left_Torso_Position) == []:\n",
        "        left_torso_empty = True\n",
        "\n",
        "      if max(Left_Position) == [] and max(Left_Torso_Position) != []:\n",
        "        left_position_empty = True\n",
        "\n",
        "      if max(Right_Position) == [] and max(Right_Torso_Position) != []:\n",
        "        right_position_empty = True\n",
        "      \n",
        "      # Replaces [0,0] with [] for torso width and height\n",
        "      for k in range(len(tracked_items)):\n",
        "        for j in range(len(tracked_items[k])):\n",
        "          if tracked_items[k][j] == [0,0]:\n",
        "            tracked_items[k][j] = []\n",
        "\n",
        "      for k in range(len(tracked_items)):\n",
        "        if verbose == True:\n",
        "          display(f'k is {k}.')\n",
        "          display(tracked_items[k])\n",
        "        try:\n",
        "          tracked_items[k] = [item for item in tracked_items[k] if item != []]\n",
        "          tracked_items[k] = np.hstack(tracked_items[k])\n",
        "          if verbose == True:\n",
        "            display(f'The length of len(tracked_items[k]) is {int(len(tracked_items[k])/2)}.')\n",
        "          tracked_items[k] = tracked_items[k].reshape(int(len(tracked_items[k])/2), 2)\n",
        "          tracked_items[k] = np.median(tracked_items[k], axis = 0)\n",
        "          tracked_items[k] = [int(tracked_items[k][0]), int(tracked_items[k][1])]\n",
        "          if verbose == True:\n",
        "            display(f'The tracked item position {tracked_items[k]}.')\n",
        "        except:\n",
        "          if verbose == True:\n",
        "            display(f'Failure to detect the tracked item {k} during the engarde positioning.')\n",
        "            display(tracked_items[k])\n",
        "          tracked_items[k] = [0,0]\n",
        "\n",
        "      if right_torso_empty == True:\n",
        "        # if verbose == True:\n",
        "        display(f'The Right Torso Position was empty so a default based on the Right BellGuard was used.')\n",
        "        display(f'tracked_items[1] is {tracked_items[1]}, left_torso_size_average is {left_torso_size_average}.')\n",
        "        torso_position_default = [tracked_items[1][0] + int(left_torso_size_average[0][0]*3/4), tracked_items[1][1] - int(left_torso_size_average[0][1]/4)]\n",
        "        tracked_items[4] = torso_position_default\n",
        "        # Sets the Right Tracking Bounding Box\n",
        "        # Sets the Right Torso Average Size to the Left Average Size\n",
        "        right_torso_size_average = left_torso_size_average\n",
        "        if verbose == True:\n",
        "          display(f'As a comparison, the Score Box Position is:')\n",
        "          display(Scoring_Box_Position)\n",
        "          display(f'The Right Torso Position is:')\n",
        "          display(Right_Torso_Position)\n",
        "          display(f'As a comparison, the tracked_items[2] is:')\n",
        "          display(tracked_items[2])\n",
        "          display(f'The tracked_items[4] is:')\n",
        "          display(tracked_items[4])\n",
        "        # Sets Right Torso Empty to False after a Default it used\n",
        "        right_torso_empty = False\n",
        "\n",
        "      if left_torso_empty == True:\n",
        "        if verbose == True:\n",
        "          display(f'The Left Torso Position was empty so a default based on the Left BellGuard was used.')\n",
        "          display(f'tracked_items[0] is {tracked_items[0]}, right_torso_size_average is {right_torso_size_average}.')\n",
        "        torso_position_default = [tracked_items[0][0] - int(right_torso_size_average[1][0]*3/4), tracked_items[0][1] - int(right_torso_size_average[0][1]/4)]\n",
        "        tracked_items[3] = torso_position_default\n",
        "        # Sets the Left Torso Average Size to the Left Average Size\n",
        "        left_torso_size_average = right_torso_size_average\n",
        "        if verbose == True:\n",
        "          display(f'As a comparison, the Score Box Position is:')\n",
        "          display(Scoring_Box_Position)\n",
        "          display(f'The Left Torso Position is:')\n",
        "          display(Left_Torso_Position)\n",
        "          display(f'As a comparison, the tracked_items[1] is:')\n",
        "          display(tracked_items[1])\n",
        "          display(f'The tracked_items[3] is:')\n",
        "          display(tracked_items[3])\n",
        "        left_torso_empty = False\n",
        "\n",
        "      if left_position_empty == True:\n",
        "        if verbose == True:\n",
        "          display(f'The Left  Position was empty so a default based on the Left Torso was used.')\n",
        "        left_torso_size_average_temp = average_list_without_null(left_torso_size_average)\n",
        "        left_position_default = [int(tracked_items[3][0] + left_torso_size_average_temp[0]), int(tracked_items[3][1] + left_torso_size_average_temp[0]/4)]\n",
        "        # left_position_default = [tracked_items[3][0] + int(left_torso_size_average[0][0]), tracked_items[3][1] + int(left_torso_size_average[0][0]/4)]\n",
        "        tracked_items[0] = left_position_default\n",
        "\n",
        "      if right_position_empty == True:\n",
        "        if verbose == True:\n",
        "          display(f'The Right  Position was empty so a default based on the Right Torso was used.')\n",
        "          display(f'The tracked_items[4] is {tracked_items[4]}.')\n",
        "          display(f'The right_torso_size_average is {right_torso_size_average}.')\n",
        "        right_torso_size_average_temp = average_list_without_null(right_torso_size_average)\n",
        "        right_position_default = [int(tracked_items[4][0] - right_torso_size_average_temp[0]), int(tracked_items[4][1] + right_torso_size_average_temp[0]/4)]\n",
        "        tracked_items[1] = right_position_default\n",
        "\n",
        "      [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position, Left_Foot_Position, Right_Foot_Position] = [[],[],[],[],[],[],[]]\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'The length of the tracked items is {len(tracked_items)}')\n",
        "\n",
        "      # Builds the Positions of the Tracked Items, iterator is solely to append multiple times\n",
        "      for i in range(engarde_length + engarde_offset):\n",
        "        Left_Position.append(tracked_items[0])\n",
        "        Right_Position.append(tracked_items[1])\n",
        "        Scoring_Box_Position.append(tracked_items[2])\n",
        "        Left_Torso_Position.append(tracked_items[3])\n",
        "        Right_Torso_Position.append(tracked_items[4])\n",
        "        Left_Foot_Position.append(tracked_items[7])\n",
        "        Right_Foot_Position.append(tracked_items[8])\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'The Left Foot Position is:')\n",
        "        display(Left_Foot_Position)\n",
        "\n",
        "      padding = width/24\n",
        "      exclusion_box = [padding,padding,padding,padding]\n",
        "      Left_Exculsion_Box = create_boundary_box(Left_Position[0],exclusion_box, False)\n",
        "      Right_Exculsion_Box = create_boundary_box(Right_Position[0],exclusion_box, False)\n",
        "\n",
        "      for m in range(len(All_Bell_Guard_Positions)):\n",
        "        for n in range(len(All_Bell_Guard_Positions[m])):\n",
        "          within_Left_Region = boundary_box_test(All_Bell_Guard_Positions[m][n], Left_Exculsion_Box)\n",
        "          within_Right_Region = boundary_box_test(All_Bell_Guard_Positions[m][n], Right_Exculsion_Box)\n",
        "          if within_Left_Region == False and within_Right_Region == False:\n",
        "            Exclusion_Areas.append(All_Bell_Guard_Positions[m][n])\n",
        "\n",
        "      # Removes duplicate Exclusion Areas that are within a given distance of each other\n",
        "      Exclusion_Areas = exclusion_area_simplification_recursion(Exclusion_Areas, width/80)\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'The Exclusion areas are:')\n",
        "        display(Exclusion_Areas)\n",
        "\n",
        "      left_torso_size_average = tracked_items[5]\n",
        "      if right_torso_empty == False:\n",
        "        right_torso_size_average = tracked_items[6]\n",
        "      elif right_torso_empty == True:\n",
        "        right_torso_size_average = tracked_items[5]\n",
        "      torso_size = [left_torso_size_average, right_torso_size_average]\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'The left_torso_size_average is {left_torso_size_average} and the right_torso_size_average is {right_torso_size_average}.')\n",
        "        display(f'The Right_Torso_Position[0] Prior to testing is: {Right_Torso_Position[0]}. ')\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'Prior to testing for success, the Left Position[0] is:')\n",
        "        display(Left_Position[0])\n",
        "        display(f'Prior to testing for success, the Right Position[0] is:')\n",
        "        display(Right_Position[0])\n",
        "        display(f'Prior to testing for success, the Left_Torso_Position[0] is:')\n",
        "        display(Left_Torso_Position[0])\n",
        "        display(f'Prior to testing for success, the Right_Torso_Position[0] is:')\n",
        "        display(Right_Torso_Position[0])\n",
        "        display(f'Perform_engarde_positioning is {perform_engarde_positioning}.')\n",
        "\n",
        "      if (Left_Position[0] == [0,0] or Right_Position[0] == [0,0] or Left_Torso_Position[0] == [0,0] or Right_Torso_Position[0] == [0,0]) and (perform_engarde_positioning == True):\n",
        "      # if max(Left_Position) == [] or max(Right_Position) == [] or max(Left_Torso_Position) == [] or max(Right_Torso_Position) == []:\n",
        "        # perform_engarde_positioning = True\n",
        "        engarde_offset = engarde_offset + offset_increment\n",
        "        display(f'The Engarde Positioning Failed, incrementing the Engarde Length by {offset_increment} frames.')\n",
        "        if verbose == True:\n",
        "          display(f'The Left Torso Position at the end of Engarde Positioning is: {Left_Torso_Position}.')\n",
        "          display(f'The Right Torso Position at the end of Engarde Positioning is: {Right_Torso_Position}.')\n",
        "      else:\n",
        "        perform_engarde_positioning = False\n",
        "        display(f'The Engarde Positioning was successful, continuing with clip analysis.')\n",
        "        display(f'The final engarde length was {engarde_length + engarde_offset}.')\n",
        "        if verbose == True:\n",
        "          display(f'The Left Torso Position at the end of Engarde Positioning is: {Left_Torso_Position}.')\n",
        "          display(f'The Right Torso Position at the end of Engarde Positioning is: {Right_Torso_Position}.')\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The frame count is {frame_count} and perform engarde positioning is {perform_engarde_positioning}.')\n",
        "    # Continues on the frames beyond the engarde length if positioning was successful\n",
        "    if verbose == True:\n",
        "      display(f'The frame count is {frame_count} and the engarde length is {engarde_length} with an offset of {engarde_offset}. Positioning is {perform_engarde_positioning}.')\n",
        "    \n",
        "    \n",
        "    # display(f'The frame_count is {frame_count}, the engarde_length is {engarde_length} and engarde_offset is {engarde_offset}.')\n",
        "    if (frame_count > (engarde_length + engarde_offset)) and (perform_engarde_positioning == False):\n",
        "\n",
        "      positions = [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position, Left_Foot_Position, Right_Foot_Position]\n",
        "\n",
        "      # Sets the certainty following the engarde positioning\n",
        "      if frame_count == engarde_length + engarde_offset + 1:\n",
        "        t3 = time.time()\n",
        "        certainty = [0,0,0,0,0,0,0]\n",
        "        if verbose == True:\n",
        "          display(f'The positions following the engarde positioning are:')\n",
        "          display(positions)\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'Certainty just prior to Bell Guard Positioning is {certainty}.')\n",
        "\n",
        "      previous_certainty = certainty\n",
        "\n",
        "      if right_torso_size_average[0] == 0 and left_torso_size_average[0] == 0:\n",
        "        if verbose == True:\n",
        "          display(f'Error, both Torso sizes are zero.')   \n",
        "      elif right_torso_size_average[0] == 0:\n",
        "        if verbose == True:\n",
        "          display(f'Right Torso Size was zero, using the Left as a Defualt.')\n",
        "        right_torso_size_average = left_torso_size_average\n",
        "      elif left_torso_size_average[0] == 0:\n",
        "        if verbose == True:\n",
        "          display(f'Left Torso Size was zero, using the Right as a Defualt.')\n",
        "        left_torso_size_average = right_torso_size_average\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'The engarde_length is {engarde_length}.')\n",
        "        display(f'The engarde_offset is {engarde_offset}.')\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'Right Torso Size average is {right_torso_size_average} at frame {frame_count} just prior to Bell Guard Positioning.')\n",
        "        display(f'Right Torso Position is {positions[4]} at frame {frame_count}.')\n",
        "\n",
        "      # Finds the Tracked Items and Returns their positions\n",
        "      [Left_Position_Temp, Right_Position_Temp, Scoring_Box_Position_Temp, Tracking_Bounding_Boxes_Temp, \\\n",
        "      Left_Torso_Position_Temp, Right_Torso_Position_Temp, Left_Foot_Position_Temp, Right_Foot_Position_Temp, certainty] \\\n",
        "      = Bell_Guard_Position_Finding(bbox, width, height, positions, frame_count, \\\n",
        "      left_torso_size_average, right_torso_size_average, (engarde_length + engarde_offset), \\\n",
        "      certainty, camera_steady, camera_motion_threshold, Exclusion_Areas, orig_img_worpt_starting_list)\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'Certainty just after to Bell Guard Positioning is {certainty}.')\n",
        "        display(f'The Left Position at frame {frame_count - 1} is {Left_Position_Temp}.')\n",
        "\n",
        "      if close_bellguards == True:\n",
        "        frames_after_light_iterator = frames_after_light_iterator + 1\n",
        "\n",
        "      # If the Positions are Close enough then the lights are assumed on and tracking is stopped\n",
        "      if ((Right_Position_Temp[0] - Left_Position_Temp[0]) < width*position_difference_ratio) or (close_bellguards == True):\n",
        "        close_bellguards = True\n",
        "        # display(f'close_bellguards set to True')\n",
        "        if frames_after_light_iterator == 0:\n",
        "          display(f'The last tracked frame is {frame_count}.')\n",
        "          # Adjusts the frames after light to include the entire clip\n",
        "          if run_entire_video == True:\n",
        "            frames_after_light_max = total_frames - frame_count\n",
        "          # Ensures that frames processed does not exceed total number of frames\n",
        "          # if (frames_after_light_iterator + frame_count) >= total_frames:\n",
        "          if (frames_after_light_max + frame_count + starting_frame) >= total_frames:\n",
        "          # if (frames_after_light_max + frame_count + engarde_offset) >= total_frames:\n",
        "            # frames_after_light_max = (frames_after_light_iterator + frame_count - 1)\n",
        "            # frames_after_light_max = total_frames - (frames_after_light_max + frame_count)\n",
        "            frames_after_light_max = total_frames - (frames_after_light_max + frame_count + starting_frame)\n",
        "            # frames_after_light_max = total_frames - (frames_after_light_max + frame_count + engarde_offset)\n",
        "            display(f'The frames after light max has been adjusted to {frames_after_light_max}')\n",
        "\n",
        "      # Appends the Returned Positions\n",
        "      Left_Position.append(Left_Position_Temp)\n",
        "      Right_Position.append(Right_Position_Temp)\n",
        "      Scoring_Box_Position.append(Scoring_Box_Position_Temp)\n",
        "      Left_Torso_Position.append(Left_Torso_Position_Temp)\n",
        "      Right_Torso_Position.append(Right_Torso_Position_Temp)\n",
        "      Left_Foot_Position.append(Left_Foot_Position_Temp)\n",
        "      Right_Foot_Position.append(Right_Foot_Position_Temp)\n",
        "      Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "\n",
        "      # Tests for a change in certainty to zero from non-zero. If a position has become certain during\n",
        "      # this frame then it back calculates previous uncertain position up to a certain position.\n",
        "      if (certainty[0] == 0 and previous_certainty[0] != 0):\n",
        "        Left_Position = position_linear_approximation(Left_Position, previous_certainty[0])\n",
        "        if verbose == True:\n",
        "          display(f'Using a Linear Approximation for frame {frame_count} for the Left Bellguard Position.')\n",
        "      elif (certainty[1] == 0 and previous_certainty[1] != 0):\n",
        "        if verbose == True:\n",
        "          display(f'The Right Position is: {Right_Position}.')\n",
        "          display(f'The Previous Certainty is: {previous_certainty[1]}')\n",
        "          display(f'Using a Linear Approximation for frame {frame_count} for the Right Bellguard Position.')\n",
        "        Right_Position = position_linear_approximation(Right_Position, previous_certainty[1])\n",
        "      elif (certainty[2] == 0 and previous_certainty[2] != 0):\n",
        "        Scoring_Box_Position = position_linear_approximation(Scoring_Box_Position, previous_certainty[2])\n",
        "      elif (certainty[3] == 0 and previous_certainty[3] != 0):\n",
        "        Left_Torso_Position = position_linear_approximation(Left_Torso_Position, previous_certainty[3])\n",
        "      elif (certainty[4] == 0 and previous_certainty[4] != 0):\n",
        "        Right_Torso_Position = position_linear_approximation(Right_Torso_Position, previous_certainty[4])\n",
        "      elif (certainty[5] == 0 and previous_certainty[5] != 0):\n",
        "        Left_Foot_Position = position_linear_approximation(Left_Foot_Position, previous_certainty[5])\n",
        "      elif (certainty[6] == 0 and previous_certainty[6] != 0):\n",
        "        Right_Foot_Position = position_linear_approximation(Right_Foot_Position, previous_certainty[6])\n",
        "      else:\n",
        "        pass    \n",
        "\n",
        "  # End of Individual Frame Processing\n",
        "\n",
        "  if not too_few_frames:\n",
        "\n",
        "    if verbose == True:\n",
        "      # Reduces the Frame Count to account for skipped frames\n",
        "      display(f'The original frame count was: {frame_count - 1} and the number of frames skipped is: {number_of_frames_skipped}.')\n",
        "    frame_count = len(bbox)\n",
        "    if verbose == True:\n",
        "      display(f'The length of the frame_count is {frame_count - 1} while the number of bboxes is {len(bbox)}.')\n",
        "\n",
        "    file_to_remove = r'/Mask_RCNN/videos/' + file_name\n",
        "    try:\n",
        "      shutil.rmtree(file_to_remove)\n",
        "    except:\n",
        "      if verbose == True:\n",
        "        display(f'ERROR removing the video file to analyze.')\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The Left Position just prior to drawing the Bell_Guards is:')\n",
        "      display(Left_Position)\n",
        "      display(f'The Right Position just prior to drawing the Bell_Guards is:')\n",
        "      display(Right_Position)\n",
        "      display(f'The Score Box Position just prior to drawing the Bell_Guards is:')\n",
        "      display(Scoring_Box_Position)\n",
        "\n",
        "    # Allows for a simple clip vector to draw in the lights if an overlay is used instead of a representative clip\n",
        "    simple_clip_vector = simple_clip_vector_generator(Left_Position, Right_Position, width)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The Right Torso ')\n",
        "      display(f'The Right Torso Position Just Prior to Drawing at frame {frame_count} is: {Right_Torso_Position}')\n",
        "\n",
        "    #Draws the Boxes on the image frame and determines scoring lights turned on\n",
        "    [left_light_comparison, right_light_comparison, save_image_list] = \\\n",
        "    draw_Bell_Guard_Position(Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, \\\n",
        "    Left_Torso_Position, Right_Torso_Position, Left_Foot_Position, Right_Foot_Position, frame_count, Tracking_Bounding_Boxes, \\\n",
        "    video_filename, width, height, engarde_length + engarde_offset, score_box_empty, camera_steady, camera_motion_threshold, \\\n",
        "    Exclusion_Areas, simple_clip_vector, save_image_list)\n",
        "\n",
        "\n",
        "    if camera_motion_compensate == True and score_box_empty == False:\n",
        "      #Adjusts the Bellguard Position Based on the Camera motion as determined by the Score_Box Position\n",
        "      Left_Position = camera_motion_adjustment(Left_Position, Scoring_Box_Position)\n",
        "      Right_Position = camera_motion_adjustment(Right_Position, Scoring_Box_Position)\n",
        "\n",
        "    if camera_motion_compensate == True and score_box_empty == False:\n",
        "      #Adjusts Left and Right Position for convenient visualization\n",
        "      [Left_Position, Right_Position] = position_down_scale(Left_Position, Right_Position, width, height)\n",
        "\n",
        "    clip_vector = clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous, width)\n",
        "\n",
        "    if smooth_video_clip == True:\n",
        "      #Smoothes the Clip using Savitzky–Golay filter\n",
        "      clip_vector = smooth_clip_vector(clip_vector, engarde_length + engarde_offset)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The final clip vector is:')\n",
        "      display(clip_vector)\n",
        "\n",
        "    # clip_vector_np_save(touch_folder, file_name[:-4], clip_vector, file_title)\n",
        "    clip_vector_np_save(touch_folder, file_name[:-4], clip_vector, file)\n",
        "\n",
        "  else:\n",
        "    # Sets output variables to none since the clip cannot run due to too few frames\n",
        "    bbox = clip_vector = 'none'\n",
        "\n",
        "  return (bbox_clip, frame_count, width, height, clip_vector_previous, clip_vector, fps, save_image_list, too_few_frames)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "mkHx2cfqpmyM"
      },
      "source": [
        "#@title Prepare_Video_Frames\n",
        "def prepare_video_frames(image_save_directory, video_directory, video_name):\n",
        "  VIDEO_DIR = video_directory\n",
        "  VIDEO_SAVE_DIR = image_save_directory\n",
        "  images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
        "  # Sort the images by integer index\n",
        "  images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "  name = str(video_name) + '.mp4'\n",
        "  outvid = os.path.join(VIDEO_DIR, name)\n",
        "\n",
        "  return (outvid, images)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "aFBlM2EDSieV"
      },
      "source": [
        "#@title Downsample_FPS\n",
        "def downsample_fps(a,b):\n",
        "  # Adjusts the elements of a larger set a to fit into the length of set b\n",
        "\n",
        "  c = []\n",
        "  remainder = 0\n",
        "  for i in range(len(b)):\n",
        "    c_temp = []\n",
        "    if verbose == True:\n",
        "      display(f'The lower range is {math.ceil(len(a)/len(b)*(i+1)-1-remainder)} and the upper range is {math.floor(len(a)/len(b)*(i+1))}.')\n",
        "    for j in range(math.ceil(len(a)/len(b)*(i)-remainder),math.floor(len(a)/len(b)*(i+1))):\n",
        "      c_temp.append(a[j])\n",
        "      if verbose == True:\n",
        "        display(f'i,j = {i},{j} and c_temp = {c_temp}')\n",
        "    remainder = (len(a)/len(b))*(i+1) - int(len(a)/len(b)*(i+1))\n",
        "    c.append(round(sum(c_temp)/len(c_temp)))\n",
        "    if verbose == True:\n",
        "      display(f'The remainder at i = {i} and j = {j} is {remainder} and c is {c}.')\n",
        "\n",
        "  return (c)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "2FDqmsziTCBC"
      },
      "source": [
        "#@title Load_Clip\n",
        "def load_clip(folder, file_name, capture_width):\n",
        "  if folder == 'Left' or folder == 'left' or folder == 'Left_Touch':\n",
        "    folder = 0\n",
        "  if folder == 'Right' or folder == 'right'or folder == 'Right_Touch':\n",
        "    folder = 1\n",
        "  if folder == 'Simul' or folder == 'simul'or folder == 'Simul':\n",
        "    folder = 2\n",
        "\n",
        "  engarde_position_buffer = 15\n",
        "  max_length = 111\n",
        "  clip_vector_length = max_length - engarde_position_buffer\n",
        "\n",
        "\n",
        "  touch_folder = ['Left_Touch', 'Right_Touch', 'Simul']\n",
        "\n",
        "  i = folder\n",
        "\n",
        "  # file = 'clip_vector_acceleration_np' + str(clip_number) + '.csv'\n",
        "  file = file_name\n",
        "  path = folder\n",
        "  # path = r'/content/drive/My Drive/projects/fencing/Fencing Clips/' + touch_folder[i] + '/' + touch_folder[i] + '_Vector_Clips_Acceleration/'\n",
        "\n",
        "  vector_data = pd.read_csv(os.path.join(path, file), header=None)\n",
        "  clip_vector = vector_data.to_numpy(dtype = np.float32)\n",
        "\n",
        "  display(os.path.join(path, file))\n",
        "\n",
        "  # Pads the clip_vector to max_length\n",
        "  # If the clip is greater than Max Length, it is truncated\n",
        "  if len(clip_vector) > max_length:\n",
        "    clip_vector = clip_vector[len(clip_vector) - max_length:]\n",
        "  padding = np.array([0,0,0,0])\n",
        "  for k in range(max_length - (len(clip_vector))):\n",
        "    clip_vector = np.vstack((clip_vector, padding))\n",
        "\n",
        "  #Normalizes the Values\n",
        "  max_value = int(capture_width/42)\n",
        "  for i in range(len(clip_vector)):\n",
        "    for j in range(2):\n",
        "      if clip_vector[i][j] < max_value:\n",
        "        clip_vector[i][j] = clip_vector[i][j] * (1/max_value)\n",
        "      else:\n",
        "        #Preserves the sign of the value\n",
        "        clip_vector[i][j] = clip_vector[i][j]/(abs(clip_vector[i][j]))\n",
        "\n",
        "  # Removes the First 15 frames to minimize engarde positioning\n",
        "  clip_vector = clip_vector[15:]\n",
        "\n",
        "  # Sets Clip_Vector to Zero if Light is on\n",
        "  for j in range(len(clip_vector)):\n",
        "    if clip_vector[j][2] == 1:\n",
        "      clip_vector[j][0] = 0\n",
        "    if clip_vector[j][3] == 1:\n",
        "      clip_vector[j][1] = 0 \n",
        "\n",
        "  clip_vector = clip_vector.reshape(1,clip_vector_length,4)\n",
        "  return (clip_vector)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "bv6yO3gTSHox"
      },
      "source": [
        "#@title Create_Folder_Hierarchy\n",
        "def create_folder_hierarchy(file_name, output_bbox_clip, output_foot_data):\n",
        "\n",
        "  # Changes the directory to the top level of My Drive\n",
        "  if not exists('/content/drive/My Drive/Sync/Acceleration'):\n",
        "    os.makedirs('/content/drive/My Drive/Sync/Acceleration')\n",
        "    display(f'Created the Sync Acceleration directory.')\n",
        "  else:\n",
        "    display(f'The Sync Acceleration directory already exists.')\n",
        "  # Creates the Tracked Clips Folder\n",
        "  if not exists('/content/drive/My Drive/Sync/Tracked Clips'):\n",
        "    os.makedirs('/content/drive/My Drive/Sync/Tracked Clips')\n",
        "    display(f'Created the Sync Tracked Clips directory.')\n",
        "  else:\n",
        "    display(f'The Sync Tracked Clips directory already exists.')\n",
        "\n",
        "  # Creates the Boundary Box Outputs if required\n",
        "  if output_bbox_clip:\n",
        "    if output_foot_data:\n",
        "      if not exists('/content/drive/My Drive/Sync/Foot Data'):\n",
        "        os.makedirs('/content/drive/My Drive/Sync/Foot Data')\n",
        "        display(f'Created the Foot Data directory.')\n",
        "      else:\n",
        "        display(f'The Foot Data directory already exists.')\n",
        "\n",
        "  return ()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "ZQ8y96lwZccc"
      },
      "source": [
        "#@title Evaluate_Model\n",
        "def evaluate_model(dataset, model, cfg):\n",
        "  # calculate the mAP for a model on a given dataset\n",
        "  APs = list()\n",
        "  for image_id in dataset.image_ids:\n",
        "\t\t# load image, bounding boxes and masks for the image id\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
        "\t\t# convert pixel values (e.g. center)\n",
        "    scaled_image = mold_image(image, cfg)\n",
        "\t\t# convert image into one sample\n",
        "    sample = expand_dims(scaled_image, 0)\n",
        "\t\t# make prediction\n",
        "    yhat = model_detection.detect(sample, verbose=0)\n",
        "\t\t# extract results for first sample\n",
        "    r = yhat[0]\n",
        "\t\t# calculate statistics, including AP\n",
        "    AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "\t\t# store\n",
        "    APs.append(AP)\n",
        "\t# calculate the mean AP across all images\n",
        "  mAP = mean(APs)\n",
        "  return mAP"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzHw3Z3Z2NLb"
      },
      "source": [
        "def youtube_clip(link):\n",
        "\n",
        "  os.chdir('/content/')\n",
        "\n",
        "  # Removes Potential Previous Files\n",
        "  if exists('video_title.mp4'):\n",
        "    os.remove('video_title.mp4')\n",
        "  if exists('video_title.mkv'):\n",
        "    os.remove('video_title.mkv')\n",
        "\n",
        "  # Gets the Link for the Youtube Clip\n",
        "  youtube_clip = link.split('=')[0][0:-2]\n",
        "  # Uses the YouTube Query to get the time in seconds of the position in the clip\n",
        "  youtube_clip_position = int(link.split('=')[1])\n",
        "  # Clip End Position\n",
        "  youtube_end_position = int(youtube_clip_position + 10)\n",
        "  \n",
        "  ydl_opts = {}\n",
        "  #Gets info for the YouTube clip\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    info_dict = ydl.extract_info(link, download=False)\n",
        "    video_title = info_dict.get('title', None)\n",
        "    file_size = info_dict.get('formats',None)[0]['filesize']\n",
        "    duration = info_dict.get('duration',None)\n",
        "    # container = info_dict.get('formats',None)[0]['container']\n",
        "\n",
        "  container = 0\n",
        "\n",
        "  #Determines where to save the YouTube clip\n",
        "  # path = f'./{video_title}.mp4'\n",
        "  path = f'./video_title.mp4'\n",
        "  ydl_opts.update({'outtmpl':path})\n",
        "\n",
        "  #Downloads and saves the YouTube clip\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([link])\n",
        "\n",
        "  # The video download may be .mp4 or mkv\n",
        "  if exists('video_title.mp4'):\n",
        "    video_title = 'video_title.mp4'\n",
        "  if exists('video_title.mkv'):\n",
        "    video_title = 'video_title.mkv'\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The clip duration is {duration}.')\n",
        "\n",
        "  if container == 'm4a_dash':\n",
        "    display(f'The youtube link is a streamed video, downloading entire stream.')\n",
        "\n",
        "  file_name = '999.mp4'\n",
        "  touch_folder = 'Left_Touch'\n",
        "\n",
        "  # Downloads the entire file if the video clip is short or a DASH file, i.e. streamed video clip\n",
        "  if duration < duration_download_limit or container == 'm4a_dash':\n",
        "    display(f'Duration is less than {duration_download_limit}. Downloading entire clip.')\n",
        "    !ffmpeg -y -ss $youtube_clip_position -i $video_title -c:v libx264 -c:a aac -frames:v 100 999.mp4\n",
        "  else:\n",
        "    display(f'Duration is greater than {duration_download_limit}. Downloading a portion of the clip.')\n",
        "    !ffmpeg $(youtube-dl -g $link | sed \"s/.*/-ss $youtube_clip_position -i &/\") -y -to $youtube_end_position -c copy out.mp4\n",
        "    [starting_frame, fps, total_frames] = determine_starting_frame('out.mp4', touch_folder, True)\n",
        "    os.chdir('/content/')\n",
        "    frames_per_second = 25\n",
        "    start_time = starting_frame/frames_per_second\n",
        "    # Clips the Video per the starting frame\n",
        "    !ffmpeg -y -ss $start_time -i out.mp4 -c:v libx264 -c:a aac -frames:v 125 999.mp4\n",
        "    # Copies the video to the appropriate folder\n",
        "    destination = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "    file_name_with_path = os.path.join(destination, file_name)\n",
        "    #Copies the clip to the Left Touch Folder. Tests for file already existing\n",
        "    if exists(file_name_with_path):\n",
        "      os.remove(file_name_with_path)\n",
        "    shutil.copy(file_name, destination)\n",
        "\n",
        "  destination = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "  file_name_with_path = os.path.join(destination, file_name)\n",
        "  #Copies the clip to the Left Touch Folder. Tests for file already existing\n",
        "  if exists(file_name_with_path):\n",
        "    os.remove(file_name_with_path)\n",
        "  shutil.copy(file_name, destination)\n",
        "\n",
        "  return"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvInBbF6LVKv"
      },
      "source": [
        "def determine_starting_frame(original_image_worpt_list, width, height):\n",
        "  # Determines the first usable frame in a clip\n",
        "\n",
        "  # Sets up the clip directories\n",
        "  [ROOT_DIR, VIDEO_DIR, VIDEO_SAVE_DIR, VIDEO_ORIG_DIR] = setup_clip_directories()\n",
        "\n",
        "  os.chdir('/content/drive/My Drive/')  \n",
        "\n",
        "  total_frames = len(original_image_worpt_list)\n",
        "  if total_frames == 0:\n",
        "    display(f'ERROR: The Video Clip selected has no frames.')\n",
        "  display(f'The total number of frames in the video are: {total_frames}')\n",
        "\n",
        "  # frame_check keeps a running account of detected object in each frame\n",
        "  frame_check = []\n",
        "  # Determines when the first complete frame of a clip is found\n",
        "  test_frame_check = False\n",
        "  # Initialize the frame iterator, counts the number of frames analyzed\n",
        "  k = 0\n",
        "  # Number of Frames to Sample\n",
        "  group_size = 10\n",
        "  # Number of Frames that must be true\n",
        "  min_positives = 2\n",
        "  # min_positives = 1\n",
        "\n",
        "  while test_frame_check == False and k < len(original_image_worpt_list):\n",
        "\n",
        "    # display(f'k is {k} and the length of orig_worpt is {len(original_image_worpt_list)}.')\n",
        "\n",
        "    frame = original_image_worpt_list[k]\n",
        "    # Runs the Detection Model\n",
        "    [bbox, none] = yolov4_run_image(frame, False)\n",
        "\n",
        "    if bbox != []:\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'a is:')\n",
        "        display(a)\n",
        "\n",
        "      # Tests if there is a viable Left and Right Torso in the Frame\n",
        "      [Left_Torso_Detection, Left_Torso_Box_Max] = torso_detection_test(bbox, 'Left', width, height)\n",
        "      [Right_Torso_Detection, Right_Torso_Box_Max]  = torso_detection_test(bbox, 'Right', width, height)\n",
        "\n",
        "      # Tests if there is a viable Left and Right Bell Guard in the Frame\n",
        "      Left_Bell_Guard_Detection = bell_guard_detection_test(bbox, Left_Torso_Box_Max, 'Left', width, height)\n",
        "      Right_Bell_Guard_Detection  = bell_guard_detection_test(bbox, Right_Torso_Box_Max, 'Right', width, height)\n",
        "\n",
        "      # display(f'At frame {m} the L/R Torso is {Left_Torso_Detection}/{Right_Torso_Detection} and L/R Bellguard is {Left_Bell_Guard_Detection}/{Right_Bell_Guard_Detection}. ')\n",
        "\n",
        "      # Checks if all the detections are True and appends 1 or 0 to the frame_check.\n",
        "      if Left_Bell_Guard_Detection == Right_Bell_Guard_Detection == Left_Torso_Detection == Right_Torso_Detection == True:\n",
        "        frame_check.append(1)\n",
        "      else:\n",
        "        frame_check.append(0)\n",
        "\n",
        "    # Increments the frame iterator\n",
        "    k = k + 1\n",
        "\n",
        "    # Tests the frame_check to determine if there are sufficient positives\n",
        "    if len(frame_check) > group_size:\n",
        "      sum_of_values = 0\n",
        "      for i in range(group_size):\n",
        "        sum_of_values = sum_of_values + frame_check[len(frame_check) - i - 1]\n",
        "  \n",
        "      if verbose == True:\n",
        "        display(f'The number of positives is {sum_of_values}.')\n",
        "      if sum_of_values >= min_positives:\n",
        "        if verbose == True:\n",
        "          display(f'The threshold has been met. Success.')\n",
        "        test_frame_check = True\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The minimum has not been achieved. Failure.')\n",
        "\n",
        "  starting_frame = k - group_size\n",
        "\n",
        "  return (starting_frame, total_frames)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoZBG0b2rWjy"
      },
      "source": [
        "def bell_guard_detection_test(bbox, Torso_Box_Max, side, width, height):\n",
        "\n",
        "  # Initializes the detection variable\n",
        "  detection = False\n",
        "\n",
        "  y_min = int(height*1/6)\n",
        "  y_max = min(int(height*2/3), Torso_Box_Max)\n",
        "  if side == 'Left':\n",
        "    x_min = 0\n",
        "    x_max = width*1/2\n",
        "  elif side == 'Right':\n",
        "    x_min = width*1/2\n",
        "    x_max = width\n",
        "\n",
        "  for i in range(len(bbox)):\n",
        "    # Defines the Center of the BBox\n",
        "    x_avg = int((bbox[i][0][1]+bbox[i][0][3])/2)\n",
        "    y_avg = int((bbox[i][0][0]+bbox[i][0][2])/2)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'bbox[i] is {bbox[i]}.')\n",
        "      display(f'bbox[i][2] is {bbox[i][2]}.')\n",
        "\n",
        "    # Checks if the BBox is within the Bounds for plausible Bell Guard\n",
        "    if  bbox[i][2] == 1 and x_min < x_avg and x_avg < x_max and y_min < y_avg and y_avg < y_max:\n",
        "      detection = True\n",
        "      break\n",
        "\n",
        "  return (detection)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD0Ajte7MCvL"
      },
      "source": [
        "def torso_detection_test(bbox, side, width, height):\n",
        "\n",
        "  # Initializes the detection variable\n",
        "  detection = False\n",
        "\n",
        "  y_min = int(height*1/6)\n",
        "  y_max = int(height*2/3)\n",
        "  if side == 'Left':\n",
        "    x_min = 0\n",
        "    x_max = width*1/2\n",
        "  elif side == 'Right':\n",
        "    x_min = width*1/2\n",
        "    x_max = width\n",
        "\n",
        "  for i in range(len(bbox)):\n",
        "    # Defines the Center of the BBox\n",
        "    x_avg = int((bbox[i][0][1]+bbox[i][0][3])/2)\n",
        "    y_avg = int((bbox[i][0][0]+bbox[i][0][2])/2)\n",
        "    # Used in Bell Guard to verify bellguard is above the bottom of the Torso Detection\n",
        "    y_box_max = int(bbox[i][0][2])\n",
        "    # Checks if the BBox is within the Bounds for plausible Bell Guard\n",
        "    if  bbox[i][2] == 3 and x_min < x_avg and x_avg < x_max and y_min < y_avg and y_avg < y_max:\n",
        "      detection = True\n",
        "      break\n",
        "\n",
        "  return (detection, y_box_max)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUJhIfFu5Osa"
      },
      "source": [
        "def Most_Recent_Sync_File():\n",
        "\n",
        "  # Creates a List of Modified File Names and Dates modifies and selects the most recent.\n",
        "  # Initializes the Lists\n",
        "  date_modified_list = []\n",
        "  file_size_list = []\n",
        "  file_names = []\n",
        "  path = '/content/drive/My Drive/Sync/'\n",
        "\n",
        "  # Cycles through the files to generate the lists\n",
        "  for file in os.listdir(path):\n",
        "    # Ensures that the File is not a Folder\n",
        "    if os.path.isfile(os.path.join(path, file)):\n",
        "      name = os.path.join(path, file)\n",
        "      file_names.append(file)\n",
        "      # date_modified_list.append(os.path.getmtime(name))\n",
        "      date_modified_list.append(os.path.getctime(name))\n",
        "      file_size_list.append(int(os.path.getsize(name)/1024))\n",
        "      # display(file)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(max(date_modified_list))\n",
        "  # Finds the index of the most recent file name\n",
        "  if date_modified_list != []:\n",
        "    index_of_max = date_modified_list.index(max(date_modified_list))\n",
        "    if verbose == True:\n",
        "      display(f'The most recent file is {file_names[index_of_max]}.')\n",
        "\n",
        "    clip = file_names[index_of_max]\n",
        "    size = file_size_list[index_of_max]\n",
        "  else:\n",
        "    clip = 'None'\n",
        "    size = 'None'\n",
        "\n",
        "  return (clip, size)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIF1zv-nEEHX"
      },
      "source": [
        "def Move_Sync_File_for_Analysis(file_name):\n",
        "\n",
        "  # Removes existing 999 file, copies and changes name of most recent file to Left Touch Folder\n",
        "  # destination = '/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "\n",
        "  \n",
        "  if not exists('/content/Mask_RCNN/videos'):\n",
        "    os.mkdir('/content/Mask_RCNN/videos')\n",
        "\n",
        "  destination = '/content/Mask_RCNN/videos'\n",
        "  # file_name = clip\n",
        "  file_source = '/content/drive/My Drive/Sync/'\n",
        "  file_name_with_source = os.path.join(file_source, file_name)\n",
        "  file_name_with_destination = os.path.join(destination, '999.mp4')\n",
        "  if exists(file_name_with_destination):\n",
        "    os.remove(file_name_with_destination)\n",
        "    if verbose == True:\n",
        "      f'{file_name} was removed from the Left Touch Folder.'\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      f'There was no file to remove from the Left Touch Folder.'\n",
        "  shutil.copy(file_name_with_source, file_name_with_destination)\n",
        "\n",
        "  if exists(file_name_with_destination):\n",
        "    display(f'The file {file_name} has been moved to {destination}.')\n",
        "  else:\n",
        "    display(f'The file {file_name} was not successfully moved.')\n",
        "\n",
        "  return"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV-myZourBFc"
      },
      "source": [
        "def save_ffmpeg(source, destination, name, fps):\n",
        "\n",
        "  os.chdir(source)\n",
        "  file_name_with_destination = os.path.join(destination, name)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The source of the files is {source}.')\n",
        "    display(f'The save destination is {destination}.')\n",
        "    display(f'Name with destionation is {file_name_with_destination}.')\n",
        "    display(f'The name is {name}.')\n",
        "\n",
        "  !ffmpeg -framerate $fps -i %1d.jpg output.mp4\n",
        "\n",
        "  file_name_with_destination = os.path.join(destination, name)\n",
        "  name = name + '.mp4'\n",
        "\n",
        "  shutil.copy('output.mp4', file_name_with_destination)\n",
        "\n",
        "  return"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAqgK1PSBi2w"
      },
      "source": [
        "def darknet_helper(img, width, height):\n",
        "  # darknet helper function to run detection on image\n",
        "\n",
        "  darknet_image = make_image(width, height, 3)\n",
        "  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img_resized = cv2.resize(img_rgb, (width, height),\n",
        "                              interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "  # get image ratios to convert bounding boxes to proper size\n",
        "  img_height, img_width, _ = img.shape\n",
        "  width_ratio = img_width/width\n",
        "  height_ratio = img_height/height\n",
        "\n",
        "  # run model on darknet style image to get detections\n",
        "  copy_image_from_bytes(darknet_image, img_resized.tobytes())\n",
        "  detections = detect_image(network, class_names, darknet_image)\n",
        "  free_image(darknet_image)\n",
        "\n",
        "  return (detections, width_ratio, height_ratio)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzPInRi0SUGE"
      },
      "source": [
        "def yolov4_run_image(image, return_image):\n",
        "\n",
        "  # %cd '/content/darknet/'\n",
        "\n",
        "  os.chdir('/content/darknet/')\n",
        "\n",
        "  # run test on person.jpg image that comes with repository\n",
        "\n",
        "  detections, width_ratio, height_ratio = darknet_helper(image, width, height)\n",
        "\n",
        "  bbox_temp = []\n",
        "  names_of_objects = ['BG', 'bellguard', 'scorebox', 'torso', 'foot']\n",
        "\n",
        "  if return_image:\n",
        "    for label, confidence, bbox in detections:\n",
        "      left, top, right, bottom = bbox2points(bbox)\n",
        "      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "      cv2.rectangle(image, (left, top), (right, bottom), class_colors[label], 2)\n",
        "      cv2.putText(image, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        class_colors[label], 2)\n",
        "  else:\n",
        "    image = 'None'\n",
        "\n",
        "  for label, confidence, bbox in detections:\n",
        "    left, top, right, bottom = bbox2points(bbox)\n",
        "    left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "    arr = np.array([top, left, bottom, right])\n",
        "\n",
        "    # change the dtype to 'float64' \n",
        "    arr = arr.astype('int32')\n",
        "    percentage = float(math.floor(float(confidence))/100)\n",
        "    class_id = names_of_objects.index(label)\n",
        "\n",
        "    bbox_temp.append([arr, percentage, class_id])\n",
        "\n",
        "  return(bbox_temp, image)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYUfqyqGmoYp"
      },
      "source": [
        "def number_of_files_in_dir(path):\n",
        "  files = [i for i in os.listdir(path)]\n",
        "  number_of_files = len(files)\n",
        "\n",
        "  return (number_of_files)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfpfuKOI1dwS"
      },
      "source": [
        "def bbox_output(video_name, bbox_clip, output_foot_data):\n",
        "  # Creates a CSV file of each detection of the format:\n",
        "  # [frame, x_center, y_center, detection confidence]\n",
        "\n",
        "  tracked_item = 0\n",
        "\n",
        "  if output_foot_data == True:\n",
        "    tracked_item = 4\n",
        "\n",
        "  if tracked_item != 0:\n",
        "    clip_items = []\n",
        "\n",
        "    for i in range(len(bbox_clip)):\n",
        "      for j in range(len(bbox_clip[i])):\n",
        "        if bbox_clip[i][j][2] == tracked_item:\n",
        "          x_center = int((bbox_clip[i][j][0][3]+bbox_clip[i][j][0][1])/2)\n",
        "          y_center = int((bbox_clip[i][j][0][2]+bbox_clip[i][j][0][0])/2)\n",
        "          clip_items.append([i,x_center,y_center, bbox_clip[i][j][1]])\n",
        "\n",
        "    df = pd.DataFrame(clip_items, columns=['frame','x', 'y', 'confidence'])\n",
        "    %cd '/content/drive/My Drive/Sync/Foot Data'\n",
        "    name = video_name[:-4] + '_foot.csv'\n",
        "    df.to_csv(name, index=False, header = True)\n",
        "\n",
        "\n",
        "  return ()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "dxP-wmmX_X5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7a328bd6-aa6c-4f87-a560-033d49aadc55"
      },
      "source": [
        "# Checks if the Folder Hierarchy is in place\n",
        "\n",
        "display(f'Checking Folder Hierarchy...')\n",
        "create_folder_hierarchy(video_filename, output_bbox_clip, output_foot_data)\n",
        "# if not exists('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips'):\n",
        "#   create_folder_hierarchy(video_filename)\n",
        "# else:\n",
        "#   display(f'The Folder Hierarchy is already in place.')\n",
        "\n",
        "ROW_Model_in_use = 'ROW_model.h5'\n",
        "\n",
        "# Checks if the ROW model is in the Fencing Clip Folder or the top level\n",
        "if exists('/content/drive/My Drive/' + ROW_Model_in_use):\n",
        "  display(f'Loading the Right of Way Model from the top level directory.')\n",
        "  ROW_Model_save_path = '/content/drive/My Drive/'\n",
        "elif exists('/content/drive/My Drive/projects/fencing/Fencing Clips/ROW_model.h5'):\n",
        "  display(f'Loading the Right of Way Model from Fencing Clips directory.')\n",
        "  ROW_Model_save_path = '/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "else:\n",
        "  # wget tool\n",
        "  # https://angelov.ai/post/2020/wget-files-from-gdrive/\n",
        "  # Loads the ROW_Model from a shared file\n",
        "  display(f'Loading the ROW_Model into Content')\n",
        "  %cd '/content/'\n",
        "  # !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1nvgqSTFITW74gPhbYX0hFZAmsN_MSBd_' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1nvgqSTFITW74gPhbYX0hFZAmsN_MSBd_\" -O ROW_model.h5 && rm -rf /tmp/cookies.txt\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xy_yvOmmzHkVzS_voJAXninjFTDFVdLn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1xy_yvOmmzHkVzS_voJAXninjFTDFVdLn\" -O ROW_model.h5 && rm -rf /tmp/cookies.txt\n",
        "  source = 'ROW_model.h5'\n",
        "  ROW_Model_save_path = '/content/'"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Checking Folder Hierarchy...'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Created the Sync Acceleration directory.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Created the Sync Tracked Clips directory.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loading the Right of Way Model from the top level directory.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPQvpuHVa2Bx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b08e3ff6-0215-4b46-e7c9-ea11733c048f"
      },
      "source": [
        "loop_analysis = True\n",
        "# Initializes the Previous and Most Recent Files\n",
        "[most_recent_file, size] = [previous_most_recent_file, size] = Most_Recent_Sync_File()\n",
        "# Sets initial size to zero\n",
        "size = 0\n",
        "while loop_analysis == True:\n",
        "  # Determines if Youtube_Link/Sync_Folder/Stored_Clip are used for analysis\n",
        "  if youtube_link != '':\n",
        "    youtube_clip(youtube_link)\n",
        "    video_filename = '999.mp4'\n",
        "    clip_call = 0\n",
        "  elif run_most_recent_clip == True:\n",
        "    # Loops until a new file is found in the folder\n",
        "    display(f'Waiting for a new file...')\n",
        "    while most_recent_file == previous_most_recent_file:\n",
        "      # Runs only the Most Recent File in the Sync Folder\n",
        "      # # Uses the Most Recent file in the Sync Folder, copies file to 999.mp4 in Left Folder\n",
        "      [most_recent_file, size] = Most_Recent_Sync_File()\n",
        "      file_names = []\n",
        "      file_names.append(most_recent_file)\n",
        "      # Imposes a 10ms time delay\n",
        "      time.sleep(0.010)\n",
        "    \n",
        "    # Saves the Previous Most Recent File for Comparison\n",
        "    previous_most_recent_file = most_recent_file\n",
        "    display(f'Using the Most Recently Synced File {most_recent_file}.')\n",
        "  elif run_entire_sync_folder == True:\n",
        "    # Sets Loop Analysis to False so that the loop only runs once.\n",
        "    loop_analysis = False\n",
        "    file_names = []\n",
        "    path = '/content/drive/My Drive/Sync/'\n",
        "\n",
        "    # Finds every object in a directory\n",
        "    for file in os.listdir(path):\n",
        "      # If the object is a file then it appends it to file_names\n",
        "      name = os.path.join(path, file)\n",
        "      if os.path.isfile(name):\n",
        "        file_names.append(file)\n",
        "\n",
        "  display(f'The files to be run are:')\n",
        "  display(file_names)\n",
        "\n",
        "  # Runs the most recent clip or every clip in the sync folder\n",
        "  for file in file_names:\n",
        "    file_name_acc = file[:-4] + '_acc.csv'\n",
        "    name = os.path.join('/content/drive/My Drive/Sync/Acceleration',file_name_acc)\n",
        "    display(f'The name of the acceleration file is {name}.')\n",
        "\n",
        "      \n",
        "    # Only runs a video if its acceleration file does not exist and within size limits\n",
        "    if not os.path.exists(name) and size < size_max:\n",
        "      display(f'Processing File {file}.')\n",
        "\n",
        "      try:\n",
        "        # Processes the Video\n",
        "        # if use_sync_folder == True or run_sync_folder == True:\n",
        "        if run_most_recent_clip == True or run_entire_sync_folder == True:\n",
        "          [bbox_clip, frame_count, capture_width, capture_height, clip_vector_previous, \\\n",
        "          clip_vector, fps, save_image_list, too_few_frames] = \\\n",
        "          process_video_clip(file, 'Sync', remove_duplicate_frames)\n",
        "        elif use_youtube_link == True:\n",
        "          display(f'Use Youtube Link')\n",
        "          assert Error\n",
        "        else:\n",
        "          display(f'A method of Analysis must be set to True:')\n",
        "          assert Error\n",
        "\n",
        "        if not too_few_frames:\n",
        "          # Loads ROW Model\n",
        "          display(f'Determining Right of Way of the Clip.')\n",
        "          model_ROW = load_model(os.path.join(ROW_Model_save_path, ROW_Model_in_use))\n",
        "\n",
        "          clip_folder = '/content/drive/My Drive/Sync/Acceleration'\n",
        "          file_name_acc = file[:-4] + '_acc.csv'\n",
        "\n",
        "          x = load_clip(clip_folder, file_name_acc, capture_width)\n",
        "          pred = model_ROW.predict(x)\n",
        "\n",
        "          pred_list = [pred[0][0], pred[0][1], pred[0][2]]\n",
        "          pred_total = pred[0][0] + pred[0][1] + pred[0][2]\n",
        "          normalized_pred_results = [int(pred[0][0]/pred_total*100), int(pred[0][1]/pred_total*100), int(pred[0][2]/pred_total*100)]\n",
        "          max_value = max(pred_list)\n",
        "          max_index = pred_list.index(max_value)\n",
        "          if max_index == 0:\n",
        "            touch = 'L'\n",
        "          elif max_index == 1:\n",
        "            touch = 'R'\n",
        "          elif max_index == 2:\n",
        "            touch = 'S'\n",
        "          else:\n",
        "            display(f'Touch determination failed.')\n",
        "\n",
        "          if create_output_video == True:\n",
        "\n",
        "            for i in range(len(save_image_list)):\n",
        "              # Saves the Images from the List to jpg format in Save Folder\n",
        "              name = '/content/Mask_RCNN/videos/save/' + str(i) + '.jpg'\n",
        "              cv2.imwrite(name, save_image_list[i])\n",
        "\n",
        "            # Saves the Images as a Video File in the Sync Folder\n",
        "            create_title_card(capture_width, capture_height, normalized_pred_results, pred_total, '', 'save')\n",
        "            # video_name = file[:-4] + '_tracked_' + touch + '_' + str(int(pred[0][0]/pred_total*100)) + '_' + str(int(pred[0][1]/pred_total*100)) + '_' + str(int(pred[0][2]/pred_total*100)) \n",
        "            video_name = file[:-4] + '_tracked_' + touch + '_' + str(int(pred[0][0]/pred_total*100)) + '_' + str(int(pred[0][1]/pred_total*100)) + '_' + str(int(pred[0][2]/pred_total*100)) + '_' + str(int((pred[0][0]+pred[0][1]+pred[0][2])*100)) \n",
        "            # video_name = str(video_filename[:-4]) + '.out'\n",
        "            image_save_dir = '/content/Mask_RCNN/videos/save/'\n",
        "            # if run_sync_folder == True or use_sync_folder == True:\n",
        "            if run_entire_sync_folder == True or run_most_recent_clip == True:\n",
        "              video_save_dir = '/content/drive/My Drive/Sync/Tracked Clips/'\n",
        "            # Sorts Images\n",
        "            [outvid, images] = prepare_video_frames(image_save_dir, video_save_dir, video_name)\n",
        "            # Creates a video from the saved images\n",
        "            destination = '/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "\n",
        "            # Adds suffix for encoding\n",
        "            video_name =  video_name + '.mp4'\n",
        "            save_ffmpeg(image_save_dir, video_save_dir, video_name, fps)\n",
        "          \n",
        "          if output_bbox_clip == True:\n",
        "            bbox_output(video_name, bbox_clip, output_foot_data)\n",
        "\n",
        "        else:\n",
        "          display(f'Too Few Frames to Analyze.')\n",
        "      except Exception as inst:\n",
        "        display(f'Error running {file}.')\n",
        "        traceback.print_exc()\n",
        "        traceback.print_stack\n",
        "    else:\n",
        "      if size > size_max:\n",
        "        display(f'File {file} is {size} kB, which is larger than the max of {size_max} kB.')\n",
        "      if not os.path.exists(name):\n",
        "        # If the file has already been run and its acceleration csv is found\n",
        "        display(f'The file {file} has already been run.')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The files to be run are:'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['Budapest World Cup 2021 MST Szilagyi 1 Kim J 3.mp4',\n",
              " 'Budapest World Cup 2021 MST Szilagyi 2 Kim J 4.mp4',\n",
              " 'Budapest World Cup 2021 MST Szilagyi 3 Kim J 4.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 11 Gu 10.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 13 Gu 12.mp4',\n",
              " 'Budapest World Cup 2021 MST Szilagyi 22 Gu 22.mp4',\n",
              " 'Budapest World Cup 2021 MST Szilagyi 23 Gu 23.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 25 Oh 23.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 26 Oh 25.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 28 Oh 28.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 28 Oh 29.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 35 Kim J 31.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 37 Kim J 33.mp4',\n",
              " 'Budapest World Cup 2021 MST Szatmari 39 Kim J 35.mp4',\n",
              " 'Budapest World Cup 2021 MST Szilagyi 40 Oh 36.mp4']"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 1 Kim J 3_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szilagyi 1 Kim J 3.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szilagyi 1 Kim J 3.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szilagyi 1 Kim J 3.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 72'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 72'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 71, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 60.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 1 Kim J 3_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 70.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:03.20, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   96 fps= 46 q=-1.0 Lsize=     643kB time=00:00:03.10 bitrate=1698.0kbits/s speed=1.48x    \n",
            "video:641kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.306103%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mframe I:2     Avg QP:17.89  size: 40684\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mframe P:24    Avg QP:22.57  size: 13189\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mframe B:70    Avg QP:25.94  size:  3677\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mconsecutive B-frames:  2.1%  2.1%  0.0% 95.8%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mmb I  I16..4: 49.0% 26.2% 24.9%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mmb P  I16..4:  6.6%  5.7%  2.4%  P16..4: 25.6%  6.4%  3.4%  0.0%  0.0%    skip:50.0%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mmb B  I16..4:  0.9%  0.4%  0.4%  B16..8: 21.3%  2.0%  0.6%  direct: 1.4%  skip:72.9%  L0:47.1% L1:49.1% BI: 3.8%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0m8x8 transform intra:32.4% inter:64.9%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mcoded y,uvDC,uvAC intra: 27.6% 46.3% 21.1% inter: 4.1% 8.3% 2.1%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mi16 v,h,dc,p: 35% 51%  6%  7%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 28% 44%  1%  1%  1%  2%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 34% 15%  4%  4%  4%  4%  4%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mi8c dc,h,v,p: 42% 42% 13%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mref P L0: 64.5%  9.4% 18.8%  7.3%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mref B L0: 81.2% 15.2%  3.6%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mref B L1: 93.4%  6.6%\n",
            "\u001b[1;36m[libx264 @ 0x55f955729e00] \u001b[0mkb/s:1638.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 2 Kim J 4_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szilagyi 2 Kim J 4.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szilagyi 2 Kim J 4.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szilagyi 2 Kim J 4.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 76'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 76'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 75, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 62.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -2'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 2 Kim J 4_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 74.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:03.33, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  100 fps= 41 q=-1.0 Lsize=     853kB time=00:00:03.23 bitrate=2160.5kbits/s speed=1.31x    \n",
            "video:851kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.236009%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mframe I:2     Avg QP:19.10  size: 40037\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mframe P:25    Avg QP:22.90  size: 15815\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mframe B:73    Avg QP:25.36  size:  5411\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mconsecutive B-frames:  2.0%  2.0%  0.0% 96.0%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mmb I  I16..4: 30.8% 44.3% 24.9%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mmb P  I16..4:  9.5%  8.0%  3.4%  P16..4: 25.0%  7.2%  3.5%  0.0%  0.0%    skip:43.4%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mmb B  I16..4:  1.5%  0.8%  0.8%  B16..8: 24.8%  3.5%  0.8%  direct: 2.4%  skip:65.3%  L0:43.9% L1:51.1% BI: 5.0%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0m8x8 transform intra:36.8% inter:66.0%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mcoded y,uvDC,uvAC intra: 27.6% 47.1% 19.7% inter: 5.4% 11.2% 2.5%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mi16 v,h,dc,p: 30% 53% 10%  8%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 24% 43%  1%  1%  1%  1%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 38% 15%  3%  4%  3%  4%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mi8c dc,h,v,p: 40% 43% 15%  3%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mref P L0: 60.5% 12.2% 19.9%  7.5%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mref B L0: 84.1% 13.6%  2.3%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mref B L1: 93.2%  6.8%\n",
            "\u001b[1;36m[libx264 @ 0x563f67959e00] \u001b[0mkb/s:2089.12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 3 Kim J 4_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szilagyi 3 Kim J 4.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szilagyi 3 Kim J 4.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szilagyi 3 Kim J 4.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 64'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 64'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 63, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 52.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 3 Kim J 4_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 62.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.93, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   88 fps= 42 q=-1.0 Lsize=     711kB time=00:00:02.83 bitrate=2055.5kbits/s speed=1.36x    \n",
            "video:709kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.261121%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mframe I:2     Avg QP:17.74  size: 44725\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mframe P:22    Avg QP:23.22  size: 15174\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mframe B:64    Avg QP:26.48  size:  4721\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mconsecutive B-frames:  2.3%  0.0%  6.8% 90.9%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mmb I  I16..4: 47.2% 26.1% 26.6%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mmb P  I16..4:  6.5%  7.0%  3.4%  P16..4: 24.8%  7.1%  3.5%  0.0%  0.0%    skip:47.7%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mmb B  I16..4:  1.3%  0.7%  0.8%  B16..8: 23.4%  2.9%  0.7%  direct: 1.9%  skip:68.3%  L0:49.0% L1:46.4% BI: 4.6%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0m8x8 transform intra:33.1% inter:62.0%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mcoded y,uvDC,uvAC intra: 31.3% 46.8% 20.3% inter: 5.3% 9.5% 2.1%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mi16 v,h,dc,p: 39% 44% 10%  7%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 22% 49%  2%  1%  1%  1%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 36% 16%  3%  4%  4%  5%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mi8c dc,h,v,p: 42% 40% 15%  3%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mref P L0: 64.1%  9.6% 18.7%  7.6%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mref B L0: 81.9% 14.8%  3.3%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mref B L1: 94.4%  5.6%\n",
            "\u001b[1;36m[libx264 @ 0x5574bd661e00] \u001b[0mkb/s:1978.39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 11 Gu 10_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 11 Gu 10.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 11 Gu 10.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 11 Gu 10.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 53'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 53'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 52, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 45.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -8'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 11 Gu 10_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 51.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.57, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   77 fps= 41 q=-1.0 Lsize=     650kB time=00:00:02.46 bitrate=2159.4kbits/s speed=1.31x    \n",
            "video:649kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.266838%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mframe I:2     Avg QP:19.52  size: 44702\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mframe P:19    Avg QP:22.15  size: 14621\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mframe B:56    Avg QP:25.15  size:  5289\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mconsecutive B-frames:  2.6%  0.0%  3.9% 93.5%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mmb I  I16..4: 38.0% 34.1% 27.8%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mmb P  I16..4:  6.7%  6.9%  4.0%  P16..4: 21.6%  6.2%  2.9%  0.0%  0.0%    skip:51.6%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mmb B  I16..4:  1.4%  0.8%  0.9%  B16..8: 21.9%  3.4%  0.8%  direct: 1.5%  skip:69.2%  L0:53.3% L1:41.3% BI: 5.5%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0m8x8 transform intra:34.3% inter:61.7%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mcoded y,uvDC,uvAC intra: 35.6% 45.4% 20.2% inter: 4.9% 7.9% 2.2%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mi16 v,h,dc,p: 38% 46%  9%  8%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 23% 37%  2%  2%  2%  2%  2%  3%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 35% 15%  3%  4%  4%  5%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mi8c dc,h,v,p: 43% 38% 16%  3%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mref P L0: 63.4%  8.4% 20.7%  7.5%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mref B L0: 81.7% 15.7%  2.6%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mref B L1: 94.9%  5.1%\n",
            "\u001b[1;36m[libx264 @ 0x557f7387fe00] \u001b[0mkb/s:2067.69\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 13 Gu 12_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 13 Gu 12.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 13 Gu 12.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 13 Gu 12.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 44'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 44'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 43, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 39.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -11'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 13 Gu 12_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63002d0200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 42.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.27, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   68 fps= 44 q=-1.0 Lsize=     523kB time=00:00:02.16 bitrate=1977.2kbits/s speed=1.39x    \n",
            "video:521kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.313210%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mframe I:2     Avg QP:18.69  size: 44848\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mframe P:17    Avg QP:22.47  size: 12986\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mframe B:49    Avg QP:24.99  size:  4545\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mconsecutive B-frames:  2.9%  2.9%  0.0% 94.1%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mmb I  I16..4: 41.5% 31.0% 27.5%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mmb P  I16..4:  5.1%  5.8%  3.5%  P16..4: 20.2%  5.8%  2.8%  0.0%  0.0%    skip:56.9%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mmb B  I16..4:  1.1%  0.7%  0.9%  B16..8: 18.8%  2.8%  0.7%  direct: 1.1%  skip:73.9%  L0:52.8% L1:42.5% BI: 4.8%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0m8x8 transform intra:33.8% inter:59.2%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mcoded y,uvDC,uvAC intra: 36.2% 43.6% 19.7% inter: 4.3% 6.9% 1.9%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mi16 v,h,dc,p: 40% 45%  7%  7%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 23% 37%  2%  2%  2%  2%  2%  3%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 29% 34% 15%  4%  4%  4%  5%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mi8c dc,h,v,p: 45% 36% 16%  3%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mref P L0: 66.3%  8.2% 17.8%  7.7%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mref B L0: 82.9% 13.9%  3.1%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mref B L1: 94.2%  5.8%\n",
            "\u001b[1;36m[libx264 @ 0x564178f47e00] \u001b[0mkb/s:1881.66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 22 Gu 22_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szilagyi 22 Gu 22.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szilagyi 22 Gu 22.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szilagyi 22 Gu 22.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 69'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 69'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 68, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 57.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 22 Gu 22_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63001934d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 67.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:03.10, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   93 fps= 40 q=-1.0 Lsize=     851kB time=00:00:03.00 bitrate=2323.2kbits/s speed= 1.3x    \n",
            "video:849kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.225943%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mframe I:2     Avg QP:18.44  size: 43634\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mframe P:23    Avg QP:21.95  size: 16772\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mframe B:68    Avg QP:23.62  size:  5817\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mconsecutive B-frames:  2.2%  0.0%  3.2% 94.6%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mmb I  I16..4: 37.1% 36.3% 26.6%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mmb P  I16..4:  8.2%  7.6%  5.1%  P16..4: 23.1%  6.8%  3.4%  0.0%  0.0%    skip:45.9%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mmb B  I16..4:  1.7%  0.8%  1.1%  B16..8: 23.8%  3.8%  0.9%  direct: 1.8%  skip:66.1%  L0:49.8% L1:44.9% BI: 5.3%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0m8x8 transform intra:32.6% inter:59.0%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mcoded y,uvDC,uvAC intra: 33.9% 45.8% 18.5% inter: 5.7% 9.5% 2.4%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mi16 v,h,dc,p: 30% 51% 11%  8%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 24% 43%  2%  1%  1%  1%  1%  3%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 38% 16%  3%  4%  3%  4%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mi8c dc,h,v,p: 40% 43% 15%  3%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mref P L0: 65.5%  9.0% 18.0%  7.5%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mref B L0: 83.3% 13.8%  2.9%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mref B L1: 94.5%  5.5%\n",
            "\u001b[1;36m[libx264 @ 0x562ff170fe00] \u001b[0mkb/s:2241.44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 23 Gu 23_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szilagyi 23 Gu 23.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szilagyi 23 Gu 23.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szilagyi 23 Gu 23.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 92'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 92'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 91, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 83.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -7'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 23 Gu 23_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6300073950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 90.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:03.87, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  116 fps= 39 q=-1.0 Lsize=    1030kB time=00:00:03.76 bitrate=2240.0kbits/s speed=1.26x    \n",
            "video:1028kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.213597%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mframe I:2     Avg QP:18.22  size: 41999\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mframe P:29    Avg QP:22.08  size: 16814\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mframe B:85    Avg QP:24.11  size:  5649\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mconsecutive B-frames:  1.7%  0.0%  5.2% 93.1%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mmb I  I16..4: 46.8% 27.7% 25.5%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mmb P  I16..4:  9.6%  8.2%  3.7%  P16..4: 26.2%  7.7%  3.9%  0.0%  0.0%    skip:40.7%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mmb B  I16..4:  1.8%  1.0%  0.9%  B16..8: 26.5%  3.9%  1.0%  direct: 1.8%  skip:63.2%  L0:47.0% L1:47.7% BI: 5.3%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0m8x8 transform intra:33.2% inter:62.5%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mcoded y,uvDC,uvAC intra: 28.5% 44.9% 17.0% inter: 5.8% 10.4% 2.5%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mi16 v,h,dc,p: 29% 55%  9%  8%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 28% 42%  2%  1%  1%  1%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 37% 16%  3%  4%  4%  4%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mi8c dc,h,v,p: 39% 44% 14%  3%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mref P L0: 62.0%  9.3% 20.1%  8.5%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mref B L0: 81.9% 15.1%  3.0%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mref B L1: 93.6%  6.4%\n",
            "\u001b[1;36m[libx264 @ 0x56420f5f9e00] \u001b[0mkb/s:2176.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 25 Oh 23_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 25 Oh 23.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 25 Oh 23.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 25 Oh 23.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 77'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 77'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 76, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 64.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -3'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 25 Oh 23_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6264287050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 75.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:03.37, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  101 fps= 40 q=-1.0 Lsize=     892kB time=00:00:03.26 bitrate=2237.3kbits/s speed=1.29x    \n",
            "video:890kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.225995%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mframe I:2     Avg QP:19.68  size: 43421\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mframe P:25    Avg QP:22.23  size: 17072\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mframe B:74    Avg QP:25.88  size:  5368\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mconsecutive B-frames:  2.0%  0.0%  3.0% 95.0%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mmb I  I16..4: 39.2% 33.4% 27.4%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mmb P  I16..4:  8.8%  7.2%  4.2%  P16..4: 24.6%  7.5%  3.8%  0.0%  0.0%    skip:43.8%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mmb B  I16..4:  1.5%  0.8%  0.9%  B16..8: 25.1%  3.8%  0.9%  direct: 1.4%  skip:65.7%  L0:49.6% L1:44.9% BI: 5.5%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0m8x8 transform intra:32.4% inter:61.0%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mcoded y,uvDC,uvAC intra: 32.6% 45.8% 18.9% inter: 5.7% 9.5% 2.5%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mi16 v,h,dc,p: 30% 52% 11%  7%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 28% 40%  2%  1%  2%  1%  2%  3%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 36% 16%  3%  4%  3%  4%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mi8c dc,h,v,p: 39% 44% 15%  3%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mref P L0: 63.5% 10.0% 19.2%  7.3%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mref B L0: 84.3% 12.9%  2.8%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mref B L1: 94.0%  6.0%\n",
            "\u001b[1;36m[libx264 @ 0x56468a295e00] \u001b[0mkb/s:2164.36\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 26 Oh 25_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 26 Oh 25.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 26 Oh 25.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 26 Oh 25.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 72'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 72'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 71, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 66.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -10'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 26 Oh 25_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f630825b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 70.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:03.20, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   96 fps= 41 q=-1.0 Lsize=     870kB time=00:00:03.10 bitrate=2298.1kbits/s speed=1.32x    \n",
            "video:868kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.225988%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mframe I:2     Avg QP:18.40  size: 38137\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mframe P:25    Avg QP:21.89  size: 16389\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mframe B:69    Avg QP:22.96  size:  5824\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mconsecutive B-frames:  2.1%  6.2%  0.0% 91.7%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mmb I  I16..4: 44.3% 31.3% 24.4%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mmb P  I16..4:  8.1%  7.9%  4.3%  P16..4: 25.4%  7.0%  3.3%  0.0%  0.0%    skip:44.1%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mmb B  I16..4:  1.7%  0.9%  0.9%  B16..8: 23.5%  3.9%  0.9%  direct: 2.4%  skip:65.6%  L0:50.1% L1:44.7% BI: 5.2%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0m8x8 transform intra:33.8% inter:64.1%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mcoded y,uvDC,uvAC intra: 30.1% 48.1% 20.2% inter: 5.4% 11.3% 2.4%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mi16 v,h,dc,p: 33% 52%  9%  7%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 21% 27% 42%  2%  1%  1%  2%  1%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 36% 15%  4%  4%  4%  5%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mi8c dc,h,v,p: 39% 43% 15%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mref P L0: 60.5%  8.5% 22.3%  8.6%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mref B L0: 83.6% 13.9%  2.5%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mref B L1: 95.1%  4.9%\n",
            "\u001b[1;36m[libx264 @ 0x55d27650de00] \u001b[0mkb/s:2219.63\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 28 Oh 28_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 28 Oh 28.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 28 Oh 28.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 28 Oh 28.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 55'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 55'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 54, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 37.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 28 Oh 28_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6264353d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 53.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.63, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   79 fps= 44 q=-1.0 Lsize=     629kB time=00:00:02.53 bitrate=2035.2kbits/s speed=1.41x    \n",
            "video:628kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.279450%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mframe I:2     Avg QP:18.59  size: 44900\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mframe P:20    Avg QP:22.90  size: 13522\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mframe B:57    Avg QP:25.52  size:  4943\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mconsecutive B-frames:  3.8%  0.0%  0.0% 96.2%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mmb I  I16..4: 41.0% 30.6% 28.5%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mmb P  I16..4:  5.7%  6.0%  3.8%  P16..4: 20.8%  5.5%  2.8%  0.0%  0.0%    skip:55.5%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mmb B  I16..4:  1.2%  0.7%  1.1%  B16..8: 18.5%  2.7%  0.7%  direct: 1.2%  skip:73.9%  L0:51.7% L1:43.2% BI: 5.1%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0m8x8 transform intra:32.6% inter:61.0%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mcoded y,uvDC,uvAC intra: 36.5% 47.2% 22.3% inter: 4.2% 7.2% 1.9%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mi16 v,h,dc,p: 38% 46%  9%  7%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 22% 42%  2%  2%  2%  2%  2%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 29% 31% 14%  4%  4%  4%  5%  4%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mi8c dc,h,v,p: 43% 37% 16%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mref P L0: 64.4%  8.2% 19.6%  7.8%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mref B L0: 83.9% 13.1%  3.1%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mref B L1: 93.2%  6.8%\n",
            "\u001b[1;36m[libx264 @ 0x55b4414efe00] \u001b[0mkb/s:1950.39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 28 Oh 29_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 28 Oh 29.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 28 Oh 29.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 28 Oh 29.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 43'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 43'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 42, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 34.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -7'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 28 Oh 29_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63002d0cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 41.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.23, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   67 fps= 39 q=-1.0 Lsize=     620kB time=00:00:02.13 bitrate=2382.6kbits/s speed=1.24x    \n",
            "video:619kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.260675%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mframe I:2     Avg QP:19.66  size: 40909\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mframe P:17    Avg QP:22.54  size: 15173\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mframe B:48    Avg QP:24.83  size:  6110\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mconsecutive B-frames:  4.5%  0.0%  0.0% 95.5%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mmb I  I16..4: 36.9% 37.5% 25.6%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mmb P  I16..4:  8.7%  8.1%  4.9%  P16..4: 19.4%  5.5%  2.5%  0.0%  0.0%    skip:50.8%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mmb B  I16..4:  2.2%  1.1%  1.1%  B16..8: 21.1%  4.1%  0.9%  direct: 2.1%  skip:67.2%  L0:44.4% L1:50.2% BI: 5.4%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0m8x8 transform intra:34.1% inter:64.6%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mcoded y,uvDC,uvAC intra: 33.4% 46.6% 19.4% inter: 5.7% 10.4% 2.2%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mi16 v,h,dc,p: 32% 52% 10%  7%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 28% 36%  2%  1%  2%  2%  2%  3%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 40% 15%  3%  4%  3%  4%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mi8c dc,h,v,p: 40% 43% 14%  3%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mref P L0: 65.1%  8.7% 18.5%  7.6%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mref B L0: 85.0% 12.3%  2.6%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mref B L1: 94.3%  5.7%\n",
            "\u001b[1;36m[libx264 @ 0x561273d9de00] \u001b[0mkb/s:2267.64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 35 Kim J 31_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 35 Kim J 31.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 35 Kim J 31.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 35 Kim J 31.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 41'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 41'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 40, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 30.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -5'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 35 Kim J 31_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6300109680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 39.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.17, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   65 fps= 43 q=-1.0 Lsize=     497kB time=00:00:02.06 bitrate=1969.4kbits/s speed=1.38x    \n",
            "video:495kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.321002%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mframe I:2     Avg QP:18.20  size: 38664\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mframe P:16    Avg QP:22.58  size: 12258\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mframe B:47    Avg QP:25.38  size:  4958\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mconsecutive B-frames:  3.1%  0.0%  4.6% 92.3%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mmb I  I16..4: 48.5% 26.7% 24.7%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mmb P  I16..4:  9.5%  8.0%  3.2%  P16..4: 19.4%  5.3%  2.2%  0.0%  0.0%    skip:52.5%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mmb B  I16..4:  1.5%  0.6%  0.6%  B16..8: 19.7%  3.1%  0.8%  direct: 3.3%  skip:70.3%  L0:49.9% L1:44.5% BI: 5.6%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0m8x8 transform intra:31.9% inter:67.3%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mcoded y,uvDC,uvAC intra: 23.7% 44.9% 17.2% inter: 4.3% 10.6% 1.8%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mi16 v,h,dc,p: 37% 52%  7%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 21% 48%  1%  1%  1%  1%  1%  2%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 37% 13%  4%  4%  4%  5%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mi8c dc,h,v,p: 42% 43% 13%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mref P L0: 66.6%  8.0% 18.3%  7.0%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mref B L0: 82.6% 14.7%  2.7%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mref B L1: 95.0%  5.0%\n",
            "\u001b[1;36m[libx264 @ 0x55c836023e00] \u001b[0mkb/s:1870.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 37 Kim J 33_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 37 Kim J 33.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 37 Kim J 33.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 37 Kim J 33.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 53'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 53'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 52, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 38.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -1'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 37 Kim J 33_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6300495ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 51.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.57, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   77 fps= 46 q=-1.0 Lsize=     593kB time=00:00:02.46 bitrate=1970.8kbits/s speed=1.47x    \n",
            "video:592kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.292450%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mframe I:2     Avg QP:18.57  size: 43220\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mframe P:19    Avg QP:22.31  size: 13159\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mframe B:56    Avg QP:25.26  size:  4799\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mconsecutive B-frames:  2.6%  0.0%  3.9% 93.5%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mmb I  I16..4: 42.1% 30.8% 27.1%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mmb P  I16..4:  5.8%  5.9%  3.4%  P16..4: 19.8%  5.3%  2.8%  0.0%  0.0%    skip:57.1%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mmb B  I16..4:  1.0%  0.7%  1.0%  B16..8: 18.4%  2.6%  0.7%  direct: 1.1%  skip:74.4%  L0:54.8% L1:40.7% BI: 4.5%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0m8x8 transform intra:33.0% inter:58.0%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mcoded y,uvDC,uvAC intra: 35.5% 45.5% 22.1% inter: 4.0% 6.8% 2.0%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mi16 v,h,dc,p: 39% 47%  8%  7%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 26% 37%  2%  2%  2%  2%  2%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 32% 15%  4%  4%  4%  5%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mi8c dc,h,v,p: 43% 39% 16%  2%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mref P L0: 66.8%  8.0% 18.0%  7.2%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mref B L0: 81.2% 15.9%  2.9%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mref B L1: 93.6%  6.4%\n",
            "\u001b[1;36m[libx264 @ 0x55ca3d09de00] \u001b[0mkb/s:1886.42\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 39 Kim J 35_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szatmari 39 Kim J 35.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szatmari 39 Kim J 35.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szatmari 39 Kim J 35.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 43'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 43'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 42, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 33.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -6'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szatmari 39 Kim J 35_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6300629560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 41.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:02.23, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=   67 fps= 43 q=-1.0 Lsize=     542kB time=00:00:02.13 bitrate=2081.8kbits/s speed=1.36x    \n",
            "video:541kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.298460%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mframe I:2     Avg QP:18.55  size: 42217\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mframe P:17    Avg QP:23.22  size: 13005\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mframe B:48    Avg QP:26.64  size:  5152\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mconsecutive B-frames:  4.5%  0.0%  0.0% 95.5%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mmb I  I16..4: 38.7% 34.9% 26.4%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mmb P  I16..4:  5.8%  6.2%  3.7%  P16..4: 19.4%  5.4%  2.4%  0.0%  0.0%    skip:57.1%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mmb B  I16..4:  1.4%  0.9%  1.0%  B16..8: 19.7%  3.2%  0.8%  direct: 1.4%  skip:71.7%  L0:51.0% L1:44.0% BI: 5.0%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0m8x8 transform intra:34.8% inter:62.3%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mcoded y,uvDC,uvAC intra: 35.1% 44.1% 18.7% inter: 4.5% 7.5% 1.8%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mi16 v,h,dc,p: 39% 45%  8%  8%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 24% 39%  2%  1%  1%  2%  2%  3%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30% 32% 14%  4%  4%  4%  5%  4%  4%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mi8c dc,h,v,p: 45% 36% 16%  3%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mref P L0: 64.7%  8.4% 19.3%  7.6%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mref B L0: 82.8% 14.1%  3.1%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mref B L1: 93.7%  6.3%\n",
            "\u001b[1;36m[libx264 @ 0x564e75231e00] \u001b[0mkb/s:1980.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The name of the acceleration file is /content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 40 Oh 36_acc.csv.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Processing File Budapest World Cup 2021 MST Szilagyi 40 Oh 36.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: Budapest World Cup 2021 MST Szilagyi 40 Oh 36.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The file to be read is /content/drive/My Drive/Sync/Budapest World Cup 2021 MST Szilagyi 40 Oh 36.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 86'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 86'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The remaining frames are 85, which is greater than 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Engarde Positioning was successful, continuing with clip analysis.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The final engarde length was 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last tracked frame is 73.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The frames after light max has been adjusted to -3'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Determining Right of Way of the Clip.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Sync/Acceleration/Budapest World Cup 2021 MST Szilagyi 40 Oh 36_acc.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 98, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 98, 4), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 96, 4).\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f63003b6170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The last image is 84.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%1d.jpg':\n",
            "  Duration: 00:00:03.67, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  110 fps= 41 q=-1.0 Lsize=     910kB time=00:00:03.56 bitrate=2089.1kbits/s speed=1.32x    \n",
            "video:907kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.232443%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mframe I:2     Avg QP:18.94  size: 41620\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mframe P:28    Avg QP:22.15  size: 15936\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mframe B:80    Avg QP:25.69  size:  4989\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mconsecutive B-frames:  1.8%  1.8%  5.5% 90.9%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mmb I  I16..4: 40.9% 33.3% 25.8%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mmb P  I16..4:  7.0%  6.2%  3.1%  P16..4: 27.2%  8.2%  4.0%  0.0%  0.0%    skip:44.3%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mmb B  I16..4:  1.3%  0.7%  0.7%  B16..8: 25.0%  3.6%  0.9%  direct: 1.6%  skip:66.3%  L0:45.0% L1:49.6% BI: 5.4%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0m8x8 transform intra:33.8% inter:62.4%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mcoded y,uvDC,uvAC intra: 30.0% 46.3% 19.6% inter: 5.4% 9.7% 2.3%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mi16 v,h,dc,p: 34% 51%  9%  7%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 24% 43%  2%  1%  1%  2%  1%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 36% 14%  3%  4%  4%  5%  3%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mi8c dc,h,v,p: 40% 41% 16%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mref P L0: 61.3%  9.4% 21.7%  7.6%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mref B L0: 82.5% 14.6%  2.9%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mref B L1: 93.7%  6.3%\n",
            "\u001b[1;36m[libx264 @ 0x55e06df85e00] \u001b[0mkb/s:2025.97\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}