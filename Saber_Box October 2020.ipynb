{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saber Box.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI8gKtzJ_7J3",
        "outputId": "6b4e3f07-d627-4283-ef04-cbaa9c90cf3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from os.path import exists, join, basename, splitext\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "COLAB = True\n",
        "%tensorflow_version 1.x \n",
        "# %tensorflow_version 2.x\n",
        "print(\"Note: using Google CoLab\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "TensorFlow 1.x selected.\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR2dQlPrqZ8"
      },
      "source": [
        "# Leave youtube Link as '' to use saved clip\n",
        "youtube_link = 'https://youtu.be/A1hKs88uVLQ?t=225'\n",
        "video_filename = '195.mp4'\n",
        "# 0,1,2,3 for Left,Right,Simul,Test Folder\n",
        "clip_call = 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IERHKVkEPos",
        "cellView": "code"
      },
      "source": [
        "#@title Initial Parameters\n",
        "# Establishes Initial Parameters\n",
        "import time\n",
        "time_stamp_dict = {}\n",
        "time_stamp_dict[\"connect_to_google_drive\"] = time.time()\n",
        "# For creating  a new .h5 detection model\n",
        "train_model = False\n",
        "# Provides additional comment feedback while running\n",
        "verbose = False\n",
        "# Crops each from to the tracking boxes\n",
        "overlay_masking_boxes = False\n",
        "# Adjusts the Reperesentative Dots based on camera motion implied by scoring box movement.\n",
        "camera_motion_compensate = False\n",
        "# Smooths the Bellguard positions\n",
        "smooth_video_clip = False\n",
        "# Assumes two lights if Bellguards are close to each other. Reduces dependency on Box detection.\n",
        "assume_lights = True\n",
        "# Ignores Lights from the Scorebox. Mitigates poor scorebox tracking.\n",
        "ignore_box_lights = True\n",
        "# Tests and Removes Duplicate Frames from the video\n",
        "remove_duplicate_frames = True\n",
        "# Downloads out and representative_out videos\n",
        "download_videos = True\n",
        "# Analyzes the Action\n",
        "analyze_action = True\n",
        "# Allows for Simple Usage\n",
        "simplified = False\n",
        "# Allows for the use of Exclusion Areas\n",
        "use_Exclusion_Areas = True\n",
        "# Either Creates a separate Representative Clip(True) or an Overlay(False)\n",
        "generate_representative = False\n",
        "\n",
        "# Parameters:\n",
        "min_torso_confidence = 0.80\n",
        "bellguard_confidence = 0.60\n",
        "# Provides for a higher confidence of bellguard detection\n",
        "bellguard_confidence_high = 0.70\n",
        "# Provides for a higher confidence of bellguard detection\n",
        "bellguard_confidence_very_high = 0.80\n",
        "# Provides for a higher confidence of bellguard detection\n",
        "bellguard_confidence_extra_very_high = 0.90\n",
        "# Allows for a different required confidence for initial detection than tracking\n",
        "bellguard_tracking_det_offset = 0.15\n",
        "wrist_conf_min = 3\n",
        "wrist_conf_high = 4\n",
        "wrist_conf_very_high = 6\n",
        "wrist_conf_extra_very_high = 8\n",
        "knee_conf_min = 3\n",
        "# The Threshold for determining duplicate frames\n",
        "duplicate_threshold_factor = 0.75\n",
        "# The Threshold for determining camera motion\n",
        "camera_motion_threshold_factor = 8\n",
        "# Bellguard difference as a fraction of capture width to assume lights on\n",
        "position_difference_ratio = .065\n",
        "\n",
        "time_stamp_dict[\"initial_parameters\"] = time.time()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZEgNVlj7ljV",
        "cellView": "code",
        "outputId": "c796401a-18b4-40e9-9031-7a8bfa368047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Git Downloads\n",
        "if not exists('keypoint.py'):\n",
        "  !wget https://raw.githubusercontent.com/facebookresearch/maskrcnn-benchmark/e0a525a0139baf7086117b7ed3fd318a4878d71c/maskrcnn_benchmark/structures/keypoint.py\n",
        "from keypoint import PersonKeypoints\n",
        "\n",
        "try:\n",
        "\n",
        "  # Mask_RCNN include setup.py\n",
        "  !git clone https://github.com/matterport/Mask_RCNN.git\n",
        "  %cd /content/Mask_RCNN/\n",
        "  !python setup.py install\n",
        "\n",
        "  # Downloads mask-rcnn-coco weights to the working directory, Mask_RCNN\n",
        "  !wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "\n",
        "except:\n",
        "  display(f'Error Downloading Mask RCNN from GitHub.')\n",
        "\n",
        "time_stamp_dict[\"key_point_and_mask_rcnn_git_download\"] = time.time()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Mask_RCNN' already exists and is not an empty directory.\n",
            "/content/Mask_RCNN\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing mask_rcnn.egg-info/PKG-INFO\n",
            "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
            "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/mask_rcnn-2.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mask_rcnn-2.1-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Copying mask_rcnn-2.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "mask-rcnn 2.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Processing dependencies for mask-rcnn==2.1\n",
            "Finished processing dependencies for mask-rcnn==2.1\n",
            "--2020-10-23 16:10:10--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201023%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201023T161010Z&X-Amz-Expires=300&X-Amz-Signature=31996d492e07f6ff9fd1d4b05a4bba907510ed5c982b267015b90b4134ca3001&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-10-23 16:10:10--  https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201023%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201023T161010Z&X-Amz-Expires=300&X-Amz-Signature=31996d492e07f6ff9fd1d4b05a4bba907510ed5c982b267015b90b4134ca3001&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.39.220\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.39.220|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257557808 (246M) [application/octet-stream]\n",
            "Saving to: ‘mask_rcnn_coco.h5.3’\n",
            "\n",
            "mask_rcnn_coco.h5.3 100%[===================>] 245.63M  76.6MB/s    in 3.2s    \n",
            "\n",
            "2020-10-23 16:10:13 (76.6 MB/s) - ‘mask_rcnn_coco.h5.3’ saved [257557808/257557808]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alv-tSgo5hMQ",
        "cellView": "code",
        "outputId": "5ef24096-7d3f-40b1-a434-08addc266afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Imports\n",
        "from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "import pandas as pd\n",
        "from xml.etree import ElementTree\n",
        "from PIL import Image\n",
        "# from mrcnn.utils import Dataset\n",
        "from matplotlib.patches import Rectangle\n",
        "import random\n",
        "import math\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "import sys\n",
        "import statistics\n",
        "import PIL\n",
        "import torchvision\n",
        "import torch\n",
        "torch.set_grad_enabled(False)\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "import os\n",
        "from os import listdir\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "from scipy import signal\n",
        "from skimage import data, img_as_float\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from tensorflow.keras.models import load_model\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "from mrcnn.utils import compute_ap\n",
        "from mrcnn.model import load_image_gt\n",
        "from mrcnn.model import mold_image\n",
        "from mrcnn.visualize import display_instances\n",
        "from mrcnn.utils import extract_bboxes\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "from mrcnn.utils import Dataset\n",
        "\n",
        "import imgaug  # https://github.com/aleju/imgaug (pip3 install imageaug)\n",
        "\n",
        "time_stamp_dict[\"imports\"] = time.time()\n",
        "\n",
        "#For Human Pose Analysis\n",
        "plt.rcParams[\"axes.grid\"] = False\n",
        "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
        "model = model.eval().cuda()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru6n5JlJ668f",
        "cellView": "code"
      },
      "source": [
        "#@title Vis_Keypoints\n",
        "def vis_keypoints(img, kps, draw, kp_thresh=2, alpha=0.7):\n",
        "    #Returns Fencer_Data from Human Pose\n",
        "    \"\"\"Visualizes keypoints (adapted from vis_one_image).\n",
        "    kps has shape (4, #keypoints) where 4 rows are (x, y, logit, prob).\n",
        "    \"\"\"\n",
        "    dataset_keypoints = PersonKeypoints.NAMES\n",
        "    kp_lines = PersonKeypoints.CONNECTIONS\n",
        "\n",
        "    #The keypoints of interest are [left wrist, right wrist, left knee, right knee, left shoulder, right shoulder] with [x,y,confidence] for each point\n",
        "    fencer_data = []\n",
        "    fencer_kp = [9,10,13,14,5,6]\n",
        "\n",
        "    for keypoint in fencer_kp:    \n",
        "      fencer_data.append([int(kps[0][keypoint]),int(kps[1][keypoint]),int(kps[2][keypoint])])\n",
        "\n",
        "    if draw == True:\n",
        "      # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n",
        "      cmap = plt.get_cmap('rainbow')\n",
        "      colors = [cmap(i) for i in np.linspace(0, 1, len(kp_lines) + 2)]\n",
        "      colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n",
        "\n",
        "      # Perform the drawing on a copy of the image, to allow for blending.\n",
        "      kp_mask = np.copy(img)\n",
        "\n",
        "      # Draw mid shoulder / mid hip first for better visualization.\n",
        "      mid_shoulder = (\n",
        "          kps[:2, dataset_keypoints.index('right_shoulder')] +\n",
        "          kps[:2, dataset_keypoints.index('left_shoulder')]) / 2.0\n",
        "      sc_mid_shoulder = np.minimum(\n",
        "          kps[2, dataset_keypoints.index('right_shoulder')],\n",
        "          kps[2, dataset_keypoints.index('left_shoulder')])\n",
        "      mid_hip = (\n",
        "          kps[:2, dataset_keypoints.index('right_hip')] +\n",
        "          kps[:2, dataset_keypoints.index('left_hip')]) / 2.0\n",
        "      sc_mid_hip = np.minimum(\n",
        "          kps[2, dataset_keypoints.index('right_hip')],\n",
        "          kps[2, dataset_keypoints.index('left_hip')])\n",
        "      nose_idx = dataset_keypoints.index('nose')\n",
        "      if sc_mid_shoulder > kp_thresh and kps[2, nose_idx] > kp_thresh:\n",
        "          cv2.line(\n",
        "              kp_mask, tuple(mid_shoulder), tuple(kps[:2, nose_idx]),\n",
        "              color=colors[len(kp_lines)], thickness=2, lineType=cv2.LINE_AA)\n",
        "      if sc_mid_shoulder > kp_thresh and sc_mid_hip > kp_thresh:\n",
        "          cv2.line(\n",
        "              kp_mask, tuple(mid_shoulder), tuple(mid_hip),\n",
        "              color=colors[len(kp_lines) + 1], thickness=2, lineType=cv2.LINE_AA)\n",
        "\n",
        "      # Draw the keypoints.\n",
        "      for l in range(len(kp_lines)):\n",
        "          i1 = kp_lines[l][0]\n",
        "          i2 = kp_lines[l][1]\n",
        "          p1 = kps[0, i1], kps[1, i1]\n",
        "          p2 = kps[0, i2], kps[1, i2]\n",
        "          if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n",
        "              cv2.line(\n",
        "                  kp_mask, p1, p2,\n",
        "                  color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n",
        "          if kps[2, i1] > kp_thresh:\n",
        "              cv2.circle(\n",
        "                  kp_mask, p1,\n",
        "                  radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n",
        "          if kps[2, i2] > kp_thresh:\n",
        "              cv2.circle(\n",
        "                  kp_mask, p2,\n",
        "                  radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n",
        "\n",
        "    # Blend the keypoints.\n",
        "    return [cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0), fencer_data]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyRq47FB5Z_y",
        "cellView": "code"
      },
      "source": [
        "#@title Overlay_Keypoints\n",
        "def overlay_keypoints(image, kps, scores, draw):\n",
        "  kps = torch.cat((kps[:, :, 0:2], scores[:, :, None]), dim=2).cpu().numpy()\n",
        "  fencer_data = []\n",
        "  for region in kps:\n",
        "    [image, fencer_data_temp] = vis_keypoints(image, region.transpose((1, 0)), draw)\n",
        "    fencer_data.append(fencer_data_temp)\n",
        "  return (image, fencer_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUJWuA_v7M8A",
        "cellView": "code"
      },
      "source": [
        "#@title Fencer_Data_Compact\n",
        "def fencer_data_compact(fencer_data):\n",
        "  # Abbreviates Fencer Data to Relevant Data\n",
        "  # Condenses fencer_data to (Left, Right) (Weapon Hand, Front Knee, Shoulder Center)\n",
        "\n",
        "  fencer_data_compact = []\n",
        "  fencer_wrist = []\n",
        "  fencer_knee = []\n",
        "  shoulder_center = []\n",
        "  keypoints = [fencer_wrist,fencer_knee,shoulder_center]\n",
        "\n",
        "  #Sorts the 4 datapoints with respect to x and returns the center two points\n",
        "  fencer_wrist.append(fencer_data[0][0])\n",
        "  fencer_wrist.append(fencer_data[0][1])\n",
        "  fencer_wrist.append(fencer_data[1][0])\n",
        "  fencer_wrist.append(fencer_data[1][1])\n",
        "  fencer_wrist = sorted(fencer_wrist, key = lambda x: x[0])\n",
        "  fencer_data_compact.append(fencer_wrist[1:3])\n",
        "\n",
        "  fencer_knee.append(fencer_data[0][2])\n",
        "  fencer_knee.append(fencer_data[0][3])\n",
        "  fencer_knee.append(fencer_data[1][2])\n",
        "  fencer_knee.append(fencer_data[1][3])\n",
        "  fencer_knee = sorted(fencer_knee, key = lambda x: x[0])\n",
        "  fencer_data_compact.append(fencer_knee[1:3])\n",
        "\n",
        "  for i in range(2):\n",
        "    shoulder_temp = []\n",
        "    for j in range(len(fencer_data[i][4])):\n",
        "      shoulder_temp.append(int((fencer_data[i][4][j] + fencer_data[i][5][j])/2))    \n",
        "    shoulder_center.append(shoulder_temp)\n",
        "  shoulder_center[0], shoulder_center[1] = shoulder_center[1], shoulder_center[0]\n",
        "  fencer_data_compact.append(shoulder_center)\n",
        "\n",
        "  return (fencer_data_compact)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vIYzqkq7lyM",
        "cellView": "code"
      },
      "source": [
        "#@title Human_Pose_Analysis\n",
        "def human_pose_analysis(frame):\n",
        "  # image = PIL.Image.open(file_name)\n",
        "  image = frame\n",
        "  image_tensor = torchvision.transforms.functional.to_tensor(image).cuda()\n",
        "  output = model([image_tensor])[0]\n",
        "\n",
        "  result_image = np.array(image.copy())\n",
        "\n",
        "  #Uses only six keypoints\n",
        "  [result_image, fencer_data] = overlay_keypoints(result_image, output['keypoints'][:6], output['keypoints_scores'][:6], True)\n",
        "\n",
        "  keypoints = [output['keypoints'][:2], output['keypoints_scores'][:2]]\n",
        "\n",
        "  return (fencer_data, keypoints)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYfwhh3UFtCT",
        "cellView": "code"
      },
      "source": [
        "#@title Fencer_Data_Verification\n",
        "def fencer_data_verification(Left_Torso_Position, left_torso_size_average, Right_Torso_Position, right_torso_size_average, fencer_data, frame):\n",
        "  #Tests that the fencer_pose_data is near the torso of the fencer\n",
        "  #Left and Right Torso positions are single x,y values in this function\n",
        "  #Format fencer_data to (Left, Right) (Weapon Hand, Front Knee, Shoulder Center)\n",
        "    #[[wristLx, wristLy, wristLconf][wristRx, wristRy, wristRconf]],[[kneeLx, kneeLy, kneeLconf][kneeRx, kneeRy, kneeRconf]],[[shldrLx, shldrLy, shldrLconf][shldrRx, shldrRy, shldrRconf]]\n",
        "  #Format torso_size_average [width, height]\n",
        "\n",
        "  fencer_data_pose_left = []\n",
        "  fencer_data_pose_right = []\n",
        "\n",
        "  for i in range(len(fencer_data)):\n",
        "    #Creates the Left Fencer_Data\n",
        "    lx_min = min(fencer_data[i][4][0],fencer_data[i][5][0]) - left_torso_size_average[1]/4\n",
        "    lx_max = max(fencer_data[i][4][0],fencer_data[i][5][0]) + left_torso_size_average[1]/4\n",
        "    ly_min = min(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    ly_max = max(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    torso_l_x = Left_Torso_Position[0]\n",
        "    torso_l_y_bottom = Left_Torso_Position[1] + left_torso_size_average[1]/2\n",
        "    torso_l_y_top = Left_Torso_Position[1] - left_torso_size_average[1]/2\n",
        "    #Checks if the torso is between the shoulders, that the shoulders and within the torso_box and that the left pose is empty\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The left fencer data verification bounding box is:')\n",
        "      display(f'{lx_min} to {lx_max} in the x direction and {torso_l_y_bottom} to {torso_l_y_top}')\n",
        "      display(f'The center points are: {torso_l_x} for x and {ly_min},{ly_max} for y.')\n",
        "\n",
        "    if torso_l_x > lx_min and torso_l_x < lx_max and torso_l_y_top < ly_min and torso_l_y_bottom > ly_max and fencer_data_pose_left == []:\n",
        "      #Checks for which wrist and knee is forward\n",
        "      if fencer_data[i][0][0] > fencer_data[i][1][0]:\n",
        "        fencer_data_pose_left.append(fencer_data[i][0])\n",
        "      else:\n",
        "        fencer_data_pose_left.append(fencer_data[i][1])\n",
        "      if fencer_data[i][2][0] > fencer_data[i][3][0]:\n",
        "        fencer_data_pose_left.append(fencer_data[i][2])\n",
        "      else:\n",
        "        fencer_data_pose_left.append(fencer_data[i][3])\n",
        "      fencer_data_shldr_temp = []\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][0] + fencer_data[i][5][0])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][1] + fencer_data[i][5][1])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][2] + fencer_data[i][5][2])/2))\n",
        "      fencer_data_pose_left.append(fencer_data_shldr_temp)\n",
        "      \n",
        "  for i in range(len(fencer_data)):\n",
        "    #Creates the Right Fencer_Data\n",
        "    rx_min = min(fencer_data[i][4][0],fencer_data[i][5][0]) - left_torso_size_average[1]/4\n",
        "    rx_max = max(fencer_data[i][4][0],fencer_data[i][5][0]) + left_torso_size_average[1]/4\n",
        "    ry_min = min(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    ry_max = max(fencer_data[i][4][1],fencer_data[i][5][1])\n",
        "    torso_r_x = Right_Torso_Position[0]\n",
        "    torso_r_y_bottom = Right_Torso_Position[1] + right_torso_size_average[1]/2\n",
        "    torso_r_y_top = Right_Torso_Position[1] - right_torso_size_average[1]/2\n",
        "    if torso_r_x > rx_min and torso_r_x < rx_max and torso_r_y_top < ry_min and torso_r_y_bottom > ry_max and fencer_data_pose_right == []:\n",
        "      #Checks for which wrist is forward\n",
        "      if fencer_data[i][0][0] > fencer_data[i][1][0]:\n",
        "        fencer_data_pose_right.append(fencer_data[i][1])\n",
        "      else:\n",
        "        fencer_data_pose_right.append(fencer_data[i][0])\n",
        "      #Checks for which knee is forward\n",
        "      if fencer_data[i][2][0] > fencer_data[i][3][0]:\n",
        "        fencer_data_pose_right.append(fencer_data[i][3])\n",
        "      else:\n",
        "        fencer_data_pose_right.append(fencer_data[i][2])\n",
        "      fencer_data_shldr_temp = []\n",
        "      # Averages the Shoulder data\n",
        "      fencer_data_shldr_temp = []\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][0] + fencer_data[i][5][0])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][1] + fencer_data[i][5][1])/2))\n",
        "      fencer_data_shldr_temp.append(int((fencer_data[i][4][2] + fencer_data[i][5][2])/2))\n",
        "      fencer_data_pose_right.append(fencer_data_shldr_temp)\n",
        "\n",
        "  # Condenses the fencer data to only relevant data\n",
        "  fencer_data = [fencer_data_pose_left, fencer_data_pose_right]\n",
        "\n",
        "  # If no pose is found, then it is set to zeros\n",
        "  if fencer_data[0] == []:\n",
        "    fencer_data[0] = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "  if fencer_data[1] == []:\n",
        "    fencer_data[1] = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The compact fencer data from verification frame {frame - 1} is:')\n",
        "    display(fencer_data)\n",
        "\n",
        "  return (fencer_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttScHtsXdr7U",
        "cellView": "code"
      },
      "source": [
        "#@title Hour Minute Second String\n",
        "def hms_string(sec_elapsed):\n",
        "  # Nicely formatted time string\n",
        "  h = int(sec_elapsed / (60 * 60))\n",
        "  m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "  s = int(sec_elapsed % 60)\n",
        "  ms = int(sec_elapsed * 1000 % 1000)\n",
        "  return f\"{h}:{m:>02}:{s:>02}:{ms:>03}\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk1r3x84LJiT",
        "cellView": "code"
      },
      "source": [
        "#@title Position_Linear_Approximation\n",
        "def position_linear_approximation(position, previous_certainty):\n",
        "  # Certainty is the number of times previous to current position that a point was not certain.\n",
        "  last_known_position = ((previous_certainty+2)*(-1))\n",
        "\n",
        "  # Finds the positional distance between two known boxes\n",
        "  x_delta = int((position[-1][0] - position[last_known_position][0])/(last_known_position+1))\n",
        "  y_delta = int((position[-1][1] - position[last_known_position][1])/(last_known_position+1))\n",
        "  delta = [x_delta, y_delta]\n",
        "\n",
        "  # Adjusts the previous positions, up to the previous certainty, based on a linear approximation\n",
        "  for j in range(2):\n",
        "    for i in range(previous_certainty+1):\n",
        "      position[i - (previous_certainty+1)][j] = position[i - (previous_certainty+2)][j] - delta[j]\n",
        "\n",
        "  return (position)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfH-PgbIdu4F",
        "cellView": "code"
      },
      "source": [
        "#@title Scoring_Box_Lights\n",
        "def scoring_box_lights(img, Scoring_Box_Position, scoring_box_size_average, default_color, frame, score_box_empty):\n",
        "\n",
        "  # A high max distance is less sensitive and a lower max distance is more sensitive\n",
        "  max_distance_total = 200\n",
        "  max_distance_specific_color = 100\n",
        "\n",
        "  # Defines the region of the top_left position of a 5x3 grid of the score_box, [xmin,ymin,xmax,ymax]\n",
        "  # Extends the Light Search Position outside of the detected box\n",
        "  xmin = Scoring_Box_Position[0] - int(scoring_box_size_average[0]/2) - int(scoring_box_size_average[0]/8)\n",
        "  xmax = Scoring_Box_Position[0] - int(scoring_box_size_average[0]/2) + int(scoring_box_size_average[0]/4)\n",
        "  ymin = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2)\n",
        "  ymax = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2) + int(scoring_box_size_average[1]/3)\n",
        "  left_light_position = [xmin, xmax, ymin, ymax]\n",
        "\n",
        "  # Defines the region of the top_right position of a 5x3 grid of the score_box, [xmin,ymin,xmax,ymax]\n",
        "  xmin = Scoring_Box_Position[0] + int(scoring_box_size_average[0]/2) - int(scoring_box_size_average[0]/4)\n",
        "  xmax = Scoring_Box_Position[0] + int(scoring_box_size_average[0]/2) + int(scoring_box_size_average[0]/8)\n",
        "  ymin = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2)\n",
        "  ymax = Scoring_Box_Position[1] - int(scoring_box_size_average[1]/2) + int(scoring_box_size_average[1]/3)\n",
        "  right_light_position = [xmin, xmax, ymin, ymax]\n",
        "\n",
        "  if default_color != []:\n",
        "    distance_temp, distance_specific_color_temp = [], []\n",
        "\n",
        "    width = left_light_position[1]-left_light_position[0]\n",
        "    height = left_light_position[3]-left_light_position[2]\n",
        "\n",
        "    #i is the x value of the image for the Left Side/Red\n",
        "    for i in range(width):\n",
        "      #j is y value of the image\n",
        "      for j in range(height):\n",
        "        #color channel of the image [B,G,R]\n",
        "        #image, img, is of format [y,x]\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = (img[pixel_position_y, pixel_position_x, 0] - default_color[0])\n",
        "        g = (img[pixel_position_y, pixel_position_x, 1] - default_color[1])\n",
        "        r = (img[pixel_position_y, pixel_position_x, 2] - default_color[2])\n",
        "        distance_temp.append(int((b**2 + g**2 + r**2)**(0.5)))\n",
        "        distance_specific_color_temp.append(abs(r))\n",
        "\n",
        "    #Sorts the distances and keeps the top quarter then finds the average\n",
        "    distance_temp.sort()\n",
        "    distance_temp = distance_temp[(int(len(distance_temp)/4)*-1):]\n",
        "    distance = int(sum(distance_temp)/len(distance_temp))\n",
        "    distance_specific_color_temp.sort()\n",
        "    distance_specific_color_temp = distance_specific_color_temp[(int(len(distance_specific_color_temp)/4)*-1):]\n",
        "    distance_specific_color = int(sum(distance_specific_color_temp)/len(distance_specific_color_temp))\n",
        "\n",
        "    #0 is no color change from the default color)\n",
        "    if distance > max_distance_total and distance_specific_color > max_distance_specific_color and score_box_empty == False:\n",
        "      left_light_comparison = 1\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      left_light_comparison = 0\n",
        "\n",
        "    #Resets b,g,r for the Right Side\n",
        "    distance_temp, distance_specific_color_temp= [], []\n",
        "    width = right_light_position[1]-right_light_position[0]\n",
        "    height = right_light_position[3]-right_light_position[2]\n",
        "\n",
        "    #i is the x value of the image\n",
        "    for i in range(width):\n",
        "      #j is y value of the image\n",
        "      for j in range(height):\n",
        "        #kcolor channel of the image [B,G,R]\n",
        "\n",
        "        # pixel_position = right_light_position[2] + j,right_light_position[0] + i\n",
        "        pixel_position_y = right_light_position[2] + j\n",
        "        pixel_position_x = right_light_position[0] + i\n",
        "        b = (img[pixel_position_y, pixel_position_x, 0] - default_color[0])\n",
        "        g = (img[pixel_position_y, pixel_position_x, 1] - default_color[1])\n",
        "        r = (img[pixel_position_y, pixel_position_x, 2] - default_color[2])\n",
        "        distance_temp.append(int((b**2 + g**2 + r**2)**(0.5)))\n",
        "        distance_specific_color_temp.append(abs(g))\n",
        "\n",
        "    #Sorts the distances and keeps the top sixth then finds the average\n",
        "    distance_temp.sort()\n",
        "    distance_temp = distance_temp[(int(len(distance_temp)/6)*-1):]\n",
        "    distance = int(sum(distance_temp)/len(distance_temp))\n",
        "    distance_specific_color_temp.sort()\n",
        "    distance_specific_color_temp = distance_specific_color_temp[(int(len(distance_specific_color_temp)/4)*-1):]\n",
        "    distance_specific_color = int(sum(distance_specific_color_temp)/len(distance_specific_color_temp))\n",
        "\n",
        "    #0 is no color change from the default color)\n",
        "    if (distance > max_distance_total and distance_specific_color > max_distance_specific_color):\n",
        "      right_light_comparison = 1\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      right_light_comparison = 0\n",
        "\n",
        "  #Finds the Defualt Color\n",
        "  else:\n",
        "    b, g, r = 0, 0, 0\n",
        "    # Cycles through the Left and Right Light Positions to determine a default color for the frame\n",
        "    width = left_light_position[1]-left_light_position[0]\n",
        "    height = left_light_position[3]-left_light_position[2]\n",
        "    for i in range(width):\n",
        "      for j in range(height):\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = b + img[pixel_position_y, pixel_position_x, 0]\n",
        "        g = g + img[pixel_position_y, pixel_position_x, 1]\n",
        "        r = r + img[pixel_position_y, pixel_position_x, 2]\n",
        "        default_color_left_temp = [int(b/(width*height)),int(g/(width*height)),int(r/(width*height))]\n",
        "    width = right_light_position[1]-right_light_position[0]\n",
        "    height = right_light_position[3]-right_light_position[2]\n",
        "    for i in range(width):\n",
        "      for j in range(height):\n",
        "        # pixel_position = right_light_position[2] + j,right_light_position[0] + i\n",
        "        pixel_position_y = left_light_position[2] + j\n",
        "        pixel_position_x = left_light_position[0] + i\n",
        "        b = b + img[pixel_position_y, pixel_position_x, 0]\n",
        "        g = g + img[pixel_position_y, pixel_position_x, 1]\n",
        "        r = r + img[pixel_position_y, pixel_position_x, 2]\n",
        "        default_color_right_temp = [int(b/(width*height)),int(g/(width*height)),int(r/(width*height))]\n",
        "    #Combines the Left and Right Default Colors for B,G,R\n",
        "    for i in range(3):\n",
        "      default_color.append((default_color_left_temp[i] + default_color_right_temp[i])/2)\n",
        "\n",
        "    # Assumes that the lights are off during the engarde phase.\n",
        "    left_light_comparison = 0\n",
        "    right_light_comparison = 0\n",
        "\n",
        "  return (left_light_comparison, right_light_comparison, default_color)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz4y3kZgPX58",
        "cellView": "code"
      },
      "source": [
        "#@title Motion_Difference_Tracking\n",
        "def motion_difference_tracking(frame, side, Bounding_Box, width, height, kernel_scaling, erosion_iterations, dilation_iterations):\n",
        "\n",
        "  # Ensures Bounding_Box is not negative\n",
        "  for i in range(len(Bounding_Box)):\n",
        "    if Bounding_Box[i] < 0:\n",
        "      Bounding_Box[i] = 0\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The original difference tracking bounding box at frame {frame - 1} is:')\n",
        "    display(Bounding_Box)\n",
        "\n",
        "  # Requires the Bounding Box to have a width and be on the image\n",
        "  if Bounding_Box[1] - Bounding_Box[0] != 0 and Bounding_Box[0] < width and Bounding_Box[0] > 0:\n",
        "    Position_y_Orig = int((Bounding_Box[3]+Bounding_Box[2])/2)\n",
        "\n",
        "    # Uses the original frames to avoid Region of Interest Boxes\n",
        "    save_path = r'/content/Mask_RCNN/videos/original/'\n",
        "    image_num = frame\n",
        "    image_name2 = str(image_num-1) + '.jpg'\n",
        "    image_name1 = str(image_num-2) + '.jpg'\n",
        "    file_name1 = os.path.join(save_path, image_name1)\n",
        "    file_name2 = os.path.join(save_path, image_name2)\n",
        "\n",
        "    # Reads the images\n",
        "    image1 = cv2.imread(file_name1)\n",
        "    image2 = cv2.imread(file_name2)\n",
        "\n",
        "    # Convert to Grayscale\n",
        "    image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "    image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
        "    image_diff = cv2.absdiff(image1_gray,image2_gray)\n",
        "\n",
        "    # Creates a Cropped Image\n",
        "    crop_img = image_diff[Bounding_Box[2]:Bounding_Box[3], Bounding_Box[0]:Bounding_Box[1]]\n",
        "\n",
        "    # Kernel is affected by Kernel Scaling which gets finer if it initially fails\n",
        "    kernel_number = int(width/(100*kernel_scaling))\n",
        "    \n",
        "    # Ensures that the kernel is odd\n",
        "    if kernel_number%2 == 0:\n",
        "      kernel_number = kernel_number + 1\n",
        "    kernel = np.ones((kernel_number,kernel_number),np.uint8)\n",
        "    \n",
        "    # Errodes\n",
        "    erosion = cv2.erode(crop_img,kernel,iterations = erosion_iterations)\n",
        "\n",
        "    # Dilates\n",
        "    dilation = cv2.dilate(erosion,kernel,iterations = dilation_iterations)\n",
        "\n",
        "    # Blurs Image\n",
        "    blur = cv2.GaussianBlur(dilation,kernel.shape,0)\n",
        "\n",
        "    # Threshold\n",
        "    ret,thresh = cv2.threshold(blur,0,90,cv2.THRESH_BINARY)\n",
        "\n",
        "    # Find contours\n",
        "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "    \n",
        "    if cnts != []:\n",
        "      c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "      left = tuple(c[c[:, :, 0].argmin()][0])\n",
        "      right = tuple(c[c[:, :, 0].argmax()][0])\n",
        "\n",
        "      if verbose == True:\n",
        "        display(f'Left/Right is {left[0]}/{right[0]}.')\n",
        "        display(f'The int(left[0]) is {int(left[0])} and of type {type(int(left[0]))} while left[0] is {type(left[0])}.')\n",
        "        display(f'Bounding_Box[0] is {Bounding_Box[0]} and is of {type(Bounding_Box[0])}.')\n",
        "\n",
        "      if side == 'Left':\n",
        "        # Obtain outer left coordinate of the contour\n",
        "        # right = tuple(c[c[:, :, 0].argmin()][0])\n",
        "        position = [int(right[0]) + Bounding_Box[0], Position_y_Orig]\n",
        "      elif side == 'Right':\n",
        "        # left = tuple(c[c[:, :, 0].argmax()][0])\n",
        "        position = [int(left[0]) + Bounding_Box[0], Position_y_Orig]\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'Side is not given')\n",
        "    \n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'There is no data from difference imaging on the {side} side.')\n",
        "      position = 'None'\n",
        "  # Error occurs if the entire image is erroded\n",
        "    # except:\n",
        "    #   display(f'There is no data from difference imaging on the {side} side.')\n",
        "    #   position = 'None'\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The kernel number for frame {frame} is {kernel_number}, the number of errosions/dilations are {erosion_iterations}/{dilation_iterations}.')\n",
        "    if cnts != []:\n",
        "      if verbose == True:\n",
        "        display(f'The resulting position is {position} and the boundary box is {Bounding_Box}. The Left/Right limits of the contour are {int(left[0]) + Bounding_Box[0]}/{int(right[0]) + Bounding_Box[0]}.')\n",
        "  else:\n",
        "    position = 'None'\n",
        "    if verbose == True:\n",
        "      display(f'The bounding box given had a width of zero.')\n",
        "\n",
        "  return(position)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_koxlAByNn7W",
        "cellView": "code"
      },
      "source": [
        "#@title Saturation_Test\n",
        "def saturation_test(box, frame):\n",
        "  # Test is a True/False return\n",
        "  # Takes an image and tests it for the expected saturation\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "  file_name = str(frame) + '.jpg'\n",
        "  name = os.path.join(path, file_name)\n",
        "  img = cv2.imread(name)\n",
        "  # Converts from BGR to HSV\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "  \n",
        "  # Tests Bellguard\n",
        "  if box[2] == 1:\n",
        "    blue_range = [50, 150]\n",
        "    green_range = [50, 150]\n",
        "    red_range = [50, 160]\n",
        "    max_delta = 25\n",
        "    # saturation_range = [0, 20]\n",
        "    saturation_range = [0, 70]\n",
        "    object_tested = 'Bellguard'\n",
        "  # Tests Torso\n",
        "  elif box[2] == 3:\n",
        "    blue_range = [60, 150]\n",
        "    green_range = [60, 150]\n",
        "    red_range = [60, 160]\n",
        "    max_delta = 30\n",
        "    saturation_range = [0, 20]\n",
        "    object_tested = 'Torso'\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'The object to test does not have a color/saturation profile.')\n",
        "\n",
        "  width = (box[0][3]-box[0][1])\n",
        "  height = (box[0][2]-box[0][0])\n",
        "\n",
        "  s_temp = []\n",
        "\n",
        "  #i is the x value of the image\n",
        "  for i in range(width):\n",
        "    #j is y value of the image\n",
        "    for j in range(height):\n",
        "      s = img[box[0][0] + j, box[0][1] + i, 1]\n",
        "      s_temp.append(s)\n",
        "\n",
        "    #Sorts the distances and keeps the top quarter then finds the average\n",
        "    s_temp.sort()\n",
        "    #Truncates to the least saturated/most gray values\n",
        "    s_temp = s_temp[:(int(len(s_temp)/2)*-1)]\n",
        "    s_temp = s_temp[:(int(len(s_temp)*3/4)*-1)]\n",
        "    #Averages the saturation values\n",
        "    s_average = int(sum(s_temp)/len(s_temp))\n",
        "\n",
        "  if s_average < saturation_range[1]:\n",
        "    test_result = True\n",
        "  else:\n",
        "    test_result = False\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The test result for the {object_tested} saturation is {test_result} with a saturation of {s_average}.')\n",
        "\n",
        "  return (test_result)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7TbVcas8QGm"
      },
      "source": [
        "def exclusion_area_simplification(a, max_dist):\n",
        "  a_simplified = []\n",
        "  for i in range(len(a)):\n",
        "    for j in range(len(a) - (i+1)):\n",
        "      # a[i] - a[(i+1)+j]\n",
        "      if a[i] != 'skip' and a[(i+1)+j] != 'skip':\n",
        "        dist = int(((a[i][0] - a[(i+1)+j][0])**2 + (a[i][1] - a[(i+1)+j][1])**2)**(0.5))\n",
        "        # display(f'The distance between {a[i]} and {a[(i+1)+j]} is {dist}.')\n",
        "        if dist < max_dist:\n",
        "          a_simplified.append(a[i])\n",
        "          a[i] = 'skip'\n",
        "          a[(i+1)+j] = 'skip'\n",
        "\n",
        "  for k in range(len(a)):\n",
        "    if a[k] != 'skip':\n",
        "      a_simplified.append(a[k])\n",
        "\n",
        "  return(a_simplified)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JUqi_WR8RMk"
      },
      "source": [
        "def exclusion_area_simplification_recursion(a, max_dist):\n",
        "  a_simplified = exclusion_area_simplification(a, max_dist)\n",
        "  a_simplified_temp = 0\n",
        "\n",
        "  while a_simplified != a_simplified_temp:\n",
        "    if verbose == True:\n",
        "      display(a_simplified)\n",
        "    a_simplified_temp = exclusion_area_simplification(a_simplified, max_dist)\n",
        "    if a_simplified_temp != a_simplified:\n",
        "      a_simplified = a_simplified_temp\n",
        "      a_simplified_temp = 0\n",
        "\n",
        "  if a_simplified_temp == a_simplified:\n",
        "    if verbose == True:\n",
        "      display(f'The two are equal.')\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'The two are NOT equal.')\n",
        "\n",
        "  return(a_simplified)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNPGfzrydJ9M",
        "cellView": "code"
      },
      "source": [
        "#@title Box_Size_Finder\n",
        "def box_size_finder(bbox, capture_width, capture_height, object_to_size):\n",
        "\n",
        "  Box_Size = [[],[]]\n",
        "  sum_of_boxes = [[],[]]\n",
        "  frame_multiplier = 1\n",
        "\n",
        "  if object_to_size == 'score_box':\n",
        "    x_min = int(capture_width/4)\n",
        "    x_max = int(capture_width*3/4)\n",
        "    bbox_category = 2\n",
        "  elif object_to_size == 'left':\n",
        "    x_min = 0\n",
        "    x_max = int(capture_width/2)\n",
        "    bbox_category = 3\n",
        "  elif object_to_size == 'right':\n",
        "    x_min = int(capture_width/2)\n",
        "    x_max = int(capture_width)\n",
        "    bbox_category = 3\n",
        "\n",
        "  # i represents the frame, minimum of 50 frames or len(bbox)\n",
        "  for i in range(min(50*frame_multiplier, len(bbox))):\n",
        "    # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "    for j in range(len(bbox[i])):\n",
        "      if (bbox[i][j][1] > 0.90 and bbox[i][j][0][1] > x_min and bbox[i][j][0][1] < x_max and bbox[i][j][2] == bbox_category):\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[0].append(bbox[i][j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[1].append(bbox[i][j][0][0])  \n",
        "        #Appends x width value:\n",
        "        Box_Size[0].append(bbox[i][j][0][3] - bbox[i][j][0][1])\n",
        "        #Appends y width value:\n",
        "        Box_Size[1].append(bbox[i][j][0][2] - bbox[i][j][0][0])\n",
        "\n",
        "  x_average = average_list(sum_of_boxes[0])\n",
        "  y_average = average_list(sum_of_boxes[1])\n",
        "\n",
        "  # scoring_box_size_average [Width, Height]\n",
        "  box_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  box_size_average.append(int(average_list(Box_Size[0])))\n",
        "  # Appends the average scoring box height\n",
        "  box_size_average.append(int(average_list(Box_Size[1])))\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Average Box Size for {object_to_size} is {box_size_average}')\n",
        "\n",
        "  return (box_size_average)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q00swZNxhP5",
        "cellView": "code"
      },
      "source": [
        "#@title Tracking_Box_Default\n",
        "def tracking_box_default(Left, Right, Score_Box, x_padding, y_padding, engarde_length):\n",
        "  # Creates a default tracking box\n",
        "\n",
        "  Tracking_Bounding_Boxes_Temp = [[],[],[]]\n",
        "  Tracking_Bounding_Boxes = []\n",
        "\n",
        "  for i in range(engarde_length):\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(Left[1] + y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(Right[1] + y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[0] - x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[0] + x_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[1] - y_padding)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(Score_Box[1]+ y_padding)\n",
        "\n",
        "    Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "\n",
        "  return (Tracking_Bounding_Boxes)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy0ESby5beUc",
        "cellView": "code"
      },
      "source": [
        "#@title Bell_Guard_Position_Finding\n",
        "def Bell_Guard_Position_Finding(bbox, capture_width, capture_height, fencer_data, positions, frame_count, left_torso_size_average, right_torso_size_average, engarde_length, previous_certainty, camera_steady, camera_motion_threshold, Exclusion_Areas):\n",
        "  # Format positions = [Left_Position, Right_Position, Score_Box_Position, Left_Torso_Position, Right_Torso_Position]\n",
        "\n",
        "  x_min = []\n",
        "  x_max = []\n",
        "  y_min = []\n",
        "  y_max = []\n",
        "\n",
        "  Left_Position = positions[0]\n",
        "  Right_Position = positions[1]\n",
        "  Scoring_Box_Position = positions[2]\n",
        "  Left_Torso_Position = positions[3]\n",
        "  Right_Torso_Position = positions[4]\n",
        "\n",
        "  # Any of the First engarde_length position can be used since the engarde position is an averaged constant\n",
        "  # Certainty is used here as a counter for how many times a bounding box does not fall in the tracking box\n",
        "  # And increases the size of the bounding box based on each miss\n",
        "\n",
        "  certainty = [0,0,0,0,0]\n",
        "  if verbose == True:\n",
        "    display(f'Previous Certainty at frame {frame_count - 1} is {previous_certainty}.')\n",
        "\n",
        "  #Establishes Previous Positions to determine speed and expected positions\n",
        "  previous_position_Left = Left_Position[-1]\n",
        "  twice_previous_position_Left = Left_Position[-2]\n",
        "  previous_position_Right = Right_Position[-1]\n",
        "  twice_previous_position_Right = Right_Position[-2]\n",
        "  previous_position_Scoring_Box = Scoring_Box_Position[-1]\n",
        "  twice_previous_position_Scoring_Box = Scoring_Box_Position[-2]\n",
        "  previous_position_Left_Torso = Left_Torso_Position[-1]\n",
        "  twice_previous_position_Left_Torso = Left_Torso_Position[-2]\n",
        "  previous_position_Right_Torso = Right_Torso_Position[-1]\n",
        "  twice_previous_position_Right_Torso = Right_Torso_Position[-2]\n",
        "\n",
        "  #Boxes are the bounding boxes for the current frame, passes less data to tracking function\n",
        "  boxes = bbox\n",
        "\n",
        "  Tracking_Bounding_Boxes_Temp = [[],[],[],[],[]]\n",
        "\n",
        "  # Torso Positions are calculated prior to the BellGuard because they are an input to the bellguard position\n",
        "\n",
        "  # Bellguard Position Tracking focuses on Tracking as opposed to detection\n",
        "  # Left_Torso Position\n",
        "  [current_position, certainty[3], Tracking_Bounding_Boxes_Left_Torso] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left_Torso, \\\n",
        "    twice_previous_position_Left_Torso, previous_certainty[3], 'Left_Torso', \\\n",
        "    frame_count, 'None', left_torso_size_average, capture_width, capture_height, \\\n",
        "    'None', engarde_length, camera_steady, camera_motion_threshold, 'None')\n",
        "  Tracking_Bounding_Boxes_Temp[3] = Tracking_Bounding_Boxes_Left_Torso\n",
        "  Left_Torso_Position = current_position\n",
        "\n",
        "  # Right_Torso Position\n",
        "  [current_position, certainty[4], Tracking_Bounding_Boxes_Right_Torso] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Right_Torso, \\\n",
        "    twice_previous_position_Right_Torso, previous_certainty[4], \"Right_Torso\", \\\n",
        "    frame_count, 'None', right_torso_size_average, capture_width, capture_height, \\\n",
        "    'None', engarde_length, camera_steady, camera_motion_threshold, 'None')\n",
        "  Tracking_Bounding_Boxes_Temp[4] = Tracking_Bounding_Boxes_Right_Torso\n",
        "  Right_Torso_Position = current_position\n",
        "\n",
        "  fencer_data = fencer_data_verification(Left_Torso_Position, left_torso_size_average, Right_Torso_Position, right_torso_size_average, \\\n",
        "    fencer_data, frame_count)\n",
        "\n",
        "  # Left Position\n",
        "  [current_position, certainty[0], Tracking_Bounding_Boxes_Left] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Left, \\\n",
        "    twice_previous_position_Left, previous_certainty[0], 'Left_BellGuard', \\\n",
        "    frame_count, Left_Torso_Position, left_torso_size_average, capture_width, \\\n",
        "    capture_height, fencer_data, engarde_length, camera_steady, camera_motion_threshold, Exclusion_Areas)\n",
        "  Tracking_Bounding_Boxes_Temp[0] = Tracking_Bounding_Boxes_Left\n",
        "  Left_Position = current_position\n",
        "\n",
        "  #  Right Position\n",
        "  [current_position, certainty[1], Tracking_Bounding_Boxes_Right] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Right, \\\n",
        "    twice_previous_position_Right, previous_certainty[1], 'Right_BellGuard', \\\n",
        "    frame_count, Right_Torso_Position, right_torso_size_average, capture_width, \\\n",
        "    capture_height, fencer_data, engarde_length, camera_steady, camera_motion_threshold, Exclusion_Areas)\n",
        "  Tracking_Bounding_Boxes_Temp[1] = Tracking_Bounding_Boxes_Right\n",
        "  Right_Position = current_position\n",
        "\n",
        "  # Scoring_Box Position\n",
        "  [current_position, certainty[2], Tracking_Bounding_Boxes_Scoring_Box] = \\\n",
        "    Bell_Guard_Position_Tracking(boxes, previous_position_Scoring_Box, \\\n",
        "    twice_previous_position_Scoring_Box, previous_certainty[2], 'Scoring_Box', \\\n",
        "    frame_count, 'None', left_torso_size_average, capture_width, capture_height, \\\n",
        "    'None', engarde_length, camera_steady, camera_motion_threshold, 'None')\n",
        "  Tracking_Bounding_Boxes_Temp[2] = Tracking_Bounding_Boxes_Scoring_Box\n",
        "  Scoring_Box_Position = current_position\n",
        "\n",
        "  Tracking_Bounding_Boxes = Tracking_Bounding_Boxes_Temp\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Length of the Left and Right Positions after the Position Finding are: {len(Left_Position)} and {len(Right_Position)}.')\n",
        "    display(f'At frame {frame_count} the certainty and previous certainty before linear approx analysis is:')\n",
        "    display(f'{certainty} and {previous_certainty}')\n",
        "\n",
        "  return (Left_Position, Right_Position, Scoring_Box_Position, Tracking_Bounding_Boxes, Left_Torso_Position, Right_Torso_Position, engarde_length, certainty)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY9eLcBIMerm",
        "cellView": "code"
      },
      "source": [
        "#@title Bell_Guard_Position_Tracking\n",
        "def Bell_Guard_Position_Tracking(boxes, previous_position, twice_previous_position, certainty, tracked_item, frame, Torso_Position, Torso_Size, capture_width, capture_height, fencer_data, engarde_length, camera_steady, camera_motion_threshold, Exclusion_Areas):\n",
        "  # Tracks the position of items\n",
        "  # tracked_item is needed since boxes only has the class of the item tracked, not the Left or Right\n",
        "  # tracked_item Format: [0,1,2,3] = [Background, Bell_Guard, Score_Box, Torso]\n",
        "\n",
        "  #Assumed inherent uncertainty\n",
        "  certainty_default = int(capture_width/12)\n",
        "  certainty_multiplier = int(capture_width/80)\n",
        "\n",
        "  #Reduces the max value of y as compared to x\n",
        "  y_limiter = 24\n",
        "\n",
        "  boxes_temp = []\n",
        "  #Filters out potential boxes based on Tracked Item, Confidence and Saturation of the Box\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard'):\n",
        "\n",
        "    #Uses only the Fencer_Data and Certainty for the Appropriate side.\n",
        "    if tracked_item == 'Left_BellGuard':\n",
        "      fencer_data = fencer_data[0]\n",
        "      bell_certainty = certainty\n",
        "    else:\n",
        "      #Assumes the Right BellGuard\n",
        "      fencer_data = fencer_data[1]\n",
        "      bell_certainty = certainty\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The boxes being tested for exclusion are:')\n",
        "      display(boxes)\n",
        "\n",
        "    within_exclusion_box = False\n",
        "    # Only Tests if Bell Guard confidence is not extra very high  \n",
        "    for j in range(len(boxes)):\n",
        "      if boxes[j][1] < bellguard_confidence_extra_very_high and use_Exclusion_Areas == True:\n",
        "        # Tests if the box is in an exlusion area\n",
        "        padding = capture_width/48\n",
        "        exclusion_padding = [padding,padding,padding,padding]\n",
        "        # Cycles through the Exclusion Areas\n",
        "        for k in range(len(Exclusion_Areas)):\n",
        "          if verbose == True:\n",
        "            display(f'Exclusion_Areas[k] is {Exclusion_Areas[k]}.')\n",
        "          Exculsion_Box = create_boundary_box(Exclusion_Areas[k],exclusion_padding, False)\n",
        "          if verbose == True:\n",
        "            display(f'boxes[j][0] is {boxes[j][0]}, boxes[j][1] is {boxes[j][1]}.')\n",
        "          test_point = [int((boxes[j][0][3]+boxes[j][0][1])/2), int((boxes[j][0][2]+boxes[j][0][0])/2)]\n",
        "          if verbose == True:\n",
        "            display(f'The test_point is {test_point}.')\n",
        "          within_tested_box = boundary_box_test(test_point, Exculsion_Box)\n",
        "          if within_tested_box == True:\n",
        "            within_exclusion_box = True\n",
        "            if verbose == True:\n",
        "              display(f'Within the exclusion box is {within_exclusion_box}.')\n",
        "          else:\n",
        "            if verbose == True:\n",
        "              display(f'Within the exclusion box is {within_exclusion_box}.')\n",
        "\n",
        "      # Allows Extra Very High Confidence regardless of exclusion areas\n",
        "      if boxes[j][1] >= bellguard_confidence_extra_very_high:\n",
        "         within_exclusion_box = False\n",
        "\n",
        "      # for k in range(len(boxes)):\n",
        "        #The minimum required certainty for a bellguard box\n",
        "      # Appends the Appropriate boxes to the boxes list to become potential positions \n",
        "      if (boxes[j][2] == 1) and (boxes[j][1] > (bellguard_confidence - bellguard_tracking_det_offset)) and within_exclusion_box == False:\n",
        "        boxes_temp.append(boxes[j])\n",
        "\n",
        "    # for j in range(len(boxes)):\n",
        "    #   #The minimum required certainty for a bellguard box\n",
        "    #   if (boxes[j][2] == 1) and (boxes[j][1] > (bellguard_confidence - bellguard_tracking_det_offset)) and within_exclusion_box == False:\n",
        "    #     boxes_temp.append(boxes[j])\n",
        "\n",
        "  elif (tracked_item == 'Left_Torso' or tracked_item == 'Right_Torso'):\n",
        "    for j in range(len(boxes)):\n",
        "      if ((boxes[j][2] == 3) and (boxes[j][1] > min_torso_confidence)):\n",
        "        #Bypasses the Saturation Test\n",
        "        # test_result = saturation_test(boxes[j], frame)\n",
        "        test_result = True\n",
        "        if test_result == True:\n",
        "          boxes_temp.append(boxes[j])\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'The saturation test failed at frame {frame_count}.')\n",
        "          else:\n",
        "            pass\n",
        "  elif (tracked_item == 'Scoring_Box'):\n",
        "    for j in range(len(boxes)):\n",
        "      if (boxes[j][2] == 2):\n",
        "        boxes_temp.append(boxes[j])\n",
        "\n",
        "  # Assigns boxes_temp to boxes\n",
        "  boxes = boxes_temp\n",
        "\n",
        "  # Creates points at the centers of the bounding boxes that are in this frame\n",
        "  x_center = []\n",
        "  y_center = []\n",
        "  for i in range(len(boxes)):\n",
        "    x_center.append(int((boxes[i][0][1] + boxes[i][0][3])/2))\n",
        "    y_center.append(int((boxes[i][0][0] + boxes[i][0][2])/2))\n",
        "\n",
        "  # Max allowed speed of a bellguard in a single frame\n",
        "  # Accounts for a position jump following the engarde positioning\n",
        "  if frame < engarde_length + 3:\n",
        "    max_speed = int(capture_width/64)\n",
        "  else:\n",
        "    max_speed = int(capture_width/24)\n",
        "\n",
        "  # Converts previous position into a speed\n",
        "  x_pos = int(previous_position[0])\n",
        "  if verbose == True:\n",
        "    display(f'previous_position is {previous_position} and twice_previous_position is {twice_previous_position}.')\n",
        "  # x_speed = int(min(previous_position[0] - twice_previous_position[0], max_speed))\n",
        "\n",
        "  x_speed = previous_position[0] - twice_previous_position[0]\n",
        "  if x_speed > 0:\n",
        "    x_speed = min(x_speed, max_speed)\n",
        "  else:\n",
        "    x_speed = max(x_speed, max_speed*-1)\n",
        "\n",
        "  y_pos = int(previous_position[1])\n",
        "  y_speed = int(min(previous_position[1] - twice_previous_position[1], int(max_speed/y_limiter)))\n",
        "  y_speed = int(max(y_speed, int(max_speed*(-1)/y_limiter)))\n",
        "\n",
        "  if (frame - 1)  == engarde_length and verbose == True:\n",
        "    display(f'THe x_speed is {x_speed} and the y_speed is {y_speed} at the engarde length, frame {frame - 1}.')\n",
        "\n",
        "  # Flips the tracking box to be between the two fencers\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Left_Torso':\n",
        "    horiz_flip = False\n",
        "    if verbose == True:\n",
        "      display(f'The horizontal flip is {horiz_flip} for the {tracked_item} at frame {frame - 1}.')\n",
        "  elif tracked_item == 'Right_BellGuard' or tracked_item == 'Right_Torso':\n",
        "    horiz_flip = True\n",
        "    if verbose == True:\n",
        "      display(f'The horizontal flip is {horiz_flip} for the {tracked_item} at frame {frame - 1}.')\n",
        "  else:\n",
        "    horiz_flip = False\n",
        "\n",
        "\n",
        "  # Defines the tracking box\n",
        "  expected_position = [(x_pos + x_speed),(y_pos + y_speed)]\n",
        "  padding = int(certainty*certainty_multiplier + certainty_default)\n",
        "  # boundary_box_for_tracking = [int(padding*5/8), padding, padding, padding]\n",
        "  boundary_box_for_tracking = [int(padding*7/8), padding, padding, padding]\n",
        "  tracking_box = create_boundary_box(expected_position, boundary_box_for_tracking, horiz_flip)\n",
        "  positions = []\n",
        "\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    # Sets the values for the torso boundary box, limits Bellguard distance from Torso center\n",
        "    boundary_box_for_torso = [int(Torso_Size[0]*0.20), int(Torso_Size[0]*3.25), int(Torso_Size[1]*.75), int(Torso_Size[1]*1.0)]\n",
        "    # Uses the boundary box to create a box based on Left/Right and expected/previous position\n",
        "    torso_box = create_boundary_box(Torso_Position, boundary_box_for_torso, horiz_flip)\n",
        "    # Finds the overlap of multiple boxes to satisy multiple restrictions\n",
        "    [x_min, x_max, y_min, y_max] = boundary_box_overlap(tracking_box, torso_box)\n",
        "    if verbose == True:\n",
        "      display(f'The Torso_Size[0] is {Torso_Size[0]}, the Horizontal Flip is {horiz_flip} and Torso_Position is {Torso_Position}.')\n",
        "  else:\n",
        "    [x_min, x_max, y_min, y_max] = tracking_box\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The tracking box for the {tracked_item} at frame {frame - 1} is: {tracking_box}.')\n",
        "\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard'):\n",
        "    if verbose == True:\n",
        "      display(f'The torso box for the {tracked_item} at frame {frame - 1} is: {torso_box}.')\n",
        "      display(f'The overlapping tracking box for the {tracked_item} at frame {frame - 1} is: {[x_min, x_max, y_min, y_max]}.')\n",
        "\n",
        "  # if (frame - 1) == 49:\n",
        "  #   display(f'The boxes at frame 49 just prior to appending to positions are:')\n",
        "  #   display(boxes)\n",
        "\n",
        "  # Creates a list of positions within the bounding boxes\n",
        "  for i in range(len(boxes)):\n",
        "    center = [x_center[i], y_center[i]]\n",
        "    tracking_result = boundary_box_test(center,tracking_box)\n",
        "    # If the center point is within both boxes for Bellguards or tracking box for other items, then it is appended to positions\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      torso_result = boundary_box_test(center,torso_box)\n",
        "      # if (frame - 1) == 49:\n",
        "      #   display(f'For ({x_center[i]},{y_center[i]}), {boxes[i][1]}%, the tracking_result is {tracking_result} and the torso_result is {torso_result}.')\n",
        "      # Allows for an incorrect engarde position for the bellguard\n",
        "      if (frame - 1) > engarde_length + 3:\n",
        "        if tracking_result == True and torso_result == True:\n",
        "          positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "      else:\n",
        "        # Only the torso results is required for the engarde positioning\n",
        "        if torso_result == True:\n",
        "          positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "    else:\n",
        "      if tracking_result == True:\n",
        "        positions.append([x_center[i],y_center[i], boxes[i][1]])\n",
        "\n",
        "  # Maximum distance only applies if there are multiple bounding boxes within the tracking box\n",
        "  maximum_distance_from_expected = int(capture_width/24)\n",
        "  # Expected Position [x,y], Limits expected position in front of the fencer\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    if verbose == True:\n",
        "      display(f'The expected position is {expected_position} and Torso Position and size is {Torso_Position[0]} and {Torso_Size[0]}.')\n",
        "    if (expected_position[0] > Torso_Position[0] + Torso_Size[0]*2.50) and tracked_item == 'Left_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was too far in front of the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0] + Torso_Size[0]*2.50), y_pos]\n",
        "    if (expected_position[0] < Torso_Position[0]) and tracked_item == 'Left_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was behind the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0]), y_pos]\n",
        "    if expected_position[0] < Torso_Position[0] - Torso_Size[0]*2.50 and tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was too far from the Torso, adjusting expected.')\n",
        "        display(f'Torso_Position[0] is {Torso_Position[0]}, Torso_Size[0] is {Torso_Size[0]}, y_pos is {y_pos}.')\n",
        "      expected_position = [int(Torso_Position[0] - Torso_Size[0]*2.50), y_pos]\n",
        "    if (expected_position[0] > Torso_Position[0]) and tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'At frame {frame - 1} the expected position of the {tracked_item} was behind the Torso, adjusting expected.')\n",
        "      expected_position = [int(Torso_Position[0]), y_pos]\n",
        "\n",
        "  #Assumed maximum distance from wrist to bellguard\n",
        "  wrist_to_bellguard_max = int(Torso_Size[0]/8)\n",
        "\n",
        "  #Sets Initial Conditions for Type of Tracking\n",
        "  using_human_pose = False\n",
        "  using_difference_images = False\n",
        "  using_difference_images_normal_kernel = False\n",
        "  using_expected = False\n",
        "  using_position = False\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The camera steady value for frame {frame - 1} is {camera_steady[frame - 1]}.')\n",
        "    if camera_steady[frame - 1] >= camera_motion_threshold:\n",
        "      if verbose == True:\n",
        "        display(f'The camera is in motion and motion detection is less reliable.')\n",
        "\n",
        "  # Determines the Bellguard Position based on number of detections, confidence, box location and motion\n",
        "  if (len(positions)) == 0:\n",
        "    if verbose == True:\n",
        "      display(f'There where no positions found for the {tracked_item} at frame {frame - 1}.')\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      #Uses a larger boundary box if high confidence in wrist position\n",
        "      if fencer_data[0][2] > wrist_conf_extra_very_high:\n",
        "        if verbose == True:\n",
        "          display(f'The wrist confidence is extra very high, using a larger human pose boundary.')\n",
        "        # Allows for an expanding boundary based on certainty\n",
        "        human_pose_boundary = [int((Torso_Size[0]*4/4)*(1+bell_certainty/4)), int((Torso_Size[0]*2.00)*(1+bell_certainty/2)), int(Torso_Size[0]), int(Torso_Size[0])]\n",
        "      elif fencer_data[0][2] > wrist_conf_very_high and fencer_data[0][2] < wrist_conf_extra_very_high:\n",
        "        if verbose == True:\n",
        "          display(f'The wrist confidence is very high, using a larger human pose boundary.')\n",
        "        # Allows for an expanding boundary based on certainty\n",
        "        human_pose_boundary = [int((Torso_Size[0]*2/3)*(1+bell_certainty/4)), int((Torso_Size[0]*1.75)*(1+bell_certainty/2)), int(Torso_Size[0]), int(Torso_Size[0])] \n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The wrist confidence is not very high, using a smaller human pose boundary.')\n",
        "        human_pose_boundary = [int((Torso_Size[0]*2/4)*(1+bell_certainty/4)), int((Torso_Size[0]*1.25)*(1+bell_certainty/2)), int(Torso_Size[0]*3/4), int(Torso_Size[0]*3/4)]\n",
        "      wrist_position = [fencer_data[0][0], fencer_data[0][1]]\n",
        "      if verbose == True:\n",
        "        display(f'Attempting Human Pose Approximation for the {tracked_item} at frame {frame - 1}.')\n",
        "      if tracked_item == 'Left_BellGuard':\n",
        "        boundary_box = create_boundary_box(expected_position, human_pose_boundary, False)\n",
        "        # Limits Human Pose Boundary to the center of the Torso\n",
        "        torso_boundary_box = [Torso_Position[0], capture_width, Torso_Position[1] - int(Torso_Size[1]*1.5),Torso_Position[1] + int(Torso_Size[1]*1.5)]\n",
        "        boundary_box = boundary_box_overlap(boundary_box, torso_boundary_box)\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "      else:\n",
        "        # Limits Human Pose Boundary to the center of the Torso\n",
        "        torso_boundary_box = [0, Torso_Position[0], Torso_Position[1] - int(Torso_Size[1]*1.5),Torso_Position[1] + int(Torso_Size[1]*1.5)]\n",
        "        human_pose_boundary = boundary_box_overlap(human_pose_boundary, torso_boundary_box)\n",
        "        boundary_box = create_boundary_box(expected_position, human_pose_boundary, True)\n",
        "        boundary_box = boundary_box_overlap(boundary_box, torso_boundary_box)\n",
        "        if verbose == True:\n",
        "          display(f'The human_pose_boundary is {human_pose_boundary}.')\n",
        "          display(f'The Torso_Position is {Torso_Position}.')\n",
        "          display(f'The Torso Boundary Box is {torso_boundary_box}.')\n",
        "          display(f'The Boundary Box is {boundary_box}.')\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "      if verbose == True:\n",
        "        display(f'{tracked_item} : wrist conf:{fencer_data[0][2]}, box_test:{box_test}.')\n",
        "      if fencer_data[0][2] > wrist_conf_min and box_test:\n",
        "        #Wrist Pose Approximation\n",
        "        if verbose == True:\n",
        "          display(f'Using the Wrist Approximation for the {tracked_item} at frame {frame - 1}.')\n",
        "          display(f'The fencer data for frame {frame - 1} is:')\n",
        "          display(fencer_data)\n",
        "        using_human_pose = True\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          position = [fencer_data[0][0] + int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "        elif tracked_item == 'Right_BellGuard':\n",
        "          #Right_Bellguard is assumed\n",
        "          position = [fencer_data[0][0] - int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'The tracked item was not a Bell Guard at frame {frame - 1}.')\n",
        "      #If the Human Pose is outside of bounds then motion difference is tried\n",
        "      else:\n",
        "        # Adds 1 since the edge of the box often results in the position\n",
        "        motion_difference_boundary = [int(Torso_Size[0]*5/8), int(Torso_Size[0] + 1), int(Torso_Size[0]/3), int(Torso_Size[0]/3)]\n",
        "        if tracked_item == 'Left_BellGuard' and camera_steady[frame - 1] < camera_motion_threshold:\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "          position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, (capture_width/2560), 3, 4)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          if position != 'None' and box_test == True:\n",
        "            using_difference_images_normal_kernel = True\n",
        "          if position == 'None' or box_test == False:\n",
        "            if verbose == True:\n",
        "              display(f'Attempting to use a smaller kernel for motion difference tracking.')\n",
        "            position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, (capture_width/640), 3, 4)\n",
        "            box_test = boundary_box_test(position, boundary_box)\n",
        "            # if position != 'None' and box_test == True:\n",
        "            #   using_difference_images_normal_kernel = True\n",
        "            if position == 'None' or box_test == False:\n",
        "              if verbose == True:\n",
        "                display(f'Attempting to use a smallest kernel for motion difference tracking.')\n",
        "              position = motion_difference_tracking(frame, 'Left', boundary_box, capture_width, capture_height, (capture_width/320), 3, 4)\n",
        "              if position == 'None':\n",
        "                if verbose == True:\n",
        "                  display(f'Using the far Left Portion of the tracking Box')\n",
        "                position = [expected_position[0] - motion_difference_boundary[0], expected_position[1]]\n",
        "          # Adjusts the position if motion detection is too far out from the Torso\n",
        "          if position[0] > Torso_Position[0] + Torso_Size[0]*2.50:\n",
        "            if verbose == True:\n",
        "              display(f'The motion detected position was too far from the torso and was adjusted')\n",
        "            position[0] = int(Torso_Position[0] + Torso_Size[0]*2.50)\n",
        "            # position = [Torso_Position[0] + Torso_Size[0]*2.25, position[1]]\n",
        "          if verbose == True:\n",
        "            display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "            display(f'The boundary box test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "          # boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          #Uses the Expected position if the motion difference is out of bounds\n",
        "          if box_test == False:\n",
        "            if verbose == True:\n",
        "              display(f'Motion difference failed, using the Expected Position for the {tracked_item} for frame {frame - 1}.')\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "          else:\n",
        "            if verbose == True:\n",
        "              display(f'The motion difference position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "            using_difference_images = True\n",
        "        elif tracked_item == 'Right_BellGuard' and camera_steady[frame - 1] < camera_motion_threshold:\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "          position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, (capture_width/2560), 3, 4)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          if position != 'None' and box_test == True:\n",
        "            using_difference_images_normal_kernel = True\n",
        "          if position == 'None' or box_test == False:\n",
        "            if verbose == True:\n",
        "              display(f'Attempting to use a smaller kernel for motion difference tracking.')\n",
        "            position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, (capture_width/640), 3, 4)\n",
        "            box_test = boundary_box_test(position, boundary_box)\n",
        "            # if position != 'None' and box_test == True:\n",
        "            #   using_difference_images_normal_kernel = True\n",
        "            if position == 'None' or box_test == False:\n",
        "              if verbose == True:\n",
        "                display(f'Attempting to use the smallest kernel for motion difference tracking.')\n",
        "              position = motion_difference_tracking(frame, 'Right', boundary_box, capture_width, capture_height, (capture_width/320), 3, 4)\n",
        "              # Uses the Right Portion of the Motion Tracking Box if no motion tracking is found\n",
        "              if position == 'None' or box_test == False:\n",
        "                if verbose == True:\n",
        "                  display(f'Using the far Right Portion of the tracking Box, ({expected_position[0] + motion_difference_boundary[0]},{expected_position[1]})')\n",
        "                position = [expected_position[0] + motion_difference_boundary[0], expected_position[1]]\n",
        "          if position[0] < Torso_Position[0] - Torso_Size[0]*2.50:\n",
        "            if verbose == True:\n",
        "              display(f'The motion detected position ({position[0]},{position[1]}) was too far from the torso ({Torso_Position[0]},{Torso_Position[1]}), with a max of {Torso_Position[0] - Torso_Size[0]*2.50} and was adjusted')\n",
        "            position[0] = int(Torso_Position[0] - Torso_Size[0]*2.50)\n",
        "            # position = [Torso_Position[0] - Torso_Size[0]*2.25, position[1]]\n",
        "          if verbose == True:\n",
        "            display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "            display(f'The boundary box test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "          boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "          box_test = boundary_box_test(position, boundary_box)\n",
        "          # box_test = False\n",
        "          if box_test == False:\n",
        "            if verbose == True:\n",
        "              display(f'Motion difference failed, using the Expected Position for the {tracked_item} for frame {frame - 1}.')\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "          else:\n",
        "            if verbose == True:\n",
        "              display(f'The motion difference position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "            using_difference_images = True\n",
        "        else:\n",
        "          if verobse == True:\n",
        "            display(f'Too much camera motion, using expected position')\n",
        "          position = expected_position\n",
        "          using_expected = True\n",
        "    else:\n",
        "      position = expected_position    \n",
        "\n",
        "    try:\n",
        "      if verbose == True:\n",
        "        display(f'The data for certainty at frame {frame -1} is:')\n",
        "        display(f'position[1] is {position[1]} which must be less than Torso_Position[1] + Torso_Size[1] which is {Torso_Position[1] + Torso_Size[1]}.')\n",
        "        display(f'abs(expected_position[0] - position[0]) is {abs(expected_position[0] - position[0])} which must be less than Torso_Size[0] which is {Torso_Size[0]/2}.')\n",
        "        display(f'The Camera motion steady is {camera_steady[frame - 1]} and must be less than {camera_motion_threshold}.')\n",
        "    except:\n",
        "      if verbose == True:\n",
        "        display(f'Certainty Data failed to display for frame {frame -1 }.')\n",
        "\n",
        "    # Criteria for Setting Certainty to zero preventing a linear appoximation adjustment of this point\n",
        "    # Allows for motion difference to be certain if it uses the largest kernel\n",
        "    if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high) or \\\n",
        "    (using_difference_images_normal_kernel == True and position[1] < Torso_Position[1] + Torso_Size[1] and abs(expected_position[0] - position[0]) < Torso_Size[0]/2 and camera_steady[frame - 1] < camera_motion_threshold):\n",
        "      if using_difference_images == True:\n",
        "        if verbose == True:\n",
        "          display(f'Using difference images for frame {frame - 1} with no detected positions')\n",
        "    # if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high):\n",
        "      certainty = 0\n",
        "    else:\n",
        "      certainty = certainty + 1\n",
        "\n",
        "  # For a single detected Bellguard Position\n",
        "  elif (len(positions)) == 1:\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      if verbose == True:\n",
        "        display(f'There is one possible position, {positions[0]} for {tracked_item} in the tracking box for frame {frame - 1}.')\n",
        "      # Allows for a large bounding box if bell guard confidence is very high\n",
        "      if positions[0][2] > bellguard_confidence_extra_very_high:\n",
        "        if verbose == True:\n",
        "          display(f'Using Bell Guard Extra Very High confidence.')\n",
        "        single_position_box = [int(Torso_Size[0]*7/8*(1+bell_certainty/4)), int(Torso_Size[0]*8/8*(1+bell_certainty/4)), int(Torso_Size[0]*12/8), int(Torso_Size[0]*12/8)]\n",
        "      elif positions[0][2] > bellguard_confidence_very_high and positions[0][2] <= bellguard_confidence_extra_very_high:\n",
        "        if verbose == True:\n",
        "          display(f'Using Bell Guard Very High confidence.')\n",
        "        single_position_box = [int(Torso_Size[0]*6/8*(1+bell_certainty/4)), int(Torso_Size[0]*6/8*(1+bell_certainty/4)), int(Torso_Size[0]*12/8), int(Torso_Size[0]*12/8)]\n",
        "      else:\n",
        "        single_position_box = [int(Torso_Size[0]*4/8*(1+bell_certainty/4)), int(Torso_Size[0]*5/8*(1+bell_certainty/4)), int(Torso_Size[0]*8/8), int(Torso_Size[0]*8/8)]\n",
        "      if tracked_item == 'Left_BellGuard':\n",
        "        boundary_box = create_boundary_box(expected_position, single_position_box, False)\n",
        "      else:\n",
        "        boundary_box = create_boundary_box(expected_position, single_position_box, True)\n",
        "      box_test = boundary_box_test(positions[0], boundary_box)\n",
        "      if verbose == True:\n",
        "        display(f'The expected position for frame {frame - 1} is {expected_position}.')\n",
        "        display(f'The single_position_box is {single_position_box} and the boundary box is {boundary_box}.')\n",
        "      if box_test == True and positions[0][2] > bellguard_confidence_high:\n",
        "        if verbose == True:\n",
        "          display(f'The detected position was used for the {tracked_item} at frame {frame - 1}.')\n",
        "        position = positions[0]\n",
        "        using_position = True\n",
        "      else:\n",
        "        #Human Pose\n",
        "        if verbose == True:\n",
        "          display(f'Attempting to use Human Pose for the {tracked_item} at frame {frame - 1}')\n",
        "        if fencer_data == 'None':\n",
        "          fencer_data = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "        human_pose_boundary = [int(Torso_Size[0]*3/4), int(Torso_Size[0]), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "        if verbose == True:\n",
        "          display(f'Fencer data for frame {frame - 1} is: {fencer_data}.')\n",
        "        wrist_position = [fencer_data[0][0], fencer_data[0][1]]\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          # Limits Human Pose Wrist detections\n",
        "          torso_boundary_box = [Torso_Position[0], capture_width, Torso_Position[0] - int(Torso_Size[1]*1.5),Torso_Position[0] + int(Torso_Size[1]*1.5)]\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, False)\n",
        "          boundary_box = boundary_box_overlap(boundary_box, torso_boundary_box)\n",
        "        else:\n",
        "          torso_boundary_box = [0, Torso_Position[0], Torso_Position[0] - int(Torso_Size[1]*1.5),Torso_Position[0] + int(Torso_Size[1]*1.5)]\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, True)\n",
        "          boundary_box = boundary_box_overlap(boundary_box, torso_boundary_box)\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "        if fencer_data[0][2] > wrist_conf_min and box_test:\n",
        "          if verbose == True:\n",
        "            display(f'{tracked_item}: wrist conf:{fencer_data[0][2]}, box_test:{box_test}.')\n",
        "            display(f'Using the Wrist Approximation for the {tracked_item} at frame {frame - 1}.')\n",
        "            display(f'The fencer data for frame {frame - 1} is:')\n",
        "          using_human_pose = True\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            position = [fencer_data[0][0] + int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "          else:\n",
        "            #Right_Bellguard is assumed\n",
        "            position = [fencer_data[0][0] - int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/12)]\n",
        "        else:\n",
        "          #Image Difference\n",
        "          if verbose == True:\n",
        "            display(f'Attempting to use Image Difference for the {tracked_item} at frame {frame - 1}')\n",
        "          motion_difference_boundary = [int(Torso_Size[0]/8), int(Torso_Size[0]/2), int(Torso_Size[0]/4), int(Torso_Size[0]/4)]\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            boundary_box = create_boundary_box(expected_position, motion_difference_boundary, False)\n",
        "            diff_position = motion_difference_tracking(frame, 'Left', [x_min, x_max, y_min, y_max], capture_width, capture_height, 1, 1, 2)\n",
        "            if diff_position == 'None':\n",
        "              diff_position = motion_difference_tracking(frame, 'Left', [x_min, x_max, y_min, y_max], capture_width, capture_height, 2, 1, 2)\n",
        "          else:\n",
        "            #Right Bellguard is assumed\n",
        "            boundary_box = create_boundary_box(expected_position, motion_difference_boundary, True)\n",
        "            diff_position = motion_difference_tracking(frame, 'Right', [x_min, x_max, y_min, y_max], capture_width, capture_height, 1, 1, 2)\n",
        "            if diff_position == 'None':\n",
        "              diff_position = motion_difference_tracking(frame, 'Right', [x_min, x_max, y_min, y_max], capture_width, capture_height, 2, 1, 2)\n",
        "          box_test = boundary_box_test(diff_position, motion_difference_boundary)\n",
        "          if box_test == True and diff_position != 'None':\n",
        "            position = diff_position\n",
        "            using_difference_images = True\n",
        "          else:\n",
        "            #Expected Position\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "          if verbose == True:\n",
        "            display(f'The position for motion difference frame {frame - 1} is ({position})')\n",
        "            display(f'The motion_difference_boundary test limits are {motion_difference_boundary} for frame {frame - 1}.')\n",
        "\n",
        "      # Designed to catch an engarde position that is outside the tracking box\n",
        "      if frame < (engarde_length + 3) and position == twice_previous_position:\n",
        "        position = positions[0]\n",
        "\n",
        "      #Sets Certainty Box\n",
        "      if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high) or (using_position == True):\n",
        "        certainty = 0\n",
        "        if verbose == True:\n",
        "          display(f'Certainty set to zero for frame {frame - 1} for the {tracked_item}.')\n",
        "      else:\n",
        "        certainty = certainty + 1\n",
        "\n",
        "    else:\n",
        "      position = positions[0]\n",
        "\n",
        "  # Multiple bounding boxes within the tracking box\n",
        "  elif (len(positions)) > 1:\n",
        "    if verbose == True:\n",
        "      display(f'Multiple Bounding Boxes Detected for the {tracked_item} at frame {frame - 1}')\n",
        "    # One set of conditions is used for Bell_Guards and another for all else\n",
        "    if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "      # If the fencer_data wrist is confident, then it is used for Bell_Guards\n",
        "      if fencer_data[0][2] > wrist_conf_min:\n",
        "        if verbose == True:\n",
        "          display(f'Wrist Confidence Greater than Minimum for the {tracked_item} at frame {frame - 1}.')\n",
        "          display(f'The Pose Confidence is {fencer_data[0][2]} with a required minimum of {wrist_conf_min}.')\n",
        "        human_pose_boundary = [int(Torso_Size[0]/4), int(Torso_Size[0]/2), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "        wrist_position = [fencer_data[0][0], fencer_data[0][1]]\n",
        "        if tracked_item == 'Left_BellGuard':\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, False)\n",
        "        else:\n",
        "          boundary_box = create_boundary_box(expected_position, human_pose_boundary, True)\n",
        "        box_test = boundary_box_test(wrist_position, boundary_box)\n",
        "        if tracked_item == 'Left_BellGuard' and box_test == True:\n",
        "          position = [fencer_data[0][0] + int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/6)]\n",
        "          using_human_pose = True\n",
        "          if verbose == True:\n",
        "            display(f'Using the wrist position of {position} for the {tracked_item} at frame {frame - 1}.')\n",
        "        elif tracked_item == 'Right_BellGuard' and box_test == True:\n",
        "          position = [fencer_data[0][0] - int(Torso_Size[0]/8), fencer_data[0][1] - int(Torso_Size[0]/6)]\n",
        "          using_human_pose = True\n",
        "          if verbose == True:\n",
        "            display(f'Using the wrist position of {position} for the {tracked_item} at frame {frame - 1}.')\n",
        "        else:\n",
        "          # Tests if the Position Confidence is High for the Bellguard\n",
        "          if positions[0][2] > bellguard_confidence_high:\n",
        "            position = multiple_box_determination(expected_position, positions, [human_pose_boundary[0], human_pose_boundary[1]], bellguard_confidence, horiz_flip)\n",
        "            using_position = True\n",
        "          else:\n",
        "            position = expected_position\n",
        "            using_expected = True\n",
        "            if verbose == True:\n",
        "              display(f'The Human Pose Box Test failed for the {tracked_item} at frame {frame - 1}, using expected position.')\n",
        "          if verbose == True:\n",
        "            display(f'The point tested is {wrist_position} and the box is {boundary_box} for human pose at for the {tracked_item} at frame {frame - 1}')\n",
        "        # If the wrist confidence is not High, while the bellguard is, then uses the Bellguard Position\n",
        "        if fencer_data[0][2] < wrist_conf_high and positions[0][2] > bellguard_confidence_high:\n",
        "          single_position_box = [int(Torso_Size[0]/2), int(Torso_Size[0]), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            boundary_box = create_boundary_box(expected_position, single_position_box, False)\n",
        "          else:\n",
        "            boundary_box = create_boundary_box(expected_position, single_position_box, True)\n",
        "          box_test = boundary_box_test(positions[0], boundary_box)\n",
        "          if box_test:\n",
        "            if verbose == True:\n",
        "              display(f'Using the High Confidence Bellguard for Multiple Boxes for the {tracked_item} at frame {frame - 1}.')\n",
        "            position = positions[0]\n",
        "            using_position = True\n",
        "\n",
        "      # If the fencer_data wrist is not confident\n",
        "      else:    \n",
        "        if verbose == True:\n",
        "          display(f'Insufficient Pose Confidence for the {tracked_item} at frame {frame - 1}.')\n",
        "          display(f'The Pose Confidence is {fencer_data[0][2]} with a required minimum of {wrist_conf_min}.')\n",
        "          display(f'The x value is  {fencer_data[0][0]} with a minimum of {x_min} and a maximum of {x_max}.')\n",
        "          display(f'The x value is  {fencer_data[0][1]} with a minimum of {y_min} and a maximum of {y_max}.')\n",
        "        # Excludes Positions too far from expected but still within the tracking box\n",
        "        within_distance_from_expected = []\n",
        "        for i in range(len(positions)):\n",
        "          expected_box = [int(Torso_Size[0]/2*(1+bell_certainty/4)), int(Torso_Size[0]*(1+bell_certainty/4)), int(Torso_Size[0]/6*(1+bell_certainty/4)), int(Torso_Size[0]/6)]\n",
        "          if tracked_item == 'Left_BellGuard':\n",
        "            boundary_box = create_boundary_box(expected_position, expected_box, False)\n",
        "          else:\n",
        "            boundary_box = create_boundary_box(expected_position, expected_box, True)\n",
        "          box_test = boundary_box_test(positions[i], boundary_box)\n",
        "          if box_test:\n",
        "            within_distance_from_expected.append(positions[i])\n",
        "\n",
        "        # Uses the most confident, i.e. the first position in the list\n",
        "        if len(within_distance_from_expected) > 0:\n",
        "          position_boundary = [int(Torso_Size[0]/4), int(Torso_Size[0]/2), int(Torso_Size[0]/2), int(Torso_Size[0]/2)]\n",
        "          position = multiple_box_determination(expected_position, positions, [position_boundary[0], position_boundary[1]], bellguard_confidence, horiz_flip)\n",
        "          certainty = 0\n",
        "          using_position = True\n",
        "        else:\n",
        "          # If the length of within_distance_from_expected is zero\n",
        "          if verbose == True:\n",
        "            display(f'Error occured finding a position within the required distance and the {tracked_item} set to expeced position at frame {frame - 1}.')\n",
        "            display(f'The expected position is {expected_position}, while the expected box is {expected_box}.')\n",
        "          position = [(x_pos + x_speed),(y_pos + y_speed)]\n",
        "          using_expected = True\n",
        "\n",
        "      #Sets Certainty Box\n",
        "      if (using_human_pose == True and fencer_data[0][2] > wrist_conf_high) or (using_position == True):\n",
        "        if verbose == True:\n",
        "          display(f'Confidence for the {tracked_item} is High so the certainty is set to zero.')\n",
        "        certainty = 0\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'Confidence for the {tracked_item} is Low so the certainty is incremented higher.')\n",
        "        certainty = certainty + 1\n",
        "\n",
        "    # If the tracked item is not a bell_guard\n",
        "    else:\n",
        "      #Uses the most confident position within the tracking box\n",
        "      position = positions[0]\n",
        "\n",
        "\n",
        "  #Prevents the Bellguard being hidden behind the knee by setting a bellguard position behind the knee to the knee position.\n",
        "  if (tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard') and fencer_data[1][2] > knee_conf_min:\n",
        "    distance_from_position_to_knee = abs(int(((position[0] - fencer_data[1][0])**2 + (position[1] - fencer_data[1][1])**2)**(0.5)))\n",
        "    if verbose == True:\n",
        "      display(f'The distance for the {tracked_item} from the knee is {distance_from_position_to_knee} and the min is {(Torso_Size[0]/2)} at frame {frame - 1}.')\n",
        "    if tracked_item == 'Left_BellGuard':\n",
        "      if distance_from_position_to_knee < (Torso_Size[0]/2) and (position[0] < fencer_data[1][0]) and (fencer_data[0][2] > wrist_conf_min):\n",
        "        position = [fencer_data[1][0] + int(Torso_Size[0]/8), fencer_data[1][1] - int(Torso_Size[0]/12)]\n",
        "        if verbose == True:\n",
        "          display(f'The {tracked_item} is near the knee at frame {frame - 1}.')\n",
        "    else:\n",
        "      #Assumes Right_BellGuard\n",
        "      if distance_from_position_to_knee < (Torso_Size[0]/2) and (position[0] > fencer_data[1][0]) and (fencer_data[0][2] > wrist_conf_min):\n",
        "        position = [fencer_data[1][0] - int(Torso_Size[0]/8), fencer_data[1][1] - int(Torso_Size[0]/12)]\n",
        "        if verbose == True:\n",
        "          display(f'The {tracked_item} is near the knee at frame {frame - 1}.')\n",
        "\n",
        "  if tracked_item == 'Left_BellGuard' or tracked_item == 'Right_BellGuard':\n",
        "    if verbose == True:\n",
        "      display(f'The position of the {tracked_item} at frame {frame - 1} is {position}.')\n",
        "\n",
        "  return (position, certainty, [x_min, x_max, y_min, y_max])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRV4tGfTVF2",
        "cellView": "code"
      },
      "source": [
        "#@title Weight_Average_List\n",
        "def weight_average_list(List):\n",
        "  # Finds the Weight Average of a List\n",
        "\n",
        "  # Prevents division by zero\n",
        "  try:\n",
        "    value_sum = 0\n",
        "    value_weight = 0\n",
        "    for i in range(len(List)):\n",
        "      value_sum = value_sum + List[i][0] * List[i][1]\n",
        "      value_weight = value_weight + List[i][1]\n",
        "    weighted_average = value_sum/value_weight\n",
        "  except:\n",
        "    weighted_average = 0\n",
        "\n",
        "  return (weighted_average)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAkHbcyRCmQk",
        "cellView": "code"
      },
      "source": [
        "#@title Average_List\n",
        "def average_list(List):\n",
        "  # Finds the Average of a List\n",
        "  try:\n",
        "    average = sum(List) / len(List)\n",
        "  except:\n",
        "    average = 0\n",
        "  return (average)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViVEUhN0WqUu"
      },
      "source": [
        "def average_list_without_null(List):\n",
        "  # Takes a List of x,y values and null values. Removes the null values and returns the average x and y values.\n",
        "\n",
        "  List_temp = []\n",
        "  for i in range(len(List)):\n",
        "    if List[i] != []:\n",
        "      List_temp.append(List[i])\n",
        "    else:\n",
        "      pass\n",
        "  \n",
        "  x_sum = 0\n",
        "  y_sum = 0\n",
        "\n",
        "  # display(f'List_Temp is {List_temp}')\n",
        "\n",
        "  for j in range(len(List_temp)):\n",
        "    x_sum = x_sum + List_temp[j][0]\n",
        "    y_sum = y_sum + List_temp[j][1]\n",
        "\n",
        "  # display(f'x_sum is {x_sum}.')\n",
        "  # display(f'y_sum is {y_sum}.')\n",
        "  # display(f'The Length of List_Temp is {len(List_temp)}.')\n",
        "\n",
        "\n",
        "  x_average = int(x_sum/len(List_temp))\n",
        "  y_average = int(y_sum/len(List_temp))\n",
        "\n",
        "  average = [x_average, y_average]\n",
        "\n",
        "  return (average)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZCO-GE5eqUl",
        "cellView": "code"
      },
      "source": [
        "#@title Color_Tester\n",
        "def color_tester(box, frame):\n",
        "  #Takes a given box and tests for a specific color range\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "  file_name = str(frame) + '.jpg'\n",
        "  name = os.path.join(path, file_name)\n",
        "  img = cv2.imread(name)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The file names to be color tested is {name}.')\n",
        "  # box[0] are the coordinates ([y1,x1,y2,x2]), box[1] is confidence and box[2] is object\n",
        "  # Tests if Bellguard is the correct color\n",
        "  if box[2] == 1:\n",
        "    blue_range = [50, 150]\n",
        "    green_range = [50, 150]\n",
        "    red_range = [50, 160]\n",
        "    max_delta = 25\n",
        "  elif box[2] == 3:\n",
        "    blue_range = [60, 150]\n",
        "    green_range = [60, 150]\n",
        "    red_range = [60, 160]\n",
        "    max_delta = 30\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'The object to test does not have a color profile.')\n",
        "\n",
        "  # OpenCV uses Blue, Green, Red order\n",
        "  b, g, r = 0, 0, 0\n",
        "\n",
        "  width = (box[0][3]-box[0][1])\n",
        "  height = (box[0][2]-box[0][0])\n",
        "\n",
        "  #i is the x value of the image\n",
        "  for i in range(width):\n",
        "    #j is y value of the image\n",
        "    for j in range(height):\n",
        "      #color channel of the image [B,G,R]\n",
        "      #image, img, is of format [y,x] \n",
        "      b = b + img[box[0][0] + j, box[0][1] + i, 0]\n",
        "      g = g + img[box[0][0] + j, box[0][1] + i, 1]\n",
        "      r = r + img[box[0][0] + j, box[0][1] + i, 2]\n",
        "\n",
        "  # Finds the Color Averages\n",
        "  b_average = int(b/(width*height))\n",
        "  g_average = int(g/(width*height))\n",
        "  r_average = int(r/(width*height))\n",
        "\n",
        "  # Finds maximum differences between colors\n",
        "  max_1 = abs(b_average - g_average)\n",
        "  max_2 = abs(b_average - r_average)\n",
        "  max_3 = abs(g_average - r_average)\n",
        "  max_delta = max(max_1, max_2, max_3)\n",
        "\n",
        "  if test_result == False:\n",
        "    if verbose == True:\n",
        "      display(f'The Color Test Result Failed for object {box[2]}.')\n",
        "\n",
        "  return (test_result)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPntbZz1PTB-",
        "cellView": "code"
      },
      "source": [
        "#@title Symmetry_Test\n",
        "def symmetry_test(width, height, left_x, left_y, right_x, right_y):\n",
        "\n",
        "  # Tests the potential left and right positions for left/right symmetry and removes outlier points\n",
        "  display(f'Commencing Symmetry Test...')\n",
        "\n",
        "  # Sets how large the allowable band is with respect to height or width\n",
        "  band_width_ratio_x = 8\n",
        "  band_width_ratio_y = 8\n",
        "\n",
        "  all_positions_x = left_x + right_x\n",
        "  all_positions_y = left_y +right_y\n",
        "  if len(all_positions_x) != len(all_positions_y):\n",
        "    display(f'ERROR...The length of the x and y positions are different.')\n",
        "\n",
        "\n",
        "  # Keeps track of which positions are most in line with the other positions\n",
        "  # Finds the X Band\n",
        "  x_distances_from_center = []\n",
        "  x_distances_from_other_points_score = []\n",
        "  for i in range(len(all_positions_x)):\n",
        "    #Determines the x_min band for each position by distance from center\n",
        "    x_distances_from_center.append(abs(int((width/2)-all_positions_x[i])))\n",
        "  #Creates an iterator that determines which x_point is close to the most other points and finds its index\n",
        "  for j in range(len(x_distances_from_center)):\n",
        "    score = 0\n",
        "    for k in range(len(x_distances_from_center) - 1):\n",
        "      if abs(x_distances_from_center[j] - x_distances_from_center[k+1]) < width/band_width_ratio_x:\n",
        "        score = score + 1\n",
        "      else:\n",
        "        pass\n",
        "    x_distances_from_other_points_score.append(score)\n",
        "  x_index_band = x_distances_from_other_points_score.index(max(x_distances_from_other_points_score))\n",
        "\n",
        "  x_min = abs(int(all_positions_x[x_index_band] - width/band_width_ratio_x))\n",
        "  x_max = abs(int(all_positions_x[x_index_band] + width/band_width_ratio_x))\n",
        "\n",
        "  # Finds the Y Band\n",
        "  y_distances_from_center = []\n",
        "  y_distances_from_other_points_score = []\n",
        "  for i in range(len(all_positions_y)):\n",
        "    y_distances_from_center.append(abs(int((height/2)-all_positions_y[i])))\n",
        "  for j in range(len(y_distances_from_center)):\n",
        "    score = 0\n",
        "    for k in range(len(y_distances_from_center) - 1):\n",
        "      if abs(y_distances_from_center[j] - y_distances_from_center[k+1]) < width/band_width_ratio_y:\n",
        "        score = score + 1\n",
        "      else:\n",
        "        pass\n",
        "    y_distances_from_other_points_score.append(score)\n",
        "  y_index_band = y_distances_from_other_points_score.index(max(y_distances_from_other_points_score))\n",
        "\n",
        "  y_min = abs(int(all_positions_y[y_index_band] - width/band_width_ratio_y))\n",
        "  y_max = abs(int(all_positions_y[y_index_band] + width/band_width_ratio_y))\n",
        "\n",
        "  # Cycles through the positions and keeps values that are in the horizontal x band\n",
        "  positionsx_temp = []\n",
        "  positionsy_temp = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The x_min/max is {x_min}/{x_max}, the band width is {width/band_width_ratio_x} and the center is {width/2}.')\n",
        "\n",
        "  for i in range(len(all_positions_x)):\n",
        "    if ((all_positions_x[i] < (width/2 - x_min)) and (all_positions_x[i] > (width/2 - x_max))) or ((all_positions_x[i] < (width/2 + x_max)) and (all_positions_x[i] > (width/2 + x_min))):\n",
        "      positionsx_temp.append(all_positions_x[i])\n",
        "      positionsy_temp.append(all_positions_y[i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  # Replaces the all position x and y lists with the temp list limited by the bands\n",
        "  all_positions_x = positionsx_temp\n",
        "  all_positions_y = positionsy_temp\n",
        "\n",
        "  #Cycles through the positions and keeps values that are in the vertical y band\n",
        "  positionsx_temp = []\n",
        "  positionsy_temp = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The y_min/max is {y_min}/{y_max}, the band width is {height/band_width_ratio_y} and the center is {height/2}.')\n",
        "\n",
        "  for i in range(len(all_positions_y)):\n",
        "    if ((all_positions_y[i] > (y_min)) and (all_positions_y[i] < (y_max))):\n",
        "      positionsx_temp.append(all_positions_x[i])\n",
        "      positionsy_temp.append(all_positions_y[i])\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  # Replaces the all position x and y lists with the temp list limited by the bands\n",
        "  all_positions_x = positionsx_temp\n",
        "  all_positions_y = positionsy_temp\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'There were originaly {len(left_x) + len(right_x)} values and {len(all_positions_x) - (len(left_x) + len(right_x))} were removed.')\n",
        "\n",
        "  # Returns the x and y values to left and right positions\n",
        "  ret_left_x, ret_left_y, ret_right_x, ret_right_y = [],[],[],[]\n",
        "\n",
        "  \n",
        "  for i in range(len(all_positions_x)):\n",
        "    # Tests if the x value is on the left or right side\n",
        "    if all_positions_x[i] < width/2:\n",
        "      ret_left_x.append(all_positions_x[i])\n",
        "      ret_left_y.append(all_positions_y[i])\n",
        "    else:\n",
        "      ret_right_x.append(all_positions_x[i])\n",
        "      ret_right_y.append(all_positions_y[i])\n",
        "  # Prevents an off center camera from removing all engarde points\n",
        "  if (len(ret_left_x) == 0) or (len(ret_left_y) == 0) or (len(ret_right_x) == 0) or (len(ret_right_y) == 0):\n",
        "    ret_left_x = left_x\n",
        "    ret_left_y = left_y\n",
        "    ret_right_x = right_x\n",
        "    ret_right_y = right_y\n",
        "\n",
        "  return (ret_left_x, ret_left_y, ret_right_x, ret_right_y)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR6IATJKmVDz",
        "cellView": "code"
      },
      "source": [
        "#@title List_Threshold_Test\n",
        "def list_threshold_test(threshold, list_to_test):\n",
        "  #Determines if a list meets a minimum threshold\n",
        "  threshold_met = False\n",
        "\n",
        "  for k in range(len(list_to_test)):\n",
        "    if list_to_test[k][1] > threshold:\n",
        "      threshold_met = True\n",
        "      break\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return(threshold_met)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnsMsig-fR4M",
        "cellView": "code"
      },
      "source": [
        "#@title Multiple_Box_Determination\n",
        "def multiple_box_determination(expected_position, positions, x_boundaries, min_conf, horiz_flip):\n",
        "\n",
        "  confidence_weighting = .9\n",
        "\n",
        "  delta_x_forward = x_boundaries[1]\n",
        "  delta_x_backward = x_boundaries[0]\n",
        "\n",
        "  if horiz_flip == True:\n",
        "    delta_temp = delta_x_forward\n",
        "    delta_x_forward = delta_x_backward\n",
        "    delta_x_backward = delta_temp\n",
        "\n",
        "  position_ratings = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'There are {len(positions)} positions available.')\n",
        "    display(f'The positions are:')\n",
        "    display(positions)  \n",
        "\n",
        "  for i in range(len(positions)):\n",
        "    delta_position = positions[i][0] - expected_position[0]\n",
        "    if verbose == True:\n",
        "      display(f'The positions{i}[0] is {positions[i][0]} and the expected_position[0] is {expected_position[0]} therefore delta position is {delta_position}.')\n",
        "    if delta_position > 0:\n",
        "      if verbose == True:\n",
        "        display(f'Position {i} is forward of the expected position.')\n",
        "      position_ratings.append(abs((delta_position/delta_x_forward)*(1-positions[i][2])**confidence_weighting))\n",
        "      if verbose == True:\n",
        "        display(f'delta_position is {delta_position}.')\n",
        "        display(f'delta_x_forward is {delta_x_forward}.')\n",
        "        display(f'positions[i][2] is {positions[i][2]}.')\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'Position {i} is behind the expected position.')\n",
        "      position_ratings.append(abs((delta_position/delta_x_backward)*(1-positions[i][2])**confidence_weighting))\n",
        "      if verbose == True:\n",
        "        display(f'delta_position is {delta_position}.')\n",
        "        display(f'delta_x_backward is {delta_x_backward}.')\n",
        "        display(f'positions[i][2] is {positions[i][2]}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(position_ratings)\n",
        "\n",
        "  position = positions[position_ratings.index(min(position_ratings))]\n",
        "\n",
        "  return (position)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtyzedR9Aief",
        "cellView": "code"
      },
      "source": [
        "#@title Boundary_Box_Overlap\n",
        "def boundary_box_overlap(box1, box2):\n",
        "  #Finds the overlap of two boxes assume (x_min, x_max, y_min, y_max)\n",
        "  \n",
        "  box_overlap = [max(box1[0], box2[0]), min(box1[1], box2[1]), max(box1[2], box2[2]), min(box1[3], box2[3])]\n",
        "\n",
        "  return(box_overlap)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8uvTRrIxL3T",
        "cellView": "code"
      },
      "source": [
        "#@title Create_Boundary_Box\n",
        "def create_boundary_box(center, padding, horiz_flip):\n",
        "  # Creates a Boundary Box based on Center Padding and if the Left and Right Boundaries should be flipped.\n",
        "  # Center is [x,y]\n",
        "  # Padding is [Behind, Front, Top, Bottom]\n",
        "  # horiz_flip is True or False\n",
        "\n",
        "  if horiz_flip == False:\n",
        "    left = center[0] - padding[0]\n",
        "    right = center[0] + padding[1]\n",
        "  elif horiz_flip == True:\n",
        "    left = center[0] - padding[1]\n",
        "    right = center[0] + padding[0]\n",
        "  else:\n",
        "    if verbose == True:\n",
        "      display(f'ERROR Horiz Flip not True or False.')\n",
        "\n",
        "  top = center[1] - padding[2]\n",
        "  bottom = center[1] + padding[3]\n",
        "\n",
        "  return ([left, right, top, bottom])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnDPSNJoQINX",
        "cellView": "code"
      },
      "source": [
        "#@title Boundary_Box_Test\n",
        "def boundary_box_test(test_point, boundary):\n",
        "  # Tests if a give point is in a Boundary Box.\n",
        "  #Format Test_Point is of the form (x,y)\n",
        "  #Format Boundary is of the form (x_min, x_max, y_min, y_max)\n",
        "  #Format Boundary is of the form (behind the fencer, in front of the fencer, above the fencer, below the fencer)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(test_point)\n",
        "    display(boundary)\n",
        "\n",
        "  if test_point != 'None':\n",
        "    if test_point[0] > boundary[0] and test_point[0] < boundary[1] and test_point[1] > boundary[2] and test_point[1] < boundary[3]:\n",
        "      box_test = True\n",
        "    else:\n",
        "      box_test = False\n",
        "  else:\n",
        "    box_test = False\n",
        "\n",
        "  return (box_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPik4RGlWPYB",
        "cellView": "code"
      },
      "source": [
        "#@title Engarde_Failure_Test\n",
        "def engarde_failure_test(bbox, bellguard_confidence, x_max, y_max, side):\n",
        "  # Tests for reasons the engarde positioning failed to detect a BellGuard\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The {side} engarde position failed due to...')\n",
        "\n",
        "  if side == 'Left':\n",
        "    oppside = 'Right'\n",
        "    k = 0\n",
        "  else:\n",
        "    oppside = 'Left'\n",
        "    k = 1\n",
        "\n",
        "\n",
        "  for j in range(len(bbox)):\n",
        "    if bbox[j][1] < bellguard_confidence:\n",
        "      if verbose == True:\n",
        "        display(f'The confidence in the {side} bellguard is too low at {bellguard_confidence}.')\n",
        "    else: \n",
        "      pass\n",
        "    if side == 'Left':\n",
        "      if bbox[j][k] > x_max:\n",
        "        if verbose == True:\n",
        "          display(f'The {side} bellguard was too far {oppside} at {bbox[j][0]} while the maximum is {x_max}.')\n",
        "      else:\n",
        "        pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'bbox at this point is: {bbox}. J is {j} and k is {k}.')\n",
        "        display(bbox[j])\n",
        "        display(bbox[j][k])\n",
        "      if bbox[j][k] < x_max:\n",
        "        if verbose == True:\n",
        "          display(f'The {side} bellguard was too far {oppside} at {bbox[j][0]} while the maximum is {x_max}.')\n",
        "    if bbox[j][k] > y_max:\n",
        "      if verbose == True:\n",
        "        display(f'The {side} bellguard was too low at {bbox[j][0]} while the maximum allowed is {y_max}.')\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzdGZ-f52Ucw",
        "cellView": "code"
      },
      "source": [
        "#@title Torso_Failure_Test\n",
        "def torso_failure_test(bbox, capture_width, capture_height, y_average, Bell_Guard_Size_average, side, frame_count, min_torso_confidence):\n",
        "  # Tests for reasons the engarde positioning failed to detect a Torso\n",
        "  # Is tested at finding tracking boxes\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The {side} Torso failed due to...') \n",
        "  for j in range(len(bbox)):\n",
        "    if bbox[j][1] > min_torso_confidence:\n",
        "      pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'The confidence is of the box is too low at only {int(bbox[j][1]*100)}% at frame {frame_count}.')\n",
        "    if bbox[j][0][2] > y_average:\n",
        "      pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'The Torso was not lower than the Bell Guard with a lower height of {bbox[j][0][2]} with a max value of {y_average} at frame {frame_count}.')\n",
        "    if bbox[j][0][2] < (y_average + 3*Bell_Guard_Size_average[1]):\n",
        "      pass\n",
        "    else:\n",
        "      if verbose == True:\n",
        "        display(f'The bottom of the torso box was too low at {bbox[j][0][2]} with a max value of {int(y_average + 3*Bell_Guard_Size_average[1])} at frame {frame_count}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'y_average is {y_average}.')\n",
        "    display(f'Bell_Guard_Size_average[1] is {Bell_Guard_Size_average[1]}.')\n",
        "\n",
        "  return"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrP957nvDFvz",
        "cellView": "code"
      },
      "source": [
        "#@title Torso_Position_Failure_Test\n",
        "def torso_position_failure_test(bbox, engarde_length, x_min_torso, x_max_torso, y_min_torso, y_max_torso, y_average, side, frame_count):\n",
        "  # Tests for reasons the engarde positioning failed to detect a Torso\n",
        "  # Is tested at torso positions\n",
        "\n",
        "  confidence = min_torso_confidence\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Analyzing the Torso Position Failure at frame {frame_count} for the {side} side...')\n",
        "  count = 0\n",
        "  \n",
        "  for k in range(len(bbox)):\n",
        "\n",
        "    if bbox[k][2] == 3 and bbox[k][1] > confidence:\n",
        "      count = count + 1\n",
        "  if verbose == True:\n",
        "    display(f'There are {len(bbox)} ROIs, {count} of them are Torsos with greater than {confidence}%.')\n",
        "\n",
        "  for j in range(len(bbox)):\n",
        "    y_center = int((bbox[j][0][0] + bbox[j][0][2])/2)\n",
        "    x_center = int((bbox[j][0][1] + bbox[j][0][3])/2)\n",
        "    if bbox[j][2] == 3 and bbox[j][1] > confidence:\n",
        "      if x_center > x_min_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {x_center} is to the Left of the Box side at {x_min_torso} at frame {frame_count}.')\n",
        "      if x_center < x_max_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {x_center} is to the Right of the Box side at {x_max_torso} at frame {frame_count}.')\n",
        "      if y_center > y_min_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {y_center} is Above the Box at {y_min_torso} at frame {frame_count}.')\n",
        "      if y_center < y_max_torso:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center at {y_center} is Below the Box at {y_max_torso} at frame {frame_count}.')\n",
        "      if bbox[j][0][2] > y_average:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso center is Below the Bell Guard at frame {frame_count}.')\n",
        "      if bbox[j][2] == 3:\n",
        "        pass\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The Torso is not labelled as a Torso at frame {frame_count}.')\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrY_K5mZafoc",
        "cellView": "code"
      },
      "source": [
        "#@title Engarde_Position\n",
        "def engarde_position(bbox, capture_width, capture_height, engarde_length, frame_count):\n",
        "  #Finds the initial positions to start tracking\n",
        "  #Format of bbox[frame][roi], ([y1,x1,y2,x2], percent certainty, type)\n",
        "\n",
        "  # Initializes the Bell Guard Positions\n",
        "  # Position format [x,y]\n",
        "  # Size format [[Width],[Height]]\n",
        "  Left_Position = []\n",
        "  Right_Position = []\n",
        "  Bell_Guard_Size = [[],[]]\n",
        "  Scoring_Box_Position = []\n",
        "  Scoring_Box_Size = [[],[]]\n",
        "  Left_Torso_Position = []\n",
        "  Left_Torso_Size = [[],[]]\n",
        "  Right_Torso_Position = []\n",
        "  Right_Torso_Size = [[],[]]\n",
        "  All_Bell_Guard_Positions = []\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The bbox for the engarde capture at frame {frame_count} is:')\n",
        "    display(bbox)\n",
        "\n",
        "  #sum_of_boxes is used to average the Left (x,y)(0), Right (x,y)(1), ScoreBox (x,y)(2), Left_Torso (x,y)(3), Right_Torso(x,y)(4) values\n",
        "  sum_of_boxes = [[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]]\n",
        "\n",
        "  # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "  for j in range(len(bbox)):\n",
        "    # The percent confidence for each roi is [i][j][1]\n",
        "    # This uses the minimum value of the bbox (top-left) to determine Left, Right, Scorebox\n",
        "    # The Bellguards must be centered within the frame, classified as Bellguards with a minimum confidence and have the correct color saturation\n",
        "    # Adds values to the Left engarde box\n",
        "    if (bbox[j][1] > bellguard_confidence and bbox[j][0][1] < int(capture_width*2/5) and bbox[j][0][0] < int(capture_height*3/4) and bbox[j][0][0] > int(capture_height*1/4) and bbox[j][2] == 1):\n",
        "      test_result = saturation_test(bbox[j], frame_count)\n",
        "      if verbose == True:\n",
        "        display(f'The result of the saturation test for the Left Engarde Position is {test_result} at frame {frame_count}.')\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[0][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[0][1].append([bbox[j][0][0], bbox[j][1]])\n",
        "        #Appends x width value:\n",
        "        Bell_Guard_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Bell_Guard_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    #Adds values to the Right engarde box\n",
        "    elif (bbox[j][1] > bellguard_confidence and bbox[j][0][1] > int(capture_width*3/5) and bbox[j][0][0] < int(capture_height*3/4) and bbox[j][0][0] > int(capture_height*1/4) and bbox[j][2] == 1):\n",
        "      # test_result = color_tester(bbox[i][j], i)\n",
        "      test_result = saturation_test(bbox[j], frame_count)\n",
        "      if verbose == True:\n",
        "        display(f'The result of the saturation test for the Right Engarde Position is {test_result} at frame {frame_count}.')\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[1][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[1][1].append([bbox[j][0][0], bbox[j][1]])\n",
        "        #Appends x width value:\n",
        "        Bell_Guard_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Bell_Guard_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    #Adds values to the ScoreBox Position\n",
        "    elif (bbox[j][1] > 0.50 and bbox[j][0][1] > int(capture_width/3) and bbox[j][0][1] < int(capture_width*(2/3)) and bbox[j][2] == 2):\n",
        "      #Appends x value:\n",
        "      sum_of_boxes[2][0].append([bbox[j][0][1], bbox[j][1]])\n",
        "      #Appends y value:\n",
        "      sum_of_boxes[2][1].append([bbox[j][0][0], bbox[j][1]])  \n",
        "      #Appends x width value:\n",
        "      Scoring_Box_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "      #Appends y width value:\n",
        "      Scoring_Box_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "    else:\n",
        "      pass\n",
        "    \n",
        "    if bbox[j][2] == 1:\n",
        "      All_Bell_Guard_Positions.append([int((bbox[j][0][3] + bbox[j][0][1])/2), int((bbox[j][0][2] + bbox[j][0][0])/2)])\n",
        "\n",
        "  try:\n",
        "    # Tests for cause of Left Engarde Position Failure\n",
        "    if len(sum_of_boxes[0][0]) == 0:\n",
        "      engarde_failure_test(bbox[j], bellguard_confidence, int(capture_width*2/5), int(capture_height*2/3), 'Left')\n",
        "    # Tests for cause of Right Engarde Position Failure\n",
        "    if len(sum_of_boxes[1][0]) == 0:\n",
        "      engarde_failure_test(bbox[j], bellguard_confidence, int(capture_width*3/5), int(capture_height*3/4), 'Right')\n",
        "  except:\n",
        "    if verbose == True:\n",
        "      display(f'There was an error in the engarde failure test and it was skipped.')\n",
        "\n",
        "  # Finds the center point\n",
        "  x_average_left = weight_average_list(sum_of_boxes[0][0])\n",
        "  y_average_left = weight_average_list(sum_of_boxes[0][1])\n",
        "  x_average_right = weight_average_list(sum_of_boxes[1][0])\n",
        "  y_average_right = weight_average_list(sum_of_boxes[1][1])\n",
        "  x_average_scorebox = weight_average_list(sum_of_boxes[2][0])\n",
        "  y_average_scorebox = weight_average_list(sum_of_boxes[2][1])\n",
        "\n",
        "  # Prevents a failure to detect the bellguard from failing to detect the torso\n",
        "  # If the bellguard is unusually high or low then it is set to the height of the opposing BellGuard\n",
        "  if (y_average_left < capture_height/5) or (y_average_left > capture_height*4/5):\n",
        "    if verbose == True:\n",
        "      display(f'The y_average_left was too high or low and was set to y_average_right.')\n",
        "    y_average_left = y_average_right\n",
        "  if (y_average_right < capture_height/5) or (y_average_right > capture_height*4/5):\n",
        "    if verbose == True:\n",
        "      display(f'The y_average_right was too high or low and was set to y_average_left.')\n",
        "    y_average_right = y_average_left\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average left position is ({x_average_left},{y_average_left}).')\n",
        "    display(f'The average right position is ({x_average_right},{y_average_right}).')\n",
        "\n",
        "  # Bell_Guard_Size_average [Width, Height]\n",
        "  Bell_Guard_Size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  Bell_Guard_Size_average.append(average_list(Bell_Guard_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  Bell_Guard_Size_average.append(average_list(Bell_Guard_Size[1]))\n",
        "\n",
        "  # Finds the Torso Position After the Bell_Guard Position because the Bell_Guard is used as a constraint\n",
        "  # j represents the rois(specific bounding box) within the frame sorted by confidence score\n",
        "  for j in range(len(bbox)):\n",
        "    # Adds values to the Left_Torso Position, similar requirements to Left guard\n",
        "    # Minimum Torso confidence, on the left half of the screen, bottom of the box is below the bellguard, but also above 3 three times the bellguard height and is labeled torso\n",
        "    if (bbox[j][1] > min_torso_confidence and bbox[j][0][1] < int(capture_width/2) and bbox[j][0][2] > (y_average_left - Bell_Guard_Size_average[1] * 2) \\\n",
        "        and bbox[j][0][2] < (y_average_left + 3*Bell_Guard_Size_average[1]) and bbox[j][2] == 3):\n",
        "      \n",
        "      # Tests the Torso Color Saturation\n",
        "      # Bypasses Saturation Test\n",
        "      # test_result = saturation_test(bbox[j], frame_count)\n",
        "      test_result = True\n",
        "      if test_result == True:\n",
        "        # Appends x value:\n",
        "        sum_of_boxes[3][0].append(bbox[j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[3][1].append(bbox[j][0][0])\n",
        "        #Appends x width value:\n",
        "        Left_Torso_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Left_Torso_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The saturation test failed at frame {frame_count}.')\n",
        "        else:\n",
        "          pass\n",
        "    # Adds values to the Right_Torso Position, similar requirements to Right guard\n",
        "    # display(f'y_average_right-Bell_Guard_Size[1] * 2 is {(y_average_right-Bell_Guard_Size[1]*2)}.')\n",
        "\n",
        "    if Bell_Guard_Size[1] == []:\n",
        "      if verbose == True:\n",
        "        display(f'The Bell Guard Height was not defined so it is set to a default of zero at frame {frame_count}.')\n",
        "      Bell_Guard_Size[1] = 0\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'bbox[j][1] is {bbox[j][1]}.')\n",
        "      display(f'bbox[j][0][1] is {bbox[j][0][1]}.')\n",
        "      display(f'bbox[j][0][2] is {bbox[j][0][2]}.')\n",
        "      display(f'y_average_right is {y_average_right}')\n",
        "      display(f'Bell_Guard_Size[1] is {Bell_Guard_Size[1]}')\n",
        "      display(f'(y_average_right-2*Bell_Guard_Size[1]) is {(y_average_right-2*Bell_Guard_Size[1])}.')\n",
        "      display(f'bbox[j][1] is {bbox[j][1]}.')\n",
        "    \n",
        "\n",
        "\n",
        "    if (bbox[j][1] > min_torso_confidence and bbox[j][0][1] > int(capture_width/2) and bbox[j][0][2] > (y_average_right-2*Bell_Guard_Size_average[1]) and bbox[j][0][2] < (y_average_right + 3*Bell_Guard_Size_average[1]) and bbox[j][2] == 3):\n",
        "      \n",
        "      # Bypasses Saturation Test\n",
        "      # test_result = saturation_test(bbox[j], frame_count)\n",
        "      test_result = True\n",
        "      if test_result == True:\n",
        "        #Appends x value:\n",
        "        sum_of_boxes[4][0].append(bbox[j][0][1])\n",
        "        #Appends y value:\n",
        "        sum_of_boxes[4][1].append(bbox[j][0][0])\n",
        "        #Appends x width value:\n",
        "        Right_Torso_Size[0].append(bbox[j][0][3] - bbox[j][0][1])\n",
        "        #Appends y width value:\n",
        "        Right_Torso_Size[1].append(bbox[j][0][2] - bbox[j][0][0])\n",
        "      else:\n",
        "        if verbose == True:\n",
        "          display(f'The saturation test failed at frame {frame_count}.')\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "  if len(sum_of_boxes[3][0]) == 0:\n",
        "    torso_failure_test(bbox, capture_width, capture_height, y_average_left, Bell_Guard_Size_average, 'Left', frame_count, min_torso_confidence)\n",
        "  if verbose == True:\n",
        "    display(f'Prior to torso failure test for right torso the y_average_left is {y_average_left}.')\n",
        "\n",
        "  if len(sum_of_boxes[4][0]) == 0:\n",
        "    torso_failure_test(bbox, capture_width, capture_height, y_average_right, Bell_Guard_Size_average, 'Right', frame_count, min_torso_confidence) \n",
        "  if verbose == True:\n",
        "    display(f'Prior to torso failure test for left torso the y_average_right is {y_average_right}.')\n",
        "\n",
        "  #Finds the top left corner then moves the average point to the center\n",
        "  x_average_left_torso = average_list(sum_of_boxes[3][0]) + average_list(Left_Torso_Size[0])/2\n",
        "  y_average_left_torso = average_list(sum_of_boxes[3][1]) + average_list(Left_Torso_Size[1])/2\n",
        "  x_average_right_torso = average_list(sum_of_boxes[4][0]) + average_list(Right_Torso_Size[0])/2\n",
        "  y_average_right_torso = average_list(sum_of_boxes[4][1]) + average_list(Right_Torso_Size[1])/2\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average left engarde position is:({x_average_left},{y_average_left})')\n",
        "    display(f'The average right engarde position is:({x_average_right},{y_average_right})')\n",
        "    display(f'The average left torso is:({int(x_average_left_torso)},{int(y_average_left_torso)})')\n",
        "    display(f'The average right torso is:({int(x_average_right_torso)},{int(y_average_right_torso)})')\n",
        "\n",
        "  # scoring_box_size_average [Width, Height]\n",
        "  scoring_box_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  scoring_box_size_average.append(average_list(Scoring_Box_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  scoring_box_size_average.append(average_list(Scoring_Box_Size[1]))\n",
        "\n",
        "  # left_torso_size_average [Width, Height]\n",
        "  left_torso_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  left_torso_size_average.append(average_list(Left_Torso_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  left_torso_size_average.append(average_list(Left_Torso_Size[1]))\n",
        "\n",
        "  # right_torso_size_average [Width, Height]\n",
        "  right_torso_size_average = []\n",
        "  # Appends the average scoring box width\n",
        "  right_torso_size_average.append(average_list(Right_Torso_Size[0]))\n",
        "  # Appends the average scoring box height\n",
        "  right_torso_size_average.append(average_list(Right_Torso_Size[1]))\n",
        "\n",
        "  #Creates Padding for the EnGarde Tracking Box\n",
        "  engarde_box_padding = int(capture_width/15)\n",
        "  torso_padding = int(capture_width/20)\n",
        "\n",
        "  x_min_engardeL = int(x_average_left - engarde_box_padding)\n",
        "  x_max_engardeL = int(x_average_left + engarde_box_padding)\n",
        "  y_min_engardeL = int(y_average_left - engarde_box_padding)\n",
        "  y_max_engardeL = int(y_average_left + engarde_box_padding)\n",
        "\n",
        "  x_min_engardeR = int(x_average_right - engarde_box_padding)\n",
        "  x_max_engardeR = int(x_average_right + engarde_box_padding)\n",
        "  y_min_engardeR = int(y_average_right - engarde_box_padding)\n",
        "  y_max_engardeR = int(y_average_right + engarde_box_padding)\n",
        "\n",
        "  x_min_engardeScore = int(x_average_scorebox - engarde_box_padding)\n",
        "  x_max_engardeScore = int(x_average_scorebox + engarde_box_padding)\n",
        "  y_min_engardeScore = int(y_average_scorebox - engarde_box_padding)\n",
        "  y_max_engardeScore = int(y_average_scorebox + engarde_box_padding)\n",
        "\n",
        "  x_min_torsoL = int(x_average_left_torso - torso_padding)\n",
        "  x_max_torsoL = int(x_average_left_torso + torso_padding)\n",
        "  y_min_torsoL = int(y_average_left_torso - torso_padding*3/2)\n",
        "  y_max_torsoL = int(y_average_left_torso + torso_padding*3/2)\n",
        "\n",
        "  x_min_torsoR = int(x_average_right_torso - torso_padding)\n",
        "  x_max_torsoR = int(x_average_right_torso + torso_padding)\n",
        "  y_min_torsoR = int(y_average_right_torso - torso_padding*3/2)\n",
        "  y_max_torsoR = int(y_average_right_torso + torso_padding*3/2)\n",
        "\n",
        "  #Iterates through the first engarde_length frames and checks if there are rois in the expected engarde position\n",
        "  for j in range(len(bbox)):\n",
        "    y_center = int((bbox[j][0][0] + bbox[j][0][2])/2)\n",
        "    x_center = int((bbox[j][0][1] + bbox[j][0][3])/2)\n",
        "    # Checks for rois in the Left Engarde Position\n",
        "    if (x_center > x_min_engardeL and x_center < x_max_engardeL and y_center > y_min_engardeL and y_center < y_max_engardeL and bbox[j][2] == 1):\n",
        "      # display(f'The roi is in the left en garde position')\n",
        "      Left_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Right Engarde Position\n",
        "    if (x_center > x_min_engardeR and x_center < x_max_engardeR and y_center > y_min_engardeR and y_center < y_max_engardeR and bbox[j][2] == 1):\n",
        "      # display(f'The roi is in the right en garde position')\n",
        "      Right_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Scoring Box Position\n",
        "    if (x_center > x_min_engardeScore and x_center < x_max_engardeScore and y_center > y_min_engardeScore and y_center < y_max_engardeScore and bbox[j][2] == 2):\n",
        "      Scoring_Box_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Left Torso Position\n",
        "    if (x_center > x_min_torsoL and x_center < x_max_torsoL and y_center > y_min_torsoL and y_center < y_max_torsoL and bbox[j][0][2] > y_average_left and bbox[j][2] == 3):\n",
        "      Left_Torso_Position.append([x_center, y_center])\n",
        "    # Checks for rois in the Right Torso Position \n",
        "    if (x_center > x_min_torsoR and x_center < x_max_torsoR and y_center > y_min_torsoR and y_center < y_max_torsoR and bbox[j][0][2] > y_average_right and bbox[j][2] == 3):\n",
        "      Right_Torso_Position.append([x_center, y_center])\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp = [[],[],[]]\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(x_min_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(x_max_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(y_min_engardeL)\n",
        "    Tracking_Bounding_Boxes_Temp[0].append(y_max_engardeL)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(x_min_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(x_max_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(y_min_engardeR)\n",
        "    Tracking_Bounding_Boxes_Temp[1].append(y_max_engardeR)\n",
        "\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(x_min_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(x_max_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(y_min_engardeScore)\n",
        "    Tracking_Bounding_Boxes_Temp[2].append(y_max_engardeScore)\n",
        "\n",
        "    Tracking_Bounding_Boxes = Tracking_Bounding_Boxes_Temp\n",
        "\n",
        "  # Tests for why a Torso Position is not Found\n",
        "  if (len(Left_Torso_Position) == 0):\n",
        "    torso_position_failure_test(bbox, engarde_length, x_min_torsoL, x_max_torsoL, y_min_torsoL, y_max_torsoL, y_average_left, 'Left', frame_count)    \n",
        "  if (len(Right_Torso_Position) == 0):\n",
        "    torso_position_failure_test(bbox, engarde_length, x_min_torsoR, x_max_torsoR, y_min_torsoR, y_max_torsoR, y_average_right, 'Right', frame_count)\n",
        "\n",
        "\n",
        "  # Averages the Left and Right x,y positions for engarde\n",
        "  # Left Bell Guard engarde position\n",
        "  # if verbose == True:\n",
        "  #   display(f'The length of the built Tracking Bounding Boxes is {len(Tracking_Bounding_Boxes[0])}.')\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Left_Position) > 0:\n",
        "    for i in range(len(Left_Position)):\n",
        "      x = x + Left_Position[i][0]\n",
        "      y = y + Left_Position[i][1]\n",
        "    x = int(x/(len(Left_Position)))\n",
        "    y = int(y/(len(Left_Position)))\n",
        "    Left_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Left_Position at Engarde is:')\n",
        "    display(Left_Position)\n",
        "\n",
        "  # Right Bell Guard engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Right_Position) > 0:\n",
        "    for i in range(len(Right_Position)):\n",
        "      x = x + Right_Position[i][0]\n",
        "      y = y + Right_Position[i][1]\n",
        "    x = int(x/(len(Right_Position)))\n",
        "    y = int(y/(len(Right_Position)))\n",
        "    Right_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Right_Position at Engarde is:')\n",
        "    display(Right_Position)\n",
        "\n",
        "  # Scoring_Box engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Scoring_Box_Position) > 0:\n",
        "    for i in range(len(Scoring_Box_Position)):\n",
        "      x = x + Scoring_Box_Position[i][0]\n",
        "      y = y + Scoring_Box_Position[i][1]\n",
        "    x = int(x/(len(Scoring_Box_Position)))\n",
        "    y = int(y/(len(Scoring_Box_Position)))\n",
        "    Scoring_Box_Position = [x,y]\n",
        "\n",
        "  if Scoring_Box_Position == [0,0]:\n",
        "    Tracking_Bounding_Boxes_Temp[2] = [0,0,0,0]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Scoring_Box_Position at Engarde is:')\n",
        "    display(Scoring_Box_Position)\n",
        "\n",
        "  # Left_Torso engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Left_Torso_Position) > 0:\n",
        "    for i in range(len(Left_Torso_Position)):\n",
        "      x = x + Left_Torso_Position[i][0]\n",
        "      y = y + Left_Torso_Position[i][1]\n",
        "    x = int(x/(len(Left_Torso_Position)))\n",
        "    y = int(y/(len(Left_Torso_Position)))\n",
        "    Left_Torso_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Left_Torso_Position at Engarde is:')\n",
        "    display(Left_Torso_Position)\n",
        "\n",
        "  # Right_Torso engarde position\n",
        "  x = 0\n",
        "  y = 0\n",
        "  if len(Right_Torso_Position) > 0:\n",
        "    for i in range(len(Right_Torso_Position)):\n",
        "      x = x + Right_Torso_Position[i][0]\n",
        "      y = y + Right_Torso_Position[i][1]\n",
        "    x = int(x/(len(Right_Torso_Position)))\n",
        "    y = int(y/(len(Right_Torso_Position)))\n",
        "    Right_Torso_Position = [x,y]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Right_Torso_Position at Engarde is:')\n",
        "    display(Right_Torso_Position)\n",
        "\n",
        "  return (Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Tracking_Bounding_Boxes, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average, All_Bell_Guard_Positions)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0ssV1qoSLh",
        "cellView": "code"
      },
      "source": [
        "#@title Draw_Bell_Guard_Position\n",
        "def draw_Bell_Guard_Position(Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Left_Torso_Position, Right_Torso_Position, frame_count, Tracking_Bounding_Boxes, video_filename, capture_width, capture_height, engarde_length, keypoints, score_box_empty, camera_steady, camera_motion_threshold, Exclusion_Areas, simple_clip_vector):\n",
        "  #Adds an overlay on the image to visualize the location of tracked objects\n",
        "\n",
        "  path = r'/content/Mask_RCNN/videos/'\n",
        "  capture = cv2.VideoCapture(os.path.join(path, video_filename))\n",
        "\n",
        "  capture.set(cv2.CAP_PROP_FRAME_WIDTH, capture_width)\n",
        "  capture.set(cv2.CAP_PROP_FRAME_HEIGHT, capture_height)\n",
        "\n",
        "  #Color format is [B,G,R]\n",
        "  left_light_color_default = [[],[],[]]\n",
        "  right_light_color_default = [[],[],[]]\n",
        "  left_light_color = []\n",
        "  right_light_color = []\n",
        "\n",
        "  # Creates a list of Files from a Directory\n",
        "  path = r'/content/Mask_RCNN/videos/save/'\n",
        "  path_orig = r'/content/Mask_RCNN/videos/original/'\n",
        "  files = [i for i in os.listdir(path)]\n",
        "  # Sorts the Files after cropping '.jpg'\n",
        "  files.sort(key=lambda x: int(x[:-4]))\n",
        "\n",
        "  left_light_comparison, right_light_comparison, default_color = [], [], []\n",
        "\n",
        "  for i, file in enumerate(files):\n",
        "    # Reads the image\n",
        "    # name = os.path.join(path_orig, file)\n",
        "    name = os.path.join(path, file)\n",
        "    img = cv2.imread(name)\n",
        "\n",
        "    # OpenCV uses Blue, Green, Red order\n",
        "    # Light_Color is of the format [[[B0],[G0],[R0]],[[B1],[G1],[R1]],[[B2],[G2],[R2]],...]\n",
        "    \n",
        "    if i <= engarde_length:\n",
        "      if scoring_box_size_average == [0,0]:\n",
        "        scoring_box_size_average = [int(capture_width/5), int(capture_height/5)]\n",
        "      if verbose == True:\n",
        "        display(f'The average scoring box size is {scoring_box_size_average}.')\n",
        "      # Uses a comparison of frames and scoring box position to determine the light off colors\n",
        "      [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = scoring_box_lights(img, Scoring_Box_Position[i], scoring_box_size_average, [], i, score_box_empty)\n",
        "      left_light_comparison.append(left_light_comparison_temp)\n",
        "      right_light_comparison.append(right_light_comparison_temp)\n",
        "      default_color.append(defualt_color_temp)\n",
        "      # Averages the Default Color on the Last iteration\n",
        "      if i == engarde_length:\n",
        "        b_temp = int(sum(default_color[0])/len(default_color[0]))\n",
        "        g_temp = int(sum(default_color[1])/len(default_color[1]))\n",
        "        r_temp = int(sum(default_color[2])/len(default_color[2]))\n",
        "        default_color = [b_temp,g_temp,r_temp]\n",
        "    elif i > engarde_length:\n",
        "      try:\n",
        "        [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = scoring_box_lights(img, Scoring_Box_Position[i], scoring_box_size_average, default_color, i, score_box_empty)\n",
        "      except:\n",
        "        if verbose == True:\n",
        "          display(f'Light Comparison Failed due to Error at frame {i}.')\n",
        "        [left_light_comparison_temp, right_light_comparison_temp, defualt_color_temp] = [0,0,[]]\n",
        "      left_light_comparison.append(left_light_comparison_temp)\n",
        "      right_light_comparison.append(right_light_comparison_temp)\n",
        "\n",
        "    # if verbose == True:\n",
        "    #   display(f'Frame Count is {frame_count}.')\n",
        "\n",
        "    #Creates the dots on the Bell Guards\n",
        "    frame = cv2.circle(img, (Left_Position[i][0], Left_Position[i][1]), 4, (118, 37, 217), -1)\n",
        "    frame = cv2.circle(frame, (Right_Position[i][0], Right_Position[i][1]), 4, (157, 212, 19), -1)\n",
        "    frame = cv2.circle(frame, (Scoring_Box_Position[i][0], Scoring_Box_Position[i][1]), 4, (255, 255, 0), -1)\n",
        "    frame = cv2.circle(frame, (Left_Torso_Position[i][0], Left_Torso_Position[i][1]), 4, (0, 255, 0), -1)\n",
        "    frame = cv2.circle(frame, (Right_Torso_Position[i][0], Right_Torso_Position[i][1]), 4, (255, 255, 0), -1)\n",
        "\n",
        "    if generate_representative == False:\n",
        "      # Creates the Representative Bell Guard Position\n",
        "      frame = cv2.circle(img, (Left_Position[i][0], int(capture_height/2)), 20, (118, 37, 217), -1)\n",
        "      frame = cv2.circle(frame, (Right_Position[i][0], int(capture_height/2)), 20, (157, 212, 19), -1)\n",
        "\n",
        "    # Creates the Light Indicators\n",
        "      rect_size = int(capture_width/40)\n",
        "      if (simple_clip_vector[i][2] == 1):\n",
        "        #Creates the Left Score Light\n",
        "        frame = cv2.rectangle(frame, (rect_size, int(rect_size*1.5)), (rect_size*5, int(rect_size*4.5)), (0, 0, 255), -1)\n",
        "      if (simple_clip_vector[i][3] == 1):\n",
        "        #Creates the Right Score Light\n",
        "        frame = cv2.rectangle(frame, (capture_width - rect_size, int(rect_size*1.5)), (capture_width - rect_size*5, int(rect_size*4.5)), (0, 255, 0), -1)\n",
        "\n",
        "    # Adds Frame Number to the Image\n",
        "    text = 'Frame' + str(i)\n",
        "    frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*1/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, )\n",
        "    # Adds if the Camera Motion is detected by difference images\n",
        "    if camera_steady[i] > camera_motion_threshold:\n",
        "      text = 'Camera'\n",
        "      frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*3/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4, )\n",
        "      text = 'Motion'\n",
        "      frame = cv2.putText(frame, text, (int(capture_width*7/8), int(capture_height*4/16)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4, )\n",
        "    \n",
        "    # Sets BellGuard Position Colors\n",
        "    left_color = (255, 0, 0)\n",
        "    right_color = (0, 255, 0)\n",
        "\n",
        "    #Creates the Tracking Boxes\n",
        "    frame = cv2.putText(frame, 'Tracking Box', (Tracking_Bounding_Boxes[i][0][0], Tracking_Bounding_Boxes[i][0][2]), cv2.FONT_HERSHEY_COMPLEX, 0.7, left_color, 2)\n",
        "    frame = cv2.rectangle(frame, (Tracking_Bounding_Boxes[i][0][0], Tracking_Bounding_Boxes[i][0][2]),(Tracking_Bounding_Boxes[i][0][1], Tracking_Bounding_Boxes[i][0][3]),left_color, 2)\n",
        "    frame = cv2.putText(frame, 'Tracking Box', (Tracking_Bounding_Boxes[i][1][0], Tracking_Bounding_Boxes[i][1][2]), cv2.FONT_HERSHEY_COMPLEX, 0.7, right_color, 2)\n",
        "    frame = cv2.rectangle(frame, (Tracking_Bounding_Boxes[i][1][0], Tracking_Bounding_Boxes[i][1][2]),(Tracking_Bounding_Boxes[i][1][1], Tracking_Bounding_Boxes[i][1][3]),right_color, 2)\n",
        "\n",
        "    [frame, none] = overlay_keypoints(frame, keypoints[i][0], keypoints[i][1], True)\n",
        "\n",
        "\n",
        "    #Draws the Exclusion Areas\n",
        "    for j in range(len(Exclusion_Areas)):\n",
        "      frame = cv2.circle(frame, (Exclusion_Areas[j][0],Exclusion_Areas[j][1]), int(capture_width/80), (144,238,144), 2)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The Tracking Box for the Left Fencer at frame {i} is:')\n",
        "      display(f'{Tracking_Bounding_Boxes[i][0][0]},{Tracking_Bounding_Boxes[i][0][2]}')\n",
        "      display(f'The Tracking Box for the Right Fencer at frame {i} is:')\n",
        "      display(f'{Tracking_Bounding_Boxes[i][1][0]},{Tracking_Bounding_Boxes[i][1][2]}')\n",
        "\n",
        "    #Saves the image frame overwriting the original image\n",
        "    name = os.path.join(path, file)\n",
        "    cv2.imwrite(name, frame)\n",
        "    if verbose == True:\n",
        "      display(f'The Draw Bell Guard frame {i} is being saved at {name}.')\n",
        "\n",
        "  #Releases capture so that other files can be used\n",
        "  capture.release()\n",
        "\n",
        "  return (left_light_comparison, right_light_comparison)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZNUlGxZAMYv",
        "cellView": "code"
      },
      "source": [
        "#@title Mask_Image\n",
        "def mask_image(frame, width, height, masking_box):\n",
        "  # Used to Mask parts of the image that are not of interest\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The masking box is:')\n",
        "    display(masking_box)\n",
        "\n",
        "  #Create the Mask\n",
        "  mask = np.zeros((height, width, 3), dtype = np.uint8);\n",
        "  for i in range(len(masking_box)):\n",
        "    mask = cv2.rectangle(mask, (masking_box[i][0], masking_box[i][2]) ,(masking_box[i][1], masking_box[i][3]), (255,255,255), -1)\n",
        "\n",
        "  #Applies the mask to Frame\n",
        "  frame = cv2.bitwise_and(mask, frame)\n",
        "\n",
        "  return (frame)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkcK4BxFc-lJ",
        "cellView": "code"
      },
      "source": [
        "#@title Create_Representative_Image\n",
        "def create_representative_image(clip_vector, capture_width, capture_height):\n",
        "  # Allows for an overlay that represents the bellguard horizontal motion and box lights\n",
        "\n",
        "  #Creates a Folder to save the images and removes previous version\n",
        "  os.chdir('/content/Mask_RCNN/videos')\n",
        "  # Removes and Recreates the Save_White_Dot to ensure the directory is empty\n",
        "  try:\n",
        "    shutil.rmtree('save_white_dot')\n",
        "    if verbose == True:\n",
        "      display(f'Removed the Save_White_Dot folder.')\n",
        "  except:\n",
        "    if verbose == True:\n",
        "      display(f'ERROR removing the Save_White_Dot folder.')\n",
        "  os.mkdir('save_white_dot')\n",
        "\n",
        "  rect_size = int(capture_width/40)\n",
        "\n",
        "  #Defines the File Path\n",
        "  path = r'/content/Mask_RCNN/videos/save_white_dot/'\n",
        "  \n",
        "  for i in range(len(clip_vector)):\n",
        "    img = np.zeros((capture_height,capture_width,3), np.uint8)\n",
        "\n",
        "    #Creates the Left Bell_Guard\n",
        "    img = cv2.circle(img, (clip_vector[i][0], int(capture_height/2)), 20, (118, 37, 217), -1)\n",
        "    #Creates the Right Bell_Guard\n",
        "    img = cv2.circle(img, (clip_vector[i][1], int(capture_height/2)), 20, (157, 212, 19), -1)\n",
        "\n",
        "    if (clip_vector[i][2] == 1):\n",
        "      #Creates the Left Score Light\n",
        "      img = cv2.rectangle(img, (rect_size, rect_size), (rect_size*5, rect_size*3), (0, 0, 255), -1)\n",
        "    if (clip_vector[i][3] == 1):\n",
        "      #Creates the Right Score Light\n",
        "      img = cv2.rectangle(img, (capture_width - rect_size, rect_size), (capture_width - rect_size*5, rect_size*3), (0, 255, 0), -1)\n",
        "\n",
        "    name = str(i) + '.jpg'\n",
        "    name = os.path.join(path, name)\n",
        "\n",
        "    cv2.imwrite(name, img)\n",
        "\n",
        "  return"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCrXhYq2f3sb",
        "cellView": "code"
      },
      "source": [
        "#@title Create_Overlay_Image\n",
        "def create_overlay_image(frame_count):\n",
        "  # Allows for an overlay that represents the bellguard horizontal motion and box lights\n",
        "\n",
        "  #Creates a Folder to save the images and removes previous version\n",
        "  os.chdir('/content/Mask_RCNN/videos/')\n",
        "  # !rm -r /content/Mask_RCNN/videos/overlay\n",
        "  # Attempts to remove the Overlay folder and recreate it to ensure that it is empty\n",
        "  try:\n",
        "    shutil.rmtree('overlay')\n",
        "  except:\n",
        "    display(f'ERROR removing the Overlay folder.')\n",
        "  # !mkdir overlay\n",
        "  os.mkdir('overlay')\n",
        "\n",
        "\n",
        "  #Defines the File Path\n",
        "  path = r'/content/Mask_RCNN/videos/overlay/'\n",
        "  path_background = r'/content/Mask_RCNN/videos/save/'\n",
        "  path_foreground = r'/content/Mask_RCNN/videos/save_white_dot/'\n",
        "  for i in range(frame_count):\n",
        "    background_name = str(i) + '.jpg'\n",
        "    background_name = os.path.join(path_background, background_name)\n",
        "\n",
        "    foreground_name = str(i) + '.jpg'\n",
        "    foreground_name = os.path.join(path_foreground, foreground_name)\n",
        "    \n",
        "    background = cv2.imread(background_name)\n",
        "    foreground = cv2.imread(foreground_name)\n",
        "\n",
        "    added_image = cv2.addWeighted(background,0.8,foreground,1.0,0)\n",
        "\n",
        "    combined_name = str(i) + '.jpg'\n",
        "    combined_name = os.path.join(path, combined_name)\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'The file added image is saved at {combined_name}.')\n",
        "\n",
        "    cv2.imwrite(combined_name, added_image)\n",
        "\n",
        "  return"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RQ1thQAPxbc",
        "cellView": "code"
      },
      "source": [
        "#@title Light_Color_Comparison\n",
        "def light_color_comparison(light_color, light_color_default, color):\n",
        "  # Deterines if a light turned on based on a default color, an input color and expected color\n",
        "\n",
        "  light_comparison = []\n",
        "  # A high max distance is less sensitive and a lower max distance is more sensitive\n",
        "  max_distance_total = 180\n",
        "  max_distance_specific_color = 90\n",
        "\n",
        "  if color == 'Red':\n",
        "    color_specific = 2\n",
        "  elif color == 'Green':\n",
        "    color_specific = 1\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Color being analyzed is {color}.')\n",
        "    display(f'The default color is:')\n",
        "    display(light_color_default)\n",
        "    display(f'With the specific color being {light_color_default[color_specific]}')\n",
        "    display(f'The max distance total is {max_distance_total}.')\n",
        "    display(f'The max distance for a specific color is {max_distance_specific_color}.')\n",
        "\n",
        "  #i cycles through each light value corresponding to each frame\n",
        "  for i in range(len(light_color)):\n",
        "    distance = 0\n",
        "    for j in range(3):\n",
        "      distance = distance + (light_color[i][j] - light_color_default[j])**2\n",
        "\n",
        "    distance_specific_color = abs(light_color[i][color_specific] - light_color_default[color_specific])\n",
        "\n",
        "    distance = int((distance)**(0.5))\n",
        "    if vebose == True:\n",
        "      display(f'The distance is {distance} and the color specific distance is {distance_specific_color} for frame {i}.')\n",
        "    #0 is no color change from the default color)\n",
        "    if (distance > max_distance_total and distance_specific_color > max_distance_specific_color):\n",
        "      light_comparison.append(1)\n",
        "      if verbose == True:\n",
        "        display(f'The light is ON.')\n",
        "    #1 is a color change from the default color\n",
        "    else:\n",
        "      light_comparison.append(0)\n",
        "      if verbose == True:\n",
        "        display(f'The light is OFF.')\n",
        "\n",
        "  return (light_comparison)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU0k7BCzVbOD",
        "cellView": "code"
      },
      "source": [
        "#@title Clip_Vector_Generator\n",
        "def clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous, width):\n",
        "  #Compiles the clip_vector that is used for the action analysis\n",
        "\n",
        "  # Allows for the assumption that both lights are on if the positions are close to each other.\n",
        "  # Useful if there is difficulty detecting the scoring box.\n",
        "  close_bellguards = False\n",
        "  # Once lights turn on it is assumed the lights stay on for the rest of the action\n",
        "  light_assumption = False\n",
        "\n",
        "  if len(Left_Position) != len(Right_Position):\n",
        "    display(f'The Left and Right Positions do not match up')\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  # This is either [] or the Previously saved Clip_Vector\n",
        "  clip_vector = clip_vector_previous\n",
        "\n",
        "  for i in range(len(Left_Position)):  \n",
        "    # Checks the lights should be assumed on if they are not already\n",
        "    # Determines if the bellguards are close to each other\n",
        "    # if (abs(Left_Position[i][0] - Right_Position[i][0]) < width*.050) and (light_assumption == False):\n",
        "    if ((Right_Position[i][0] - Left_Position[i][0]) < width*position_difference_ratio) and (light_assumption == False):\n",
        "      close_bellguards = True\n",
        "\n",
        "    # Adjusts the clip vector to reflect scoring box light assumptions\n",
        "    clip_vector_temp = [[],[],[],[]]\n",
        "    clip_vector_temp[0] = Left_Position[i][0]\n",
        "    clip_vector_temp[1] = Right_Position[i][0]\n",
        "    if (assume_lights == True and close_bellguards == True) or light_assumption == True:\n",
        "      clip_vector_temp[2] = 1\n",
        "      clip_vector_temp[3] = 1\n",
        "      light_assumption = True\n",
        "    else:\n",
        "      if ignore_box_lights == True:\n",
        "        clip_vector_temp[2] = 0\n",
        "        clip_vector_temp[3] = 0\n",
        "      else:\n",
        "        clip_vector_temp[2] = left_light_comparison[i]\n",
        "        clip_vector_temp[3] = right_light_comparison[i]\n",
        "\n",
        "    clip_vector.append(clip_vector_temp)\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtuA8ocWoV9l"
      },
      "source": [
        "def simple_clip_vector_generator(Left_Position, Right_Position, width):\n",
        "  #Compiles the clip_vector that is used for the action analysis\n",
        "\n",
        "  # Allows for the assumption that both lights are on if the positions are close to each other.\n",
        "  # Useful if there is difficulty detecting the scoring box.\n",
        "  close_bellguards = False\n",
        "  # Once lights turn on it is assumed the lights stay on for the rest of the action\n",
        "  light_assumption = False\n",
        "\n",
        "  if len(Left_Position) != len(Right_Position):\n",
        "    display(f'The Left and Right Positions do not match up')\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  clip_vector = []\n",
        "\n",
        "  for i in range(len(Left_Position)):  \n",
        "    # Checks the lights should be assumed on if they are not already\n",
        "    # Determines if the bellguards are close to each other\n",
        "    if ((Right_Position[i][0] - Left_Position[i][0]) < width*position_difference_ratio) and (light_assumption == False):\n",
        "      close_bellguards = True\n",
        "\n",
        "    # Adjusts the clip vector to reflect scoring box light assumptions\n",
        "    clip_vector_temp = [[],[],[],[]]\n",
        "    clip_vector_temp[0] = Left_Position[i][0]\n",
        "    clip_vector_temp[1] = Right_Position[i][0]\n",
        "    if (assume_lights == True and close_bellguards == True) or light_assumption == True:\n",
        "      clip_vector_temp[2] = 1\n",
        "      clip_vector_temp[3] = 1\n",
        "      light_assumption = True\n",
        "    else:\n",
        "      clip_vector_temp[2] = 0\n",
        "      clip_vector_temp[3] = 0\n",
        "\n",
        "    clip_vector.append(clip_vector_temp)\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zNZqZbF3AKY",
        "cellView": "code"
      },
      "source": [
        "#@title Clip_Vector_Np_Save\n",
        "def clip_vector_np_save(clip_call, file_number, clip_vector):\n",
        "  # Saves the clip vector for future use\n",
        "  # Clip_Call Left_Touch, Right_Touch, Simul\n",
        "\n",
        "  # Generates the clip_vector speed based on the clip_vector\n",
        "  clip_vector_speed = []\n",
        "  for i in range(len(clip_vector)-1):\n",
        "    clip_vector_speed.append([])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][0]-clip_vector[i][0])\n",
        "    # Reverses the Right Fencers position so that positive is towards the opponent\n",
        "    clip_vector_speed[i].append(clip_vector[i][1]-clip_vector[i+1][1])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][2])\n",
        "    clip_vector_speed[i].append(clip_vector[i+1][3])\n",
        "\n",
        "  # Generates the clip_vector acceleration based on the clip_vector\n",
        "  clip_vector_acceleration = []\n",
        "  for i in range(len(clip_vector_speed)-1):\n",
        "    clip_vector_acceleration.append([])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][0]-clip_vector[i][0])\n",
        "    # Reverses the Right Fencers position so that positive is towards the opponent\n",
        "    clip_vector_acceleration[i].append(clip_vector[i][1]-clip_vector[i+1][1])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][2])\n",
        "    clip_vector_acceleration[i].append(clip_vector[i+1][3])\n",
        "\n",
        "  path = '/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "\n",
        "  # Saves the clip_vector as a numpy array\n",
        "  clip_vector_np = np.asarray(clip_vector)\n",
        "  name = os.path.join(path, clip_call)\n",
        "  name_2 = clip_call + '_Vector_Clips'\n",
        "  name = os.path.join(name, name_2)\n",
        "  if verbose == True:\n",
        "    display(f'The name of the path for clip vectors to be saved is:')\n",
        "    display(name)\n",
        "\n",
        "  # Changes the directory to the sub folder of the fenncing clip\n",
        "  # %cd $name\n",
        "  os.chdir(name)\n",
        "\n",
        "  clip_vector_np_name = 'clip_vector_np' + str(file_number) + '.csv'\n",
        "  # Saves to the current directory\n",
        "  np.savetxt(clip_vector_np_name, clip_vector_np, delimiter=',')\n",
        "\n",
        "  # Saves the clip_vector_speed\n",
        "  clip_vector_speed_np = np.asarray(clip_vector_speed)\n",
        "  name = os.path.join(path, clip_call)\n",
        "  name_2 = clip_call + '_Vector_Clips_Speed'\n",
        "  name = os.path.join(name, name_2)\n",
        "\n",
        "  # Changes the directory to the sub folder for the speed fencing clip\n",
        "  # %cd $name\n",
        "  os.chdir(name)\n",
        "\n",
        "  clip_vector_speed_np_name = 'clip_vector_speed_np' + str(file_number) + '.csv'\n",
        "  # Saves to the current directory\n",
        "  np.savetxt(clip_vector_speed_np_name, clip_vector_speed_np, delimiter=',')\n",
        "\n",
        "  # Saves the clip_vector_acceleration\n",
        "  clip_vector_acceleration_np = np.asarray(clip_vector_acceleration)\n",
        "  name = os.path.join(path, clip_call)\n",
        "  name_2 = clip_call + '_Vector_Clips_Acceleration'\n",
        "  name = os.path.join(name, name_2)\n",
        "\n",
        "  # Changes the directory to the sub folder for the acceleration fencning clip\n",
        "  # %cd $name\n",
        "  os.chdir(name)\n",
        "\n",
        "  clip_vector_acceleration_np_name = 'clip_vector_acceleration_np' + str(file_number) + '.csv'\n",
        "  #Saves to the current directory\n",
        "  np.savetxt(clip_vector_acceleration_np_name, clip_vector_acceleration_np, delimiter=',')\n",
        "\n",
        "  return"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1WK5HCkX_eX",
        "cellView": "code"
      },
      "source": [
        "#@title Left_Right_Test\n",
        "def Left_Right_Test(Left_Position, Right_Position):\n",
        "  # Requires that the Left and Right BellGuards be on the Left and Right sides respectively\n",
        "\n",
        "  #Left_Position is chosen arbitrarily for length\n",
        "  for i in range(len(Left_Position)):\n",
        "    if Left_Position[i][0] > Right_Position[i][0]:\n",
        "      if verbose == True:\n",
        "        display(f'The Left and Right were swapped on frame {i} and are now corrected.')\n",
        "      position_temp = Left_Position[i]\n",
        "      Left_Position[i] = Right_Position[i]\n",
        "      Right_Position[i] = position_temp\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return (Left_Position, Right_Position)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHKEKonnjipI",
        "cellView": "code"
      },
      "source": [
        "#@title Camera_Motion_Adjustment\n",
        "def camera_motion_adjustment(Position, Score_Box_Position):\n",
        "  # Takes a Position as an input and adjusts the position to compensate for camera motion\n",
        "  # Uses solely the x position of the scoring box to calculate motion\n",
        "  # Ignores the change in angle as the camera is rotated\n",
        "  # This is only used when it is assumed that the Scoring Box is well detected and tracked\n",
        "\n",
        "  Score_Box_Position_Temp = []\n",
        "  #Converts Scoring Box Positions to solely x value\n",
        "  #Scoring Box Position is of the format [x0,x1,x2...]\n",
        "  for i in range(len(Score_Box_Position)):\n",
        "    Score_Box_Position_Temp.append(Score_Box_Position[i][0])\n",
        "\n",
        "  for j in range(len(Position)):\n",
        "    score_box_delta = Score_Box_Position_Temp[j] - Score_Box_Position_Temp[0]\n",
        "    Position[j][0] = Position[j][0] - score_box_delta\n",
        "\n",
        "  return (Position)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAKPWXQz0R4l",
        "cellView": "code"
      },
      "source": [
        "#@title Position_Down_Scale\n",
        "def position_down_scale(Position1, Position2, capture_width, capture_height):\n",
        "  # Scales the Position Down to the Capture Width in the x axis if required for visualization convenience\n",
        "  # Does not alter the Clip Vector Data\n",
        "  \n",
        "  position_temp = []\n",
        "\n",
        "  for i in range(len(Position1)):\n",
        "    position_temp.append(Position1[i][0])\n",
        "\n",
        "  for j in range(len(Position2)):\n",
        "    position_temp.append(Position2[j][0])\n",
        "\n",
        "  min_x_position = min(position_temp)\n",
        "  max_x_position = max(position_temp)\n",
        "\n",
        "  if min_x_position < 0:\n",
        "    #Shifts the bellguards to the right for the camera moving to the left\n",
        "    for i in range(len(Position1)):\n",
        "      Position1[i][0] = int(Position1[i][0] - min_x_position)\n",
        "\n",
        "    for j in range(len(Position2)):\n",
        "      Position2[j][0] = int(Position2[j][0] - min_x_position)\n",
        "\n",
        "  # Absolute Pixel\n",
        "  if max_x_position > capture_width:\n",
        "    #Scales the max x position if greater than the screen\n",
        "    for i in range(len(Position1)):\n",
        "      Position1[i][0] = int(Position1[i][0] * capture_width / max_x_position)\n",
        "\n",
        "    for j in range(len(Position2)):\n",
        "      Position2[j][0] = int(Position2[j][0] * capture_width / max_x_position)\n",
        "\n",
        "  return (Position1, Position2)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbuXubXE4XRf",
        "cellView": "code"
      },
      "source": [
        "#@title Random_Colors\n",
        "def random_colors(N):\n",
        "    np.random.seed(1)\n",
        "    colors = [tuple(255 * np.random.rand(3)) for _ in range(N)]\n",
        "    return colors"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFVyAiia4ZcB",
        "cellView": "code"
      },
      "source": [
        "#@title Apply_Mask\n",
        "def apply_mask(image, mask, color, alpha=0.5):\n",
        "    \"\"\"apply mask to image\"\"\"\n",
        "    for n, c in enumerate(color):\n",
        "        image[:, :, n] = np.where(\n",
        "            mask == 1,\n",
        "            image[:, :, n] * (1 - alpha) + alpha * c,\n",
        "            image[:, :, n]\n",
        "        )\n",
        "    return image"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urekTB1e4bf4",
        "cellView": "code"
      },
      "source": [
        "#@title Display_Instances\n",
        "def display_instances(image, boxes, masks, ids, names, scores, file_name):\n",
        "    \"\"\"\n",
        "        take the image and results and apply the mask, box, and Label\n",
        "    \"\"\"\n",
        "    n_instances = boxes.shape[0]\n",
        "    colors = random_colors(n_instances)\n",
        "\n",
        "    if not n_instances:\n",
        "        print('NO INSTANCES TO DISPLAY')\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    for i, color in enumerate(colors):\n",
        "        if not np.any(boxes[i]):\n",
        "            continue\n",
        "\n",
        "        y1, x1, y2, x2 = boxes[i]\n",
        "        # label = boxes[i][4]\n",
        "        label = names[ids[i]]\n",
        "        score = scores[i] if scores is not None else None\n",
        "        caption = '{} {:.2f}'.format(label, score) if score else label\n",
        "        mask = masks[:, :, i]\n",
        "        # display(f'The mask is: {mask}')\n",
        "        # image = apply_mask(image, mask, color)\n",
        "        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "        image = cv2.putText(\n",
        "            image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2\n",
        "        )\n",
        "\n",
        "    return image"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKAQZNzFfoWM",
        "cellView": "code"
      },
      "source": [
        "#@title Save_Clip_Progress\n",
        "def save_clip_progress(bbox, frame_count, capture_width, capture_height, clip_vector_previous):\n",
        "\n",
        "  # Counts the files in the directory '/content/Mask_RCNN/videos/save/'\n",
        "\n",
        "  # %cd /content/Mask_RCNN/videos/save/\n",
        "  os.chdir('/content/Mask_RCNN/videos/save/')\n",
        "  number_of_stored_frames = len(os.listdir())\n",
        "  display(f'The number of stored frames in the save folder is {number_of_stored_frames}.')\n",
        "\n",
        "  if number_of_stored_frames > 500:\n",
        "    #Passes the Bounding Boxes to Determine position of items of interest\n",
        "    [Left_Position, Right_Position, Scoring_Box_Position, scoring_box_size_average, Tracking_Bounding_Boxes, \\\n",
        "     Left_Torso_Position, Right_Torso_Position] = Bell_Guard_Position_Finding(bbox, capture_width, capture_height)\n",
        "\n",
        "    #Draws the Boxes on the image frame and determines scoring lights turned on\n",
        "    [left_light_comparison, right_light_comparison] = draw_Bell_Guard_Position(Left_Position, Right_Position, \\\n",
        "      Scoring_Box_Position, scoring_box_size_average, Left_Torso_Position, Right_Torso_Position, frame_count, \\\n",
        "      Tracking_Bounding_Boxes, video_filename, capture_width, capture_height, engarde_length)\n",
        "\n",
        "    #Adjusts the Bellguard Position Based on the Camera motion as determined by the Score_Box Position\n",
        "    Left_Position = camera_motion_adjustment(Left_Position, Scoring_Box_Position)\n",
        "    Right_Position = camera_motion_adjustment(Right_Position, Scoring_Box_Position)\n",
        "\n",
        "    #Adjusts Left and Right Position for convenient visualization\n",
        "    [Left_Position, Right_Position] = position_down_scale(Left_Position, Right_Position, capture_width, capture_height)\n",
        "\n",
        "    #Creates a vector representing the clip, format [left_x, right_x, left_lights, right_lights]\n",
        "    clip_vector = clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous)\n",
        "\n",
        "    # clip_vector = smooth_clip_vector(clip_vector, engarde_length)\n",
        "\n",
        "    clip_call = 'Temp_Clip_Vector'\n",
        "    file_number = '1'\n",
        "\n",
        "    #Saves the Clip, Speed and Acceleration Vectors\n",
        "    clip_vector_np_save(clip_call, file_number, clip_vector)\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  return()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UhmJ-_-kyG3",
        "cellView": "code"
      },
      "source": [
        "#@title Smooth_Clip_Vector\n",
        "def smooth_clip_vector(clip_vector, engarde_length):\n",
        "  # Allows for smoothing the clip_vector\n",
        "\n",
        "  a = []\n",
        "  b = []\n",
        "  for i in range(engarde_length, len(clip_vector)):\n",
        "    a.append(clip_vector[i][0])\n",
        "    b.append(clip_vector[i][1])\n",
        "\n",
        "  x = np.linspace(engarde_length,len(clip_vector), len(clip_vector) - engarde_length)\n",
        "\n",
        "  # sos = signal.ellip(13, 0.009, 80, 0.05, output='sos')\n",
        "  # yhata = signal.sosfilt(sos, a)\n",
        "  if len(a)%2 == 1:\n",
        "    yhata = signal.savgol_filter(a, len(a), 11)\n",
        "    yhatb = signal.savgol_filter(b, len(b), 11)\n",
        "  else:\n",
        "    yhata = signal.savgol_filter(a, len(a) - 1, 11)\n",
        "    yhatb = signal.savgol_filter(b, len(b) - 1, 11)    \n",
        "\n",
        "  # plt.plot(x,a, color='black')\n",
        "  # plt.plot(x,yhata, color='red')\n",
        "  plt.plot(x,b, color='black')\n",
        "  plt.plot(x,yhatb, color='blue')\n",
        "  plt.show()\n",
        "\n",
        "  vector_clip_smooth = []\n",
        "\n",
        "  for j in range(len(clip_vector)):\n",
        "    if j <= engarde_length:\n",
        "      clip_vector_smooth_temp = [clip_vector[j][0], clip_vector[j][1], clip_vector[j][2], clip_vector[j][3]]\n",
        "    else:\n",
        "      clip_vector_smooth_temp = [int(yhata[j - engarde_length]), int(yhatb[j - engarde_length]), clip_vector[j][2], clip_vector[j][3]]\n",
        "    vector_clip_smooth.append(clip_vector_smooth_temp)\n",
        "\n",
        "  return (vector_clip_smooth)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug5H_4QojaEk",
        "cellView": "code"
      },
      "source": [
        "#@title Load_Clip_Vector\n",
        "def load_clip_vector():\n",
        "  # Only used for large clips\n",
        "\n",
        "  display(f'Loading the Clip Vector...')\n",
        "  filename = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Temp_Clip_Vector/Temp_Clip_Vector_Clips/clip_vector_np1.csv'\n",
        "\n",
        "  display(f'Attempting to load:')\n",
        "  display(filename)\n",
        "  try:\n",
        "    vector_data = pd.read_csv(filename, header=None)\n",
        "    arr = vector_data.to_numpy(dtype = np.int32)\n",
        "    clip_vector = arr.tolist()\n",
        "  except:\n",
        "    display(f'Load Failure...')\n",
        "    display(f'The clip_vector did not exist so it is set to []')\n",
        "    clip_vector = []\n",
        "\n",
        "  return (clip_vector)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22PKpGGUnmKK",
        "cellView": "code"
      },
      "source": [
        "#@title Create_Tracking_Masks\n",
        "def create_tracking_masks(previous_positions, certainty, frame_count, torso_size, width, height):\n",
        "  #Creates Tracking Boxes that can be used to mask the image, ignoring parts that are not of interest\n",
        "  #Format, Tracking_Boxes = [Left, Right, Scorebox], Left = [x_min, x_max, y_min, y_max]\n",
        "  #Format, Previous Positions\n",
        "          # previous_positions  = [[Left_Position[-1], Left_Position[-2]], \\\n",
        "          #                      [Right_Position[-1], Right_Position[-2]], \\\n",
        "          #                      [Scoring_Box_Position[-1], Scoring_Box_Position[-2]], \\\n",
        "          #                      [Left_Torso_Position[-1], Left_Torso_Position[-2]], \\\n",
        "          #                      [Right_Torso_Position[-1], Right_Torso_Position[-2]]]\n",
        "\n",
        "          #Format, positions are [x,y]\n",
        "\n",
        "  #Format, torso_position = [[Left_x,Lefty],[Right_x,Right_y]]\n",
        "  #Format, torso_size = [[Lw,Lh], [Rw,Rh]]\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'Creating Tracking Masks...')\n",
        "    display(f'The Previous Positions are:')\n",
        "    display(previous_positions)\n",
        "    display(f'The torso sizes are:')\n",
        "    display(torso_size)\n",
        "\n",
        "  frame_mask = []\n",
        "\n",
        "  #Certainty is the number of times the bellguard has not been detected in previous frames\n",
        "  #certainty_default is the minimum size of the tracking box\n",
        "  certainty_default = int(width/16)\n",
        "  #certainty_multiplier is how much the tracking box enlarges following a missed\n",
        "  certainty_multiplier = int(width/80)\n",
        "  y_limiter = 24\n",
        "\n",
        "  #Max allowed speed of a bellguard in a single frame\n",
        "  max_speed = int(width/48)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The length of the previous positions is: {len(previous_positions)}.')\n",
        "\n",
        "  for i in range(len(previous_positions)):\n",
        "    if verbose == True:\n",
        "      display(f'The masking iteration for frame {frame_count} is {i}.')\n",
        "    #FINDS THE LEFT MASKING BOX\n",
        "    x_pos = previous_positions[i][0][0]\n",
        "    y_pos = previous_positions[i][0][1]\n",
        "    #Converts previous position into a speed\n",
        "    x_speed = min(previous_positions[i][0][0] - previous_positions[i][1][0], max_speed)\n",
        "    # Limits the maximum vertical speed with relation to x\n",
        "    y_speed = min(previous_positions[i][0][1] - previous_positions[i][1][1], int(max_speed/y_limiter))\n",
        "\n",
        "    if verbose == True:\n",
        "      display(f'x and y position is ({x_pos},{y_pos}) and the speeds are ({x_speed},{y_speed}).')\n",
        "\n",
        "    x_min = x_pos + (x_speed) - (certainty[i]*certainty_multiplier) - certainty_default\n",
        "    x_max = x_pos + (x_speed) + (certainty[i]*certainty_multiplier) + certainty_default\n",
        "    y_min = y_pos + (y_speed) - (certainty[i]*certainty_multiplier) - certainty_default\n",
        "    y_max = y_pos + (y_speed) + (certainty[i]*certainty_multiplier) + certainty_default\n",
        "\n",
        "    #Appends the mask to collection of tracked areas\n",
        "    frame_mask.append([x_min, x_max, y_min, y_max])\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Frame Mask for frame {frame_count} is:')\n",
        "    display(frame_mask)\n",
        "\n",
        "  return(frame_mask)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrtkw3MVw-H4",
        "cellView": "code"
      },
      "source": [
        "def make_video(outvid, images=None, fps=25, size=None,\n",
        "               is_color=True, format=\"FMP4\"):\n",
        "  \n",
        "    \"\"\"\n",
        "    Create a video from a list of images.\n",
        " \n",
        "    @param      outvid      output video\n",
        "    @param      images      list of images to use in the video\n",
        "    @param      fps         frame per second\n",
        "    @param      size        size of each frame\n",
        "    @param      is_color    color\n",
        "    @param      format      see http://www.fourcc.org/codecs.php\n",
        "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
        " \n",
        "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
        "    By default, the video will have the size of the first image.\n",
        "    It will resize every image to this size before adding them to the video.\n",
        "    \"\"\"\n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*format)\n",
        "    vid = None\n",
        "    for image in images:\n",
        "        if not os.path.exists(image):\n",
        "            raise FileNotFoundError(image)\n",
        "        img = imread(image)\n",
        "        if vid is None:\n",
        "            if size is None:\n",
        "                size = img.shape[1], img.shape[0]\n",
        "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
        "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
        "            img = resize(img, size)\n",
        "        vid.write(img)\n",
        "    vid.release()\n",
        "    return vid"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZZzHy8XnxaE",
        "cellView": "code"
      },
      "source": [
        "def mean_of_a_numpy_percentile(arr, percentile_cutoff):\n",
        "  # Returns a percentile value of a numpy array\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The average of arr is {np.average(arr)}.')\n",
        "\n",
        "  percentile_value = np.percentile(arr, percentile_cutoff)\n",
        "\n",
        "  # Uses just the percentile without averaging\n",
        "  array_percentile_mean = percentile_value\n",
        "\n",
        "  return (array_percentile_mean)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5E4dnezv1Mf",
        "cellView": "code"
      },
      "source": [
        "#@title Frame_Comparison_SSIM\n",
        "def frame_comparison_ssim(frame_number, frame, width, height, frame_count, engarde_length):\n",
        "\n",
        "  # Determines if the subsequent frame is identical to the current or if there was camera motion\n",
        "  # Uses Mean Square Error and Structural Similarity Index\n",
        "\n",
        "  save_path = r'/content/Mask_RCNN/videos/original/'\n",
        "  image_num = frame_number\n",
        "  image_name1 = str(image_num-1) + '.jpg'\n",
        "  file_name1 = os.path.join(save_path, image_name1)\n",
        "  # file_name2 = os.path.join(save_path, image_name2)\n",
        "\n",
        "  image1 = cv2.imread(file_name1)\n",
        "  image2 = frame\n",
        "\n",
        "  # Uses a tighter crop for engarde positioning to minimize motion outside the bout\n",
        "  if frame_number <= engarde_length:\n",
        "    crop_image1 = image1[int(height*1/5):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*1/5):int(height*3/4), 0:width]\n",
        "  else:\n",
        "    # Removes the bottom of the frame to minimize the effect of overlays and shadowing in the foreground\n",
        "    crop_image1 = image1[int(height*0):int(height*2/4), 0:width]\n",
        "    crop_image2 = image2[int(height*0):int(height*2/4), 0:width]\n",
        "\n",
        "  # Calculate MSE\n",
        "  m = np.linalg.norm(image1 - image2)\n",
        "  \n",
        "  # # If GrayScale\n",
        "  # s = ssim(imageA, imageB)\n",
        "  # If Color\n",
        "  s = ssim(crop_image1, crop_image2, multichannel=True)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Mean Square Error of frame {frame_count} is {m}.')\n",
        "    display(f'The Structural Similarity Index of frame {frame_count} is {s}.')\n",
        "\n",
        "  return(m, s)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jpMiJc_pD9c",
        "cellView": "code"
      },
      "source": [
        "#@title Frame_Comparison\n",
        "def frame_comparison(frame_number, frame, width, height, frame_count, engarde_length):\n",
        "  # Determines if the subsequent frame is identical to the current or if there was camera motion\n",
        "  # By calculating an average Hue from an HSV image. The Hue is then correlated to an average \n",
        "  # color difference between frames.\n",
        "\n",
        "  save_path = r'/content/Mask_RCNN/videos/original/'\n",
        "  image_num = frame_number\n",
        "  # image_name2 = str(image_num) + '.jpg'\n",
        "  image_name1 = str(image_num-1) + '.jpg'\n",
        "  file_name1 = os.path.join(save_path, image_name1)\n",
        "  # file_name2 = os.path.join(save_path, image_name2)\n",
        "\n",
        "  image1 = cv2.imread(file_name1)\n",
        "  image2 = frame\n",
        "\n",
        "  # Uses a tighter crop for engarde positioning to minimize motion outside the bout\n",
        "  if frame_number <= engarde_length:\n",
        "    crop_image1 = image1[int(height*1/5):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*1/5):int(height*3/4), 0:width]\n",
        "  else:\n",
        "    # Removes the bottom of the frame to minimize the effect of overlays and shadowing in the foreground\n",
        "    crop_image1 = image1[int(height*0):int(height*3/4), 0:width]\n",
        "    crop_image2 = image2[int(height*0):int(height*3/4), 0:width]\n",
        "\n",
        "  #Convert to Grayscale and find the Difference\n",
        "  image1_gray = cv2.cvtColor(crop_image1, cv2.COLOR_BGR2GRAY)\n",
        "  image2_gray = cv2.cvtColor(crop_image2, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "\n",
        "  # Finds the HSV of image2\n",
        "  image2_HSV = cv2.cvtColor(image2, cv2.COLOR_BGR2HSV)\n",
        "  h_average = np.average(image2_HSV[0])\n",
        "\n",
        "  # Uses Uncropped Frames\n",
        "  # image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "  # image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
        "  # image_diff = cv2.absdiff(image1_gray,image2_gray)\n",
        "  image_diff = cv2.absdiff(crop_image1,crop_image2)\n",
        "  # image_diff_color = cv2.absdiff(image1,image2)\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The max for the difference of frame {frame_count} is {np.amax(image_diff)}.')\n",
        "    display(f'The average/median for the difference of frame {frame_count} is {np.average(image_diff)}/{np.median(image_diff)}.')\n",
        "    # display(f'The max for the color difference of frame {frame_count} is {np.amax(image_diff_color)}.')\n",
        "    # display(f'The average/median for the color difference of frame {frame_count} is {np.average(image_diff_color)}/{np.median(image_diff_color)}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The shape of image_diff is {image_diff.shape}.')\n",
        "\n",
        "  # display(f'The shape of image_diff is {image_diff_color.shape}.')\n",
        "\n",
        "  # if frame_number <= engarde_length:\n",
        "  #   image_percentile_mean = mean_of_a_numpy_percentile(image_diff, image_percentile)\n",
        "  #   # image_percentile_mean = mean_of_a_numpy_percentile(image_diff_color, image_percentile)\n",
        "  #   display(f'The image percentile mean is {image_percentile_mean} for frame {frame_count}.')\n",
        "  # else:\n",
        "  #   image_percentile_mean = np.average(image_diff)\n",
        "  #   display(f'The image difference average is {np.average(image_diff)} for frame {frame_count}.')\n",
        "\n",
        "  average_image_diff = np.average(image_diff)\n",
        "  if verbose == True:\n",
        "    display(f'The image difference average is {np.average(image_diff)} for frame {frame_count}.')\n",
        "\n",
        "  # average_diff = np.average(image_diff)\n",
        "  # average_diff = np.average(image_diff_color)\n",
        "\n",
        "  return(average_image_diff, h_average)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awUaS8HiKKIK"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fop2haomlTNr",
        "cellView": "code"
      },
      "source": [
        "#@title Test_and_Remove_Duplicate_Frames\n",
        "def test_and_remove_duplicate_frames(file_name, touch_folder, ROOT_DIR, engarde_length):\n",
        "  # Creates a List of unique frames with by comparing the previous and current frames\n",
        "  # This compensates for video compression that may give duplicate frames when FPS is changed\n",
        "\n",
        "  camera_steady = []\n",
        "  engarde_diff_average_arr = np.array([])\n",
        "  engarde_hue_average_arr = np.array([])\n",
        "\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"save\")\n",
        "  VIDEO_ORIG_DIR = os.path.join(VIDEO_DIR, \"original\")\n",
        "  VIDEO_ORIGWORPT_DIR = os.path.join(VIDEO_DIR, \"original_without_repeats\")\n",
        "\n",
        "  display(f'The video directory is {VIDEO_DIR}/{file_name}')\n",
        "  capture = cv2.VideoCapture(os.path.join(VIDEO_DIR, file_name))\n",
        "\n",
        "  total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  if total_frames == 0:\n",
        "    display(f'ERROR: The Video Clip selected has no frames.')\n",
        "  display(f'The total number of frames in the video are: {total_frames}')\n",
        "\n",
        "  width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "  capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
        "  capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "\n",
        "  #Tests for the Same Frames and removes identical frames\n",
        "  frame_count = 0\n",
        "  num_of_deleted_frames = 0\n",
        "  frame_test = True\n",
        "\n",
        "  while True:\n",
        "    ret, frame = capture.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    display(f'frame_count is {frame_count}.')\n",
        "\n",
        "    #Saves an original version of the frame without Regions of Interest\n",
        "    name_orig = '{0}.jpg'.format(frame_count)\n",
        "    name_orig = os.path.join(VIDEO_ORIG_DIR, name_orig)\n",
        "    name_orig_worpt = '{0}.jpg'.format(frame_count - num_of_deleted_frames)\n",
        "    name_orig_worpt = os.path.join(VIDEO_ORIGWORPT_DIR, name_orig_worpt)\n",
        "\n",
        "    if frame_count > 0:\n",
        "      if verbose == True:\n",
        "        display(f'Performing Difference Check for Frame {frame_count}')\n",
        "      # True implies a unique frame while False is a repeat\n",
        "      [average_diff, h_average] = frame_comparison(frame_count, frame, width, height, frame_count, engarde_length)\n",
        "\n",
        "      # [m,s] = frame_comparison_ssim(frame_count, frame, width, height, frame_count, engarde_length)\n",
        "      # average_diff = s\n",
        "      # duplicate_threshold = s/10\n",
        "      # camera_motion_threshold = s*10\n",
        "\n",
        "      # Uses the Engarde Positioning to determine a baseline difference level between frames\n",
        "      if frame_count < engarde_length:\n",
        "        # engarde_diff_average_list.append(average_diff)\n",
        "        engarde_diff_average_arr = np.append(engarde_diff_average_arr, average_diff)\n",
        "        engarde_hue_average_arr = np.append(engarde_hue_average_arr, h_average)\n",
        "\n",
        "      elif frame_count == engarde_length:\n",
        "\n",
        "        engarde_diff_average_arr = np.append(engarde_diff_average_arr, average_diff)\n",
        "        engarde_hue_average_arr = np.append(engarde_hue_average_arr, h_average)\n",
        "\n",
        "        # Emperically Derived Threshold Based on Hue\n",
        "        duplicate_threshold = np.average(engarde_hue_average_arr) * -.037+3.8663\n",
        "        camera_motion_threshold = np.percentile(engarde_diff_average_arr, 40) * camera_motion_threshold_factor\n",
        "                \n",
        "        if verbose == True:\n",
        "          display(f'The Duplicate Threshold at the Engarde Length is {duplicate_threshold}.')\n",
        "          display(f'The Camera Motion Threshold at the Engarde Length is {camera_motion_threshold}.')\n",
        "\n",
        "      elif frame_count > engarde_length:\n",
        "        if average_diff < duplicate_threshold:\n",
        "          if verbose == True:\n",
        "            display(f'The frame {frame_count} is identical to frame {frame_count - 1}.')\n",
        "          frame_test = False\n",
        "        else:\n",
        "          if verbose == True:\n",
        "            display(f'Frame {frame_count} is unique.')\n",
        "          frame_test = True\n",
        "        # display(f'{frame_count} greater than engarde length')\n",
        "\n",
        "    # Saves the Image in Either Original or Original and Without Repeat\n",
        "    # Excludes frames that are Part of the Engarde Positioning\n",
        "    if (frame_test == True) or (frame_count <= engarde_length):\n",
        "      cv2.imwrite(name_orig, frame)\n",
        "      cv2.imwrite(name_orig_worpt, frame)\n",
        "      if frame_count > 0:\n",
        "        camera_steady.append(average_diff)\n",
        "      else:\n",
        "        camera_steady.append(0)\n",
        "    else:\n",
        "      cv2.imwrite(name_orig, frame)\n",
        "      num_of_deleted_frames += 1\n",
        "    frame_count += 1\n",
        "\n",
        "  # Releases the Video Capture\n",
        "  capture.release()\n",
        "\n",
        "  # Saves the new video file over the original\n",
        "  # Directory of images to run detection on\n",
        "  # %cd /content/Mask_RCNN/\n",
        "  os.chdir('/content/Mask_RCNN/')\n",
        "  ROOT_DIR = os.getcwd()\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"original_without_repeats\")\n",
        "  images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
        "  # Sort the images by integer index\n",
        "  images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "  # name = str(iterator) + '.mp4'\n",
        "  # name = str(file_name) + '.mp4'\n",
        "  name = file_name\n",
        "  display(f'The iterator file_name is {name}.')\n",
        "  outvid = os.path.join(VIDEO_DIR, name)\n",
        "  make_video(outvid, images, fps=fps)\n",
        "\n",
        "  return (camera_steady, duplicate_threshold, camera_motion_threshold)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw5PNoa-gmJa",
        "cellView": "code"
      },
      "source": [
        "#@title Process_Video_Clip\n",
        "def process_video_clip(file_name, touch_folder, remove_duplicate_frames, time_stamp_dict):\n",
        "  # Processes the video\n",
        "  \n",
        "  time_stamp_dict[\"process_video\"] = time.time()\n",
        "\n",
        "  display(f'The video file_name is: {file_name}')\n",
        "\n",
        "  # Initiates Conditions\n",
        "  score_box_empty = False\n",
        "  right_torso_empty = False\n",
        "  left_torso_empty = False\n",
        "  left_position_empty = False\n",
        "  right_position_empty = False\n",
        "\n",
        "  os.chdir('/content/Mask_RCNN/')\n",
        "  try:\n",
        "    os.mkdir('videos')\n",
        "  except:\n",
        "    display(f'ERROR creating the video directory')\n",
        "  display(f'os.getcwd() is: {os.getcwd()}')\n",
        "  ROOT_DIR = os.getcwd()\n",
        "  MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"save\")\n",
        "  VIDEO_ORIG_DIR = os.path.join(VIDEO_DIR, \"original\")\n",
        "  VIDEO_ORIGWORPT_DIR = os.path.join(VIDEO_DIR, \"original_without_repeats\")\n",
        "  display(f'The ROOT_DIR is: {ROOT_DIR}')\n",
        "\n",
        "  COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "\n",
        "  display(f'The COCO model path is : {COCO_MODEL_PATH}')\n",
        "\n",
        "  # Removes and recreates the save directory effectively emptying the folder\n",
        "  # Attempts to remove folders to ensure folders are empty\n",
        "  os.chdir('/content/Mask_RCNN/videos')\n",
        "  try:\n",
        "    shutil.rmtree('save')\n",
        "    shutil.rmtree('original')\n",
        "    shutil.rmtree('original_without_repeats')\n",
        "  except:\n",
        "    display(f'Error removing save/original/original_without_repeats folders')\n",
        "  os.mkdir('save')\n",
        "  os.mkdir('original')\n",
        "  os.mkdir('original_without_repeats')\n",
        "\n",
        "  os.chdir('/content/Mask_RCNN')\n",
        "\n",
        "  # Copies the Video from the Video Clip folder \n",
        "  path = '/content/drive/My Drive/projects/fencing/Fencing Clips/' + touch_folder + '/' + file_name\n",
        "  display(f'The path is: {path}')\n",
        "  destination = '/content/Mask_RCNN/videos'\n",
        "  shutil.copy(path, destination)\n",
        "\n",
        "  engarde_length = 10\n",
        "\n",
        "  time_stamp_dict[\"create_directories\"] = time.time()\n",
        "\n",
        "  # Removes Duplicates and Detects Camera motion in Frames\n",
        "  if remove_duplicate_frames == True:\n",
        "    [camera_steady, duplicate_threshold, camera_motion_threshold] = test_and_remove_duplicate_frames(file_name, touch_folder, ROOT_DIR, engarde_length)\n",
        "    display(f'The duplicate frames of video {file_name}.mp4 have been removed')\n",
        "\n",
        "  time_stamp_dict[\"remove_duplicate_frames\"] = time.time()\n",
        "\n",
        "  display(f'The COCO model path is : {COCO_MODEL_PATH}')\n",
        "\n",
        "  if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH) \n",
        "\n",
        "  config = InferenceConfig()\n",
        "  config.display()\n",
        "  # model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n",
        "  model = MaskRCNN(mode='inference', model_dir='./', config=PredictionConfig())\n",
        "\n",
        "  model_path = 'mask_rcnn_bell_guard_cfg_0005.h5'\n",
        "  model.load_weights(model_path, by_name=True)\n",
        "\n",
        "  class_names = ['BG', 'Bell_Guard', 'Score_Box', 'Torso']\n",
        "\n",
        "  capture = cv2.VideoCapture(os.path.join(VIDEO_DIR, file_name))\n",
        "\n",
        "  total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  if total_frames == 0:\n",
        "    display(f'ERROR: The Video Clip selected has no frames.')\n",
        "  display(f'The total number of frames in the video are: {total_frames}')\n",
        "\n",
        "  try:\n",
        "    if not os.path.exists(VIDEO_SAVE_DIR):\n",
        "          os.makedirs(VIDEO_SAVE_DIR)\n",
        "  except OSError:\n",
        "    print ('Error: Creating directory of data')\n",
        "\n",
        "  frames = []\n",
        "  frame_count = 0\n",
        "\n",
        "  width  = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The capture width is: {width}')\n",
        "    display(f'The capture height is: {height}')\n",
        "\n",
        "  capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
        "  capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "\n",
        "  # bbox = []\n",
        "  keypoints = []\n",
        "  fencer_data = []\n",
        "  Left_Position = []\n",
        "  Right_Position = []\n",
        "  Scoring_Box_Position = []\n",
        "  Left_Torso_Position = []\n",
        "  Right_Torso_Position = []\n",
        "  left_torso_size_average = []\n",
        "  right_torso_size_average = []\n",
        "  Tracking_Bounding_Boxes = []\n",
        "  All_Bell_Guard_Positions = []\n",
        "  Exclusion_Areas = []\n",
        "  \n",
        "  # t0 = time.time()\n",
        "\n",
        "  skip_frames = False\n",
        "  skip_frame_counter = 0\n",
        "  frames_to_skip = 300\n",
        "  number_of_frames_skipped = 0\n",
        "  # Assumes any video clip of greater than 10000 frames may require multiple runs\n",
        "  if total_frames > 10000:\n",
        "    clip_vector_previous = load_clip_vector()\n",
        "    frame_count = len(clip_vector_previous)\n",
        "  else:\n",
        "    clip_vector_previous = []\n",
        "  \n",
        "  # Continues until it breaks by not finding a return from attempting to read a frame capture\n",
        "  time_stamp_dict[\"frame_processing_start\"] = time.time()\n",
        "\n",
        "  while True:\n",
        "    # if (frame_count%20) == 0:\n",
        "      # t1 = time.time()\n",
        "      # display(f'Processing frame {frame_count} of {total_frames}. Time elapsed {hms_string(t1 - t0)}.')\n",
        "\n",
        "    if (frame_count%2000) == 0 and (frame_count != 0):\n",
        "      display(f'Saving Clip Progress...')\n",
        "      save_clip_progress(bbox, frame_count, width, height, clip_vector_previous)\n",
        "    #Creates Bounding Box List\n",
        "    if frame_count == int(total_frames/2):\n",
        "      time_stamp_dict[\"Before_Reading_a_Frame\"] = time.time()\n",
        "    ret, frame = capture.read()\n",
        "    if frame_count == int(total_frames/2):\n",
        "      time_stamp_dict[\"After_Reading_a_Frame\"] = time.time()\n",
        "    #Excludes the Tracking Boxes for the Engarde Length    \n",
        "    if not ret:\n",
        "      break\n",
        "    # Save each frame of the video to a list\n",
        "    frame_count += 1\n",
        "    if skip_frames == False:\n",
        "      # t10 = time.time()\n",
        "      frames = [frame]\n",
        "\n",
        "      # Runs the Detection Model for Bellguard, Torso, Scoring_Box\n",
        "      if frame_count == int(total_frames/2):\n",
        "        time_stamp_dict[\"Before_Getting_Frame_Results\"] = time.time()\n",
        "      results = model.detect(frames, verbose=0)\n",
        "      if frame_count == int(total_frames/2):\n",
        "        time_stamp_dict[\"After_Getting_Frame_Results\"] = time.time()\n",
        "      # Runs the Human Pose Analysis\n",
        "      if frame_count == int(total_frames/2):\n",
        "        time_stamp_dict[\"Before_Human_Pose_Results\"] = time.time()\n",
        "      [fencer_data_temp, keypoints_temp] = human_pose_analysis(frame)\n",
        "      fencer_data.append(fencer_data_temp)\n",
        "      keypoints.append(keypoints_temp)\n",
        "      if frame_count == int(total_frames/2):\n",
        "        time_stamp_dict[\"After_Human_Pose_Results\"] = time.time()\n",
        "\n",
        "      if frame_count == int(total_frames/2):\n",
        "        time_stamp_dict[\"Before_Combining_and_Saving_BBox_Results\"] = time.time()\n",
        "\n",
        "      for i, item in enumerate(zip(frames, results)):\n",
        "        frame = item[0]\n",
        "        r = item[1]        \n",
        "        name = '{0}.jpg'.format(frame_count + i - 1)\n",
        "        #Saves an original version of the frame without Regions of Interest\n",
        "        name_orig = os.path.join(VIDEO_ORIG_DIR, name)\n",
        "        cv2.imwrite(name_orig, frame)\n",
        "\n",
        "        # Format: display_instances(image, boxes, masks, ids, names, scores):\n",
        "        frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'], file_name)\n",
        "\n",
        "        #Saves the farme as an image with ids overlayed\n",
        "        name = os.path.join(VIDEO_SAVE_DIR, name)\n",
        "        cv2.imwrite(name, frame)\n",
        "\n",
        "        #Captures the Bounding Box data in bbox\n",
        "        bbox = []\n",
        "        for j in range(len(r['rois'])):\n",
        "          bbox.append([r['rois'][j],r['scores'][j],r['class_ids'][j]])\n",
        "        a = r['class_ids'].tolist()\n",
        "\n",
        "      if frame_count == int(total_frames/2):\n",
        "        time_stamp_dict[\"After_Combining_and_Saving_BBox_Results\"] = time.time()\n",
        "\n",
        "      # t11 = time.time()\n",
        "      # Displays the time required to process a given frame\n",
        "      # display(f'The time to process frame {frame_count} is {hms_string(t11 - t10)}.')\n",
        "\n",
        "      if frame_count <= engarde_length:\n",
        "        # if frame_count == 1:\n",
        "        #   t2 = time.time()\n",
        "        if frame_count == int(engarde_length/2):\n",
        "          time_stamp_dict[\"Before_Engarde_Positioning_of_a_Frame\"] = time.time()\n",
        "\n",
        "        [Left_Position_Temp, Right_Position_Temp, Scoring_Box_Position_Temp, scoring_box_size_average, Tracking_Bounding_Boxes_Temp, \\\n",
        "          Left_Torso_Position_Temp, Right_Torso_Position_Temp, left_torso_size_average_Temp, right_torso_size_average_Temp, All_Bell_Guard_Positions_Temp] \\\n",
        "          = engarde_position(bbox, width, height, engarde_length, frame_count-1)\n",
        "\n",
        "        Left_Position.append(Left_Position_Temp)\n",
        "        if len(Left_Position_Temp) < 2 and frame_count > (engarde_length - 2):\n",
        "          # display(f'The Fencer Data Temp is:')\n",
        "          # display(fencer_data_temp)\n",
        "          if verbose == True:\n",
        "            display(f'Left Bell Guard has few detections during engarde, appending human pose position {[fencer_data_temp[1][0][0],fencer_data_temp[1][0][1]] }.')\n",
        "          Left_Position.append([fencer_data_temp[1][0][0],fencer_data_temp[1][0][1]])\n",
        "        Right_Position.append(Right_Position_Temp)\n",
        "        if len(Right_Position_Temp) < 2 and frame_count > (engarde_length - 2):\n",
        "          if verbose == True:\n",
        "            display(f'Right Bell Guard has few detections during engarde, appending human pose position {[fencer_data_temp[0][0][0],fencer_data_temp[0][0][1]]}.')\n",
        "          Left_Position.append([fencer_data_temp[0][0][0],fencer_data_temp[0][0][1]])\n",
        "        Scoring_Box_Position.append(Scoring_Box_Position_Temp)\n",
        "        Left_Torso_Position.append(Left_Torso_Position_Temp)\n",
        "        Right_Torso_Position.append(Right_Torso_Position_Temp)\n",
        "        left_torso_size_average.append(left_torso_size_average_Temp)\n",
        "        right_torso_size_average.append(right_torso_size_average_Temp)\n",
        "        if verbose == True:\n",
        "          display(f'At frame {frame_count} the tracking Bounding Boxes Temp is:')\n",
        "          display(Tracking_Bounding_Boxes_Temp)\n",
        "        Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "        # Creates a List of each Bell Guard Position in each engarde positioning frame. Used for excluding false positives\n",
        "        All_Bell_Guard_Positions.append(All_Bell_Guard_Positions_Temp)\n",
        "\n",
        "        if frame_count == int(engarde_length/2):\n",
        "          time_stamp_dict[\"After_Engarde_Positioning_of_a_Frame\"] = time.time()\n",
        "\n",
        "      # Displays the time to process the engarde positioning frames\n",
        "      # display(f'Processing frame {frame_count} of {total_frames}. Time elapsed {hms_string(t1 - t0)}.')\n",
        "\n",
        "      if frame_count == engarde_length:\n",
        "\n",
        "        time_stamp_dict[\"Before_Engarde_Length\"] = time.time()\n",
        "\n",
        "        # display(f'Time elapsed processing the engarde positions: {hms_string(t2 - t0)}.')\n",
        "        if verbose == True:\n",
        "          display(f'Commencing the Engarde Length Processing.')\n",
        "        tracked_items = [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average]\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'The Scoring Box Position is:')\n",
        "          display(Scoring_Box_Position)\n",
        "        if max(Scoring_Box_Position) == []:\n",
        "          if verbose == True:\n",
        "            display(f'The Scoring Box Position was empty so a default was used.')\n",
        "          score_box_empty = True\n",
        "          tracked_items[2] = [[width/2, height/2], [width/2, height/2], [width/2, height/2]]\n",
        "\n",
        "        # Tests for empty Positions\n",
        "        if max(Right_Torso_Position) == []:\n",
        "          right_torso_empty = True\n",
        "\n",
        "        if max(Left_Torso_Position) == []:\n",
        "          left_torso_empty = True\n",
        "\n",
        "        if max(Left_Position) == [] and max(Left_Torso_Position) != []:\n",
        "          left_position_empty = True\n",
        "\n",
        "        if max(Right_Position) == [] and max(Right_Torso_Position) != []:\n",
        "          right_position_empty = True\n",
        "        \n",
        "        # Replaces [0,0] with [] for torso width and height\n",
        "        for k in range(len(tracked_items)):\n",
        "          for j in range(len(tracked_items[k])):\n",
        "            if tracked_items[k][j] == [0,0]:\n",
        "              tracked_items[k][j] = []\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'The tracked items k are [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position, left_torso_size_average, right_torso_size_average].')\n",
        "        for k in range(len(tracked_items)):\n",
        "          if verbose == True:\n",
        "            display(f'k is {k}.')\n",
        "            display(tracked_items[k])\n",
        "          try:\n",
        "            tracked_items[k] = [item for item in tracked_items[k] if item != []]\n",
        "            tracked_items[k] = np.hstack(tracked_items[k])\n",
        "            if verbose == True:\n",
        "              display(f'The length of len(tracked_items[k]) is {int(len(tracked_items[k])/2)}.')\n",
        "            tracked_items[k] = tracked_items[k].reshape(int(len(tracked_items[k])/2), 2)\n",
        "            tracked_items[k] = np.median(tracked_items[k], axis = 0)\n",
        "            tracked_items[k] = [int(tracked_items[k][0]), int(tracked_items[k][1])]\n",
        "            if verbose == True:\n",
        "              display(f'The tracked item position {tracked_items[k]}.')\n",
        "          except:\n",
        "            if verbose == True:\n",
        "              display(f'Failure to detect the tracked item {k} during the engarde positioning.')\n",
        "              display(tracked_items[k])\n",
        "            tracked_items[k] = [0,0]\n",
        "\n",
        "        if right_torso_empty == True:\n",
        "          if verbose == True:\n",
        "            display(f'The Right Torso Position was empty so a default based on the Right BellGuard was used.')\n",
        "            display(f'tracked_items[1] is {tracked_items[1]}, left_torso_size_average is {left_torso_size_average}.')\n",
        "          torso_position_default = [tracked_items[1][0] + int(left_torso_size_average[0][0]*3/4), tracked_items[1][1] - int(left_torso_size_average[0][1]/4)]\n",
        "          tracked_items[4] = torso_position_default\n",
        "          if verbose == True:\n",
        "            display(f'As a comparison, the Score Box Position is:')\n",
        "            display(Scoring_Box_Position)\n",
        "            display(f'The Right Torso Position is:')\n",
        "            display(Right_Torso_Position)\n",
        "            display(f'As a comparison, the tracked_items[2] is:')\n",
        "            display(tracked_items[2])\n",
        "            display(f'The tracked_items[4] is:')\n",
        "            display(tracked_items[4])\n",
        "\n",
        "        if left_torso_empty == True:\n",
        "          if verbose == True:\n",
        "            display(f'The Left Torso Position was empty so a default based on the Left BellGuard was used.')\n",
        "            display(f'tracked_items[0] is {tracked_items[0]}, right_torso_size_average is {right_torso_size_average}.')\n",
        "          torso_position_default = [tracked_items[0][0] - int(right_torso_size_average[0][0]*3/4), tracked_items[0][1] - int(right_torso_size_average[0][1]/4)]\n",
        "          tracked_items[3] = torso_position_default\n",
        "          if verbose == True:\n",
        "            display(f'As a comparison, the Score Box Position is:')\n",
        "            display(Scoring_Box_Position)\n",
        "            display(f'The Right Torso Position is:')\n",
        "            display(Right_Torso_Position)\n",
        "            display(f'As a comparison, the tracked_items[2] is:')\n",
        "            display(tracked_items[2])\n",
        "            display(f'The tracked_items[4] is:')\n",
        "            display(tracked_items[4])\n",
        "\n",
        "        if left_position_empty == True:\n",
        "          if verbose == True:\n",
        "            display(f'The Left  Position was empty so a default based on the Left Torso was used.')\n",
        "          left_torso_size_average_temp = average_list_without_null(left_torso_size_average)\n",
        "          left_position_default = [int(tracked_items[3][0] + left_torso_size_average_temp[0]), int(tracked_items[3][1] + left_torso_size_average_temp[0]/4)]\n",
        "          # left_position_default = [tracked_items[3][0] + int(left_torso_size_average[0][0]), tracked_items[3][1] + int(left_torso_size_average[0][0]/4)]\n",
        "          tracked_items[0] = left_position_default\n",
        "\n",
        "        if right_position_empty == True:\n",
        "          if verbose == True:\n",
        "            display(f'The Right  Position was empty so a default based on the Right Torso was used.')\n",
        "            display(f'The tracked_items[4] is {tracked_items[4]}.')\n",
        "            display(f'The right_torso_size_average is {right_torso_size_average}.')\n",
        "          right_torso_size_average_temp = average_list_without_null(right_torso_size_average)\n",
        "          right_position_default = [int(tracked_items[4][0] - right_torso_size_average_temp[0]), int(tracked_items[4][1] + right_torso_size_average_temp[0]/4)]\n",
        "          tracked_items[1] = right_position_default\n",
        "\n",
        "        [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position] = [[],[],[],[],[]]\n",
        "\n",
        "        # Builds the Positions of the Tracked Items\n",
        "        for i in range(engarde_length):\n",
        "          Left_Position.append(tracked_items[0])\n",
        "          Right_Position.append(tracked_items[1])\n",
        "          Scoring_Box_Position.append(tracked_items[2])\n",
        "          Left_Torso_Position.append(tracked_items[3])\n",
        "          Right_Torso_Position.append(tracked_items[4])\n",
        "\n",
        "        # Sorts through All Bellguard detections and defines the exclusion areas.\n",
        "        # display(f'The Left Position is: {Left_Position}')\n",
        "        # within_Left_Region = False\n",
        "        # within_Right_Region = False\n",
        "\n",
        "        padding = width/24\n",
        "        exclusion_box = [padding,padding,padding,padding]\n",
        "        Left_Exculsion_Box = create_boundary_box(Left_Position[0],exclusion_box, False)\n",
        "        Right_Exculsion_Box = create_boundary_box(Right_Position[0],exclusion_box, False)\n",
        "\n",
        "        for i in range(len(All_Bell_Guard_Positions)):\n",
        "          for j in range(len(All_Bell_Guard_Positions[i])):\n",
        "            within_Left_Region = boundary_box_test(All_Bell_Guard_Positions[i][j], Left_Exculsion_Box)\n",
        "            within_Right_Region = boundary_box_test(All_Bell_Guard_Positions[i][j], Right_Exculsion_Box)\n",
        "            if within_Left_Region == False and within_Right_Region == False:\n",
        "              Exclusion_Areas.append(All_Bell_Guard_Positions[i][j])\n",
        "\n",
        "        # Removes duplicate Exclusion Areas that are within a given distance of each other\n",
        "        Exclusion_Areas = exclusion_area_simplification_recursion(Exclusion_Areas, width/80)\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'The Exclusion areas are:')\n",
        "          display(Exclusion_Areas)\n",
        "\n",
        "        left_torso_size_average = tracked_items[5]\n",
        "        if right_torso_empty == False:\n",
        "          right_torso_size_average = tracked_items[6]\n",
        "        elif right_torso_empty == True:\n",
        "          right_torso_size_average = tracked_items[5]\n",
        "        torso_size = [left_torso_size_average, right_torso_size_average]\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'The left_torso_size_average is {left_torso_size_average} and the right_torso_size_average is {right_torso_size_average}.')\n",
        "\n",
        "        time_stamp_dict[\"After_Engarde_Length\"] = time.time()\n",
        "\n",
        "      if frame_count > engarde_length:\n",
        "\n",
        "        if frame_count == int(total_frames/2):\n",
        "          time_stamp_dict[\"Before_Processing_of_a_Post_Engarde_Frame\"] = time.time()\n",
        "\n",
        "        positions = [Left_Position, Right_Position, Scoring_Box_Position, Left_Torso_Position, Right_Torso_Position]\n",
        "\n",
        "        # Sets the certainty following the engarde positioning\n",
        "        if frame_count == engarde_length + 1:\n",
        "          t3 = time.time()\n",
        "          certainty = [0,0,0,0,0]\n",
        "          if verbose == True:\n",
        "            display(f'The positions following the engarde positioning are:')\n",
        "            display(positions)\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'Certainty just prior to Bell Guard Positioning is {certainty}.')\n",
        "\n",
        "        previous_certainty = certainty\n",
        "\n",
        "        if right_torso_size_average[0] == 0 and left_torso_size_average[0] == 0:\n",
        "          if verbose == True:\n",
        "            display(f'Error, both Torso sizes are zero.')   \n",
        "        elif right_torso_size_average[0] == 0:\n",
        "          if verbose == True:\n",
        "            display(f'Right Torso Size was zero, using the Left as a Defualt.')\n",
        "          right_torso_size_average = left_torso_size_average\n",
        "        elif left_torso_size_average[0] == 0:\n",
        "          if verbose == True:\n",
        "            display(f'Left Torso Size was zero, using the Right as a Defualt.')\n",
        "          left_torso_size_average = right_torso_size_average\n",
        "\n",
        "        # Finds the Tracked Items and Returns their positions\n",
        "        [Left_Position_Temp, Right_Position_Temp, Scoring_Box_Position_Temp, Tracking_Bounding_Boxes_Temp, \\\n",
        "         Left_Torso_Position_Temp, Right_Torso_Position_Temp, engarde_length, certainty] \\\n",
        "         = Bell_Guard_Position_Finding(bbox, width, height, fencer_data_temp, positions, frame_count, \\\n",
        "         left_torso_size_average, right_torso_size_average, engarde_length, certainty, camera_steady, camera_motion_threshold, Exclusion_Areas)\n",
        "\n",
        "        if verbose == True:\n",
        "          display(f'Certainty just after to Bell Guard Positioning is {certainty}.')\n",
        "          display(f'The Left Position at frame {frame_count - 1} is {Left_Position_Temp}.')\n",
        "\n",
        "        # Appends the Returned Positions\n",
        "        Left_Position.append(Left_Position_Temp)\n",
        "        Right_Position.append(Right_Position_Temp)\n",
        "        Scoring_Box_Position.append(Scoring_Box_Position_Temp)\n",
        "        Left_Torso_Position.append(Left_Torso_Position_Temp)\n",
        "        Right_Torso_Position.append(Right_Torso_Position_Temp)\n",
        "        Tracking_Bounding_Boxes.append(Tracking_Bounding_Boxes_Temp)\n",
        "\n",
        "        # Tests for a change in certainty to zero from non-zero. If a position has become certain during\n",
        "        # this frame then it back calculates previous uncertain position up to a certain position.\n",
        "        if (certainty[0] == 0 and previous_certainty[0] != 0):\n",
        "          Left_Position = position_linear_approximation(Left_Position, previous_certainty[0])\n",
        "          if verbose == True:\n",
        "            display(f'Using a Linear Approximation for frame {frame_count} for the Left Bellguard Position.')\n",
        "        elif (certainty[1] == 0 and previous_certainty[1] != 0):\n",
        "          if verbose == True:\n",
        "            display(f'The Right Position is: {Right_Position}.')\n",
        "            display(f'The Previous Certainty is: {previous_certainty[1]}')\n",
        "            display(f'Using a Linear Approximation for frame {frame_count} for the Right Bellguard Position.')\n",
        "          Right_Position = position_linear_approximation(Right_Position, previous_certainty[1])\n",
        "        elif (certainty[2] == 0 and previous_certainty[2] != 0):\n",
        "          Scoring_Box_Position = position_linear_approximation(Scoring_Box_Position, previous_certainty[2])\n",
        "        elif (certainty[3] == 0 and previous_certainty[3] != 0):\n",
        "          Left_Torso_Position = position_linear_approximation(Left_Torso_Position, previous_certainty[3])\n",
        "        elif (certainty[4] == 0 and previous_certainty[4] != 0):\n",
        "          Right_Torso_Position = position_linear_approximation(Right_Torso_Position, previous_certainty[4])\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if frame_count == int(total_frames/2):\n",
        "          time_stamp_dict[\"After_Processing_of_a_Post_Engarde_Frame\"] = time.time()      \n",
        "\n",
        "\n",
        "  time_stamp_dict[\"frame_processing_end\"] = time.time()\n",
        "\n",
        "  # t4 = time.time()\n",
        "  if verbose == True:\n",
        "    # display(f'Time elapsed to process the engarde frames is  {hms_string(t3 - t2)}.')\n",
        "    # display(f'Time elapsed to process the post engarde frames is  {hms_string(t4 - t3)}.')\n",
        "    # display(f'Time elapsed processing the clip the total clip is {hms_string(t4 - t0)}.')\n",
        "    # Reduces the Frame Count to account for skipped frames\n",
        "    display(f'The original frame count was: {frame_count - 1} and the number of frames skipped is: {number_of_frames_skipped}.')\n",
        "  frame_count = len(bbox)\n",
        "  if verbose == True:\n",
        "    display(f'The length of the frame_count is {frame_count - 1} while the number of bboxes is {len(bbox)}.')\n",
        "\n",
        "  file_to_remove = r'/Mask_RCNN/videos/' + file_name\n",
        "  # Removes the File if it already exists\n",
        "  # !rm $file_to_remove\n",
        "  try:\n",
        "    shutil.rmtree(file_to_remove)\n",
        "  except:\n",
        "    if verbose == True:\n",
        "      display(f'ERROR removing the video file to analyze.')\n",
        "\n",
        "  capture.release()\n",
        "\n",
        "  time_stamp_dict[\"after_capture_release\"] = time.time()\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The Left Position just prior to drawing the Bell_Guards is:')\n",
        "    display(Left_Position)\n",
        "    display(f'The Right Position just prior to drawing the Bell_Guards is:')\n",
        "    display(Right_Position)\n",
        "\n",
        "  time_stamp_dict[\"before_generate_simple_representative\"] = time.time()\n",
        "\n",
        "  if generate_representative == False:\n",
        "    # Allows for a simple clip vector to draw in the lights if an overlay is used instead of a representative clip\n",
        "    simple_clip_vector = simple_clip_vector_generator(Left_Position, Right_Position, width)\n",
        "  else:\n",
        "    simple_clip_vector = 'None'\n",
        "\n",
        "  time_stamp_dict[\"after_generate_simple_representative\"] = time.time()\n",
        "\n",
        "  time_stamp_dict[\"before_light_comparison\"] = time.time()\n",
        "\n",
        "  # t5 = time.time()\n",
        "  #Draws the Boxes on the image frame and determines scoring lights turned on\n",
        "  [left_light_comparison, right_light_comparison] = draw_Bell_Guard_Position(Left_Position, Right_Position, Scoring_Box_Position, \\\n",
        "    scoring_box_size_average, Left_Torso_Position, Right_Torso_Position, frame_count, Tracking_Bounding_Boxes, \\\n",
        "    video_filename, width, height, engarde_length, keypoints, score_box_empty, camera_steady, camera_motion_threshold, Exclusion_Areas, simple_clip_vector)\n",
        "  # t6 = time.time()\n",
        "  # display(f'Time elapsed while drawing the Bell_Guard positions is {hms_string(t6 - t5)}.')\n",
        "\n",
        "  time_stamp_dict[\"after_light_comparison\"] = time.time()\n",
        "\n",
        "  if camera_motion_compensate == True and score_box_empty == False:\n",
        "    #Adjusts the Bellguard Position Based on the Camera motion as determined by the Score_Box Position\n",
        "    Left_Position = camera_motion_adjustment(Left_Position, Scoring_Box_Position)\n",
        "    Right_Position = camera_motion_adjustment(Right_Position, Scoring_Box_Position)\n",
        "\n",
        "  # t7 = time.time()\n",
        "  # display(f'Time elapsed for the camera motion adjustment is {hms_string(t7 - t6)}.')\n",
        "\n",
        "  if camera_motion_compensate == True and score_box_empty == False:\n",
        "    #Adjusts Left and Right Position for convenient visualization\n",
        "    [Left_Position, Right_Position] = position_down_scale(Left_Position, Right_Position, width, height)\n",
        "\n",
        "  # t8 = time.time()\n",
        "  #Creates a vector representing the clip, format [left_x, right_x, left_lights, right_lights]\n",
        "\n",
        "  time_stamp_dict[\"before_clip_vector_generation\"] = time.time()\n",
        "\n",
        "  clip_vector = clip_vector_generator(Left_Position, Right_Position, left_light_comparison, right_light_comparison, clip_vector_previous, width)\n",
        "\n",
        "  time_stamp_dict[\"after_clip_vector_generation\"] = time.time()\n",
        "\n",
        "  if smooth_video_clip == True:\n",
        "    #Smoothes the Clip using Savitzky–Golay filter\n",
        "    clip_vector = smooth_clip_vector(clip_vector, engarde_length)\n",
        "\n",
        "  # t9 = time.time()\n",
        "  # display(f'Time elapsed to generate the clip vector is {hms_string(t9 - t8)}.')\n",
        "\n",
        "  if verbose == True:\n",
        "    display(f'The final clip vector is:')\n",
        "    display(clip_vector)\n",
        "\n",
        "  return (bbox, frame_count, width, height, clip_vector_previous, fencer_data, keypoints, clip_vector, fps, time_stamp_dict)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkHx2cfqpmyM",
        "cellView": "code"
      },
      "source": [
        "#@title Prepare_Video_Frames\n",
        "def prepare_video_frames(img_directory, video_title):\n",
        "\n",
        "  ROOT_DIR = '/content/Mask_RCNN/'\n",
        "  VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "  VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, img_directory)\n",
        "  images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
        "  # Sort the images by integer index\n",
        "  images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "  name = str(iterator) + '.' + video_title + '.mp4'\n",
        "  outvid = os.path.join(VIDEO_DIR, name)\n",
        "\n",
        "  return (outvid, images)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBlM2EDSieV",
        "cellView": "code"
      },
      "source": [
        "#@title Downsample_FPS\n",
        "def downsample_fps(a,b):\n",
        "  # Adjusts the elements of a larger set a to fit into the length of set b\n",
        "\n",
        "  c = []\n",
        "  remainder = 0\n",
        "  for i in range(len(b)):\n",
        "    c_temp = []\n",
        "    if verbose == True:\n",
        "      display(f'The lower range is {math.ceil(len(a)/len(b)*(i+1)-1-remainder)} and the upper range is {math.floor(len(a)/len(b)*(i+1))}.')\n",
        "    for j in range(math.ceil(len(a)/len(b)*(i)-remainder),math.floor(len(a)/len(b)*(i+1))):\n",
        "      c_temp.append(a[j])\n",
        "      if verbose == True:\n",
        "        display(f'i,j = {i},{j} and c_temp = {c_temp}')\n",
        "    remainder = (len(a)/len(b))*(i+1) - int(len(a)/len(b)*(i+1))\n",
        "    c.append(round(sum(c_temp)/len(c_temp)))\n",
        "    if verbose == True:\n",
        "      display(f'The remainder at i = {i} and j = {j} is {remainder} and c is {c}.')\n",
        "\n",
        "  return (c)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FDqmsziTCBC",
        "cellView": "code"
      },
      "source": [
        "#@title Load_Clip\n",
        "def load_clip(folder, clip_number, max_length, capture_width):\n",
        "  if folder == 'Left' or folder == 'left' or folder == 'Left_Touch':\n",
        "    folder = 0\n",
        "  if folder == 'Right' or folder == 'right'or folder == 'Right_Touch':\n",
        "    folder = 1\n",
        "  if folder == 'Simul' or folder == 'simul'or folder == 'Simul':\n",
        "    folder = 2\n",
        "\n",
        "  touch_folder = ['Left_Touch', 'Right_Touch', 'Simul']\n",
        "\n",
        "  i = folder\n",
        "\n",
        "  file = 'clip_vector_acceleration_np' + str(clip_number) + '.csv'\n",
        "  path = r'/content/drive/My Drive/projects/fencing/Fencing Clips/' + touch_folder[i] + '/' + touch_folder[i] + '_Vector_Clips_Acceleration/'\n",
        "\n",
        "  vector_data = pd.read_csv(os.path.join(path, file), header=None)\n",
        "  clip_vector = vector_data.to_numpy(dtype = np.float32)\n",
        "\n",
        "  display(os.path.join(path, file))\n",
        "\n",
        "  # Pads the clip_vector to 103\n",
        "  # If the clip is greater than Max Length, it is truncated\n",
        "  if len(clip_vector) > max_length:\n",
        "    clip_vector = clip_vector[len(clip_vector) - max_length:]\n",
        "  padding = np.array([0,0,0,0])\n",
        "  for k in range(max_length - (len(clip_vector))):\n",
        "    clip_vector = np.vstack((clip_vector, padding))\n",
        "\n",
        "  #Normalizes the Values\n",
        "  # max_value = 31\n",
        "  max_value = int(capture_width/42)\n",
        "  for i in range(len(clip_vector)):\n",
        "    for j in range(2):\n",
        "      if clip_vector[i][j] < max_value:\n",
        "        clip_vector[i][j] = clip_vector[i][j] * (1/max_value)\n",
        "      else:\n",
        "        #Preserves the sign of the value\n",
        "        clip_vector[i][j] = clip_vector[i][j]/(abs(clip_vector[i][j]))\n",
        "\n",
        "  # Removes the First 15 frames to minimize engarde positioning\n",
        "  clip_vector = clip_vector[15:]\n",
        "\n",
        "  # Sets Clip_Vector to Zero if Light is on\n",
        "  for j in range(len(clip_vector)):\n",
        "    if clip_vector[j][2] == 1:\n",
        "      clip_vector[j][0] = 0\n",
        "    if clip_vector[j][3] == 1:\n",
        "      clip_vector[j][1] = 0 \n",
        "\n",
        "  clip_vector = clip_vector.reshape(1,clip_vector_length,4)\n",
        "  return (clip_vector)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6yO3gTSHox",
        "cellView": "code"
      },
      "source": [
        "#@title Create_Folder_Hierarchy\n",
        "def create_folder_hierarchy(file_name):\n",
        "\n",
        "  # Creates File Path in Google Drive\n",
        "  # !mkdir -p '/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips'\n",
        "  # try:\n",
        "  if not exists('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips'):    \n",
        "    os.makedirs('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips')\n",
        "  # except:\n",
        "  else:\n",
        "    display(f'The Left Touch Vector Clips directory already exists.')\n",
        "    # display(f'ERROR creating the Left_Touch_Vector_Clips')\n",
        "  # %cd '/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "  os.chdir('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/')\n",
        "  # !mkdir Left_Touch_Vector_Clips_Speed\n",
        "  # try:\n",
        "  if not exists('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips_Speed'):\n",
        "    os.mkdir('Left_Touch_Vector_Clips_Speed')\n",
        "  # except:\n",
        "  else:\n",
        "    display(f'The Left Touch Vector Clips Speed directory already exists.')\n",
        "    # display(f'ERROR creating the Left_Touch_Vector_Clips_Speed')\n",
        "  # !mkdir Left_Touch_Vector_Clips_Acceleration\n",
        "  # try:\n",
        "  if not exists('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips_Acceleration'):\n",
        "    os.mkdir('Left_Touch_Vector_Clips_Acceleration')\n",
        "  # except:\n",
        "  else:\n",
        "    display(f'The Left Touch Vector Clips Acceleration directory already exists.')\n",
        "    # display(f'ERROR creating the Left_Touch_Vector_Clips_Speed')\n",
        "\n",
        "  # # Only copies the file \n",
        "  # if youtube_link == '':\n",
        "  #   # Copies Video File to Left_Touch Folder\n",
        "  #   file_name = r'/content/drive/My Drive/' + file_name\n",
        "  #   destination = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "  #   # !cp {file_name} {destination}\n",
        "  #   shutil.copy(file_name, destination)\n",
        "\n",
        "  return"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ8y96lwZccc",
        "cellView": "code"
      },
      "source": [
        "#@title Evaluate_Model\n",
        "def evaluate_model(dataset, model, cfg):\n",
        "  # calculate the mAP for a model on a given dataset\n",
        "  APs = list()\n",
        "  for image_id in dataset.image_ids:\n",
        "\t\t# load image, bounding boxes and masks for the image id\n",
        "    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n",
        "\t\t# convert pixel values (e.g. center)\n",
        "    scaled_image = mold_image(image, cfg)\n",
        "\t\t# convert image into one sample\n",
        "    sample = expand_dims(scaled_image, 0)\n",
        "\t\t# make prediction\n",
        "    yhat = model.detect(sample, verbose=0)\n",
        "\t\t# extract results for first sample\n",
        "    r = yhat[0]\n",
        "\t\t# calculate statistics, including AP\n",
        "    AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
        "\t\t# store\n",
        "    APs.append(AP)\n",
        "\t# calculate the mean AP across all images\n",
        "  mAP = mean(APs)\n",
        "  return mAP"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzHw3Z3Z2NLb"
      },
      "source": [
        "def youtube_clip(link):\n",
        "\n",
        "  !pip install youtube_dl\n",
        "  import youtube_dl\n",
        "\n",
        "  os.chdir('/content/')\n",
        "\n",
        "  #Uses the YouTube Query to get the time in seconds of the position in the clip\n",
        "  youtube_clip_position = int(link.split('=')[1])\n",
        "\n",
        "  ydl_opts = {}\n",
        "\n",
        "  #Gets info for the YouTube clip\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    info_dict = ydl.extract_info(link, download=False)\n",
        "    video_title = info_dict.get('title', None)\n",
        "    file_size = info_dict.get('formats',None)[0]['filesize']\n",
        "    duration = info_dict.get('duration',None)\n",
        "\n",
        "  #Determines where to save the YouTube clip\n",
        "  # path = f'./{video_title}.mp4'\n",
        "  path = f'./video_title.mp4'\n",
        "  ydl_opts.update({'outtmpl':path})\n",
        "\n",
        "  #Downloads and saves the YouTube clip\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([link])\n",
        "\n",
        "  !ffmpeg -y -ss $youtube_clip_position -i video_title.mp4 -c:v libx264 -c:a aac -frames:v 100 999.mp4\n",
        "\n",
        "  file_name = '999.mp4'\n",
        "  destination = r'/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/'\n",
        "  file_name_with_path = os.path.join(destination, file_name)\n",
        "  #Copies the clip to the Left Touch Folder. Tests for file already existing\n",
        "  if exists(file_name_with_path):\n",
        "    os.remove(file_name_with_path)\n",
        "  shutil.copy(file_name, destination)\n",
        "\n",
        "  return"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac1bWiP8DDF2",
        "cellView": "code"
      },
      "source": [
        "#@title Bell_GuardDataset\n",
        "class Bell_GuardDataset(Dataset):\n",
        "  #class that defines and loads the Bell_Guard dataset\n",
        "\t# load the dataset definitions\n",
        "  def load_dataset(self, dataset_dir, is_train=True):\n",
        "\t\t# define the two classes\n",
        "    self.add_class(\"dataset\", 1, \"bellguard\")\n",
        "    self.add_class(\"dataset\", 2, \"scorebox\")\n",
        "    self.add_class(\"dataset\", 3, \"torso\")\n",
        "    # self.add_class(\"dataset\", 4, \"person\")\n",
        "\n",
        "    # Adds the Images to be Analyzed\n",
        "\t\t# Defines data locations\n",
        "    images_dir = dataset_dir + '/images/'\n",
        "    annotations_dir = dataset_dir + '/annots/'\n",
        "\t\t# Finds all images\n",
        "    for filename in listdir(images_dir):\n",
        "      # Extracts image id\n",
        "      image_id = filename[5:-4]\n",
        "\t\t\t# Skips all images after the training set\n",
        "      if is_train and int(image_id) >= train_set_number:\n",
        "        continue\n",
        "\t\t\t# Skips all images before the training set\n",
        "      if not is_train and int(image_id) < train_set_number:\n",
        "        continue\n",
        "      img_path = images_dir + filename\n",
        "      display(img_path)\n",
        "      ann_path = annotations_dir + 'Image' + image_id + '.xml'\n",
        "      display(ann_path)\n",
        "\t\t\t# Add to dataset\n",
        "      self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
        " \n",
        "\t# Extract bounding boxes from an annotation file\n",
        "  def extract_boxes(self, filename):\n",
        "\t\t# load and parse the file\n",
        "    tree = ElementTree.parse(filename)\n",
        "\t\t# get the root of the document\n",
        "    root = tree.getroot()\n",
        "\t\t# extract each bounding box\n",
        "    boxes = list()\n",
        "\n",
        "    objects = root.findall('.//object')\n",
        "    # Adds the object found to the end of boxes. Boxes now has 5 values instead of 4.\n",
        "    objects_to_find = ['bellguard', 'scorebox', 'torso']\n",
        "    for object_to_find_iterator in objects_to_find:\n",
        "      # display(f'The object iterator is: {object_to_find_iterator}')\n",
        "      for obj in objects:\n",
        "        if (obj.find('.name').text) == object_to_find_iterator:\n",
        "          xmin = obj.find('.bndbox/xmin')\n",
        "          ymin = obj.find('.bndbox/ymin')\n",
        "          xmax = obj.find('.bndbox/xmax')\n",
        "          ymax = obj.find('.bndbox/ymax')\n",
        "          coors = [int(xmin.text), int(ymin.text), int(xmax.text), int(ymax.text), object_to_find_iterator]\n",
        "          # display(coors)\n",
        "          boxes.append(coors)\n",
        "        else:\n",
        "          pass\n",
        "\n",
        "\t\t# Extracts image dimensions\n",
        "    width = int(root.find('.//size/width').text)\n",
        "    height = int(root.find('.//size/height').text)\n",
        "    return boxes, width, height\n",
        " \n",
        "\t# Loads the masks for an image\n",
        "  def load_mask(self, image_id):\n",
        "\t\t# Gets details of image\n",
        "    info = self.image_info[image_id]\n",
        "\t\t# Defines box file location\n",
        "    path = info['annotation']\n",
        "\t\t# Loads XML\n",
        "    object_to_find = 'bellguard'\n",
        "    # boxes, w, h = self.extract_boxes(path, object_to_find)\n",
        "    display(f'The image_id is: {image_id}')\n",
        "\n",
        "    boxes, w, h = self.extract_boxes(path)\n",
        "\t\t# Creates one array for all masks, each on a different channel\n",
        "    masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
        "\t\t# Creates masks\n",
        "    class_ids = list()\n",
        "    # len(boxes) is the number of 5 value lists within the list of boxes\n",
        "    for i in range(len(boxes)):\n",
        "      box = boxes[i]\n",
        "      row_s, row_e = box[1], box[3]\n",
        "      col_s, col_e = box[0], box[2]\n",
        "      masks[row_s:row_e, col_s:col_e, i] = 1\n",
        "      class_ids.append(self.class_names.index(box[4]))\n",
        "      # class_ids.append(self.class_names.index('Bell_Guard'))\n",
        "\n",
        "    # display(f'The class_ids in load_mask are: {asarray(class_ids, dtype=\"int32\")}')\n",
        "    return masks, asarray(class_ids, dtype='int32')\n",
        " \n",
        "\t# load an image reference\n",
        "  def image_reference(self, image_id):\n",
        "    info = self.image_info[image_id]\n",
        "    return info['path']"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UelgtD9fLzrR",
        "cellView": "code"
      },
      "source": [
        "#@title Bell_GuardConfig\n",
        "class Bell_GuardConfig(Config):\n",
        "\t#Bell_GuardConfig\n",
        "\tGPU_COUNT = 1\n",
        "\tIMAGES_PER_GPU = 1\n",
        "\n",
        "\t# define the name of the configuration\n",
        "\tNAME = \"Bell_Guard_cfg\"\n",
        "\tNUM_CLASSES = 1 + 3\n",
        "\t# number of training steps per epoch\n",
        "\tSTEPS_PER_EPOCH = 131"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfWX2andCydq",
        "cellView": "code"
      },
      "source": [
        "#@title InferenceConfig\n",
        "class InferenceConfig(Config):\n",
        "  GPU_COUNT = 1\n",
        "  #Same as batch_size\n",
        "  IMAGES_PER_GPU = 1\n",
        "  NUM_CLASSES = 4\n",
        "  KEYPOINT_MASK_POOL_SIZE = 7"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joO1L4fvZWpy",
        "cellView": "code"
      },
      "source": [
        "#@title PredictionConfig\n",
        "class PredictionConfig(Config):\n",
        "  # Evaluate_Model\n",
        "  # Defines the prediction configuration\n",
        "\t# define the name of the configuration\n",
        "  NAME = \"bell_guard_cfg\"\n",
        "\t# number of classes (background + bell_guard + scorebox + torso)\n",
        "  NUM_CLASSES = 1 + 3\n",
        "  GPU_COUNT = 1\n",
        "  IMAGES_PER_GPU = 1"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxP-wmmX_X5a",
        "cellView": "code"
      },
      "source": [
        "time_stamp_dict[\"load_functions\"] = time.time()\n",
        "\n",
        "# Load the pre-trained fencing object detection model\n",
        "if exists('/content/drive/My Drive/mask_rcnn_bell_guard_cfg_0005.h5'):\n",
        "  source = '/content/drive/My Drive/mask_rcnn_bell_guard_cfg_0005.h5'\n",
        "  destination = '/content/Mask_RCNN/'\n",
        "  shutil.copy(source, destination)\n",
        "else:\n",
        "  display(f'The Bell Guard model was not found.')\n",
        "  assert False\n",
        "\n",
        "time_stamp_dict[\"load_model\"] = time.time()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-RRulU0obH5",
        "outputId": "982853bf-50f5-4999-fad4-f4240bd760f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Checks if the Folder Hierarchy is in place\n",
        "if not exists('/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips'):\n",
        "  create_folder_hierarchy(video_filename)\n",
        "  # clip_call = 0\n",
        "else:\n",
        "  display(f'The Folder Hierarchy is already in place.')\n",
        "\n",
        "if youtube_link != '':\n",
        "  time_stamp_dict[\"Before_YouTube_Clip\"] = time.time()\n",
        "  youtube_clip(youtube_link)\n",
        "  video_filename = '999.mp4'\n",
        "  clip_call = 0\n",
        "  time_stamp_dict[\"After_YouTube_Clip\"] = time.time()\n",
        "else:\n",
        "  time_stamp_dict[\"Before_YouTube_Clip\"] = time.time()\n",
        "  time_stamp_dict[\"After_YouTube_Clip\"] = time.time()\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Folder Hierarchy is already in place.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube_dl in /usr/local/lib/python3.6/dist-packages (2020.9.20)\n",
            "[youtube] A1hKs88uVLQ: Downloading webpage\n",
            "[youtube] A1hKs88uVLQ: Downloading webpage\n",
            "[download] ./video_title.mp4 has already been downloaded and merged\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'video_title.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "  Duration: 00:11:40.13, start: 0.000000, bitrate: 1362 kb/s\n",
            "    Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt470bg/bt709/bt709), 1280x720 [SAR 1:1 DAR 16:9], 1228 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
            "    Metadata:\n",
            "      handler_name    : SoundHandler\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
            "  Stream #0:1 -> #0:1 (aac (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '999.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0(und): Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=-1--1, 25 fps, 12800 tbn, 25 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)\n",
            "    Metadata:\n",
            "      handler_name    : SoundHandler\n",
            "      encoder         : Lavc57.107.100 aac\n",
            "frame=  100 fps= 22 q=-1.0 Lsize=     730kB time=00:00:04.04 bitrate=1479.3kbits/s speed=0.886x    \n",
            "video:661kB audio:64kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.638245%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mframe I:1     Avg QP:20.48  size: 40028\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mframe P:26    Avg QP:21.22  size: 13215\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mframe B:73    Avg QP:23.92  size:  4013\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mconsecutive B-frames:  2.0%  2.0%  0.0% 96.0%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mmb I  I16..4: 54.6% 23.4% 21.9%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mmb P  I16..4:  8.1%  3.7%  2.6%  P16..4: 32.5%  8.1%  3.7%  0.0%  0.0%    skip:41.3%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mmb B  I16..4:  1.0%  0.2%  0.4%  B16..8: 32.8%  3.0%  0.5%  direct: 1.7%  skip:60.4%  L0:46.9% L1:46.8% BI: 6.3%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0m8x8 transform intra:22.3% inter:61.7%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mcoded y,uvDC,uvAC intra: 29.7% 51.0% 17.4% inter: 5.6% 9.3% 0.6%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mi16 v,h,dc,p: 14% 75%  3%  7%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  9% 33% 42%  2%  2%  2%  3%  2%  5%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 11% 51% 14%  3%  3%  3%  5%  3%  5%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mi8c dc,h,v,p: 23% 62%  8%  7%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mref P L0: 63.5%  9.9% 19.6%  7.1%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mref B L0: 87.9% 10.0%  2.0%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mref B L1: 96.0%  4.0%\n",
            "\u001b[1;36m[libx264 @ 0x5609b4086800] \u001b[0mkb/s:1353.16\n",
            "\u001b[1;36m[aac @ 0x5609b4087700] \u001b[0mQavg: 463.353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPQvpuHVa2Bx",
        "outputId": "79a68853-94b7-4e6e-f557-de8d6d87657f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "touch_folders = ['Left_Touch', 'Right_Touch', 'Simul']\n",
        "\n",
        "root_path = r'/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "video_dir_clips = root_path + touch_folders[clip_call]\n",
        "display(f'The video_dir_clips path is: {video_dir_clips}')\n",
        "\n",
        "path = r'/content/Mask_RCNN/videos/'\n",
        "\n",
        "[bbox, frame_count, capture_width, capture_height, clip_vector_previous, fencer_data, keypoints, clip_vector, fps, time_stamp_dict] = \\\n",
        "  process_video_clip(video_filename, touch_folders[clip_call], remove_duplicate_frames, time_stamp_dict)\n",
        "\n",
        "# Removes the .mp4 from the String\n",
        "# if simplified == True:\n",
        "iterator = video_filename[:-4]\n",
        "\n",
        "\n",
        "#Saves the Clip, Speed and Acceleration Vectors\n",
        "time_stamp_dict[\"before_saving_clip_vector\"] = time.time()\n",
        "clip_vector_np_save(touch_folders[clip_call], iterator, clip_vector)\n",
        "time_stamp_dict[\"after_saving_clip_vector\"] = time.time()\n",
        "\n",
        "if generate_representative == True:\n",
        "  #Saves Images for the Representative Video\n",
        "  create_representative_image(clip_vector, capture_width, capture_height)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video_dir_clips path is: /content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video file_name is: 999.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ERROR creating the video directory'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'os.getcwd() is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The ROOT_DIR is: /content/Mask_RCNN'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The COCO model path is : /content/Mask_RCNN/mask_rcnn_coco.h5'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The path is: /content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/999.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video directory is /content/Mask_RCNN/videos/999.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 100'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 0.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 1.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 2.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 3.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 5.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 6.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 7.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 8.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 9.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 10.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 11.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 12.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 13.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 14.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 15.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 16.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 17.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 18.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 19.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 20.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 21.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 22.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 23.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 24.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 25.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 26.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 27.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 28.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 29.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 30.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 31.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 32.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 33.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 34.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 35.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 36.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 37.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 38.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 39.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 40.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 41.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 42.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 43.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 44.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 45.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 46.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 47.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 48.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 49.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 50.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 51.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 52.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 53.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 54.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 55.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 56.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 57.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 58.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 59.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 60.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 61.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 62.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 63.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 64.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 65.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 66.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 67.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 68.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 69.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 70.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 71.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 72.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 73.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 74.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 75.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 76.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 77.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 78.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 79.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 80.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 81.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 82.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 83.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 84.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 85.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 86.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 87.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 88.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 89.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 90.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 91.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 92.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 93.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 94.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 95.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 96.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 97.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 98.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frame_count is 99.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The iterator file_name is 999.mp4.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The duplicate frames of video 999.mp4.mp4 have been removed'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The COCO model path is : /content/Mask_RCNN/mask_rcnn_coco.h5'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  1024\n",
            "IMAGE_META_SIZE                16\n",
            "IMAGE_MIN_DIM                  800\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [1024 1024    3]\n",
            "KEYPOINT_MASK_POOL_SIZE        7\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           None\n",
            "NUM_CLASSES                    4\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                1000\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The total number of frames in the video are: 100'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/ops/boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  keep = keep.nonzero().squeeze(1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlYBdce86fM",
        "outputId": "1fe4e56e-f896-4641-f2d3-a448b8a41b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Prepares and Downloads videos\n",
        "time_stamp_dict[\"before_video_download\"] = time.time()\n",
        "if download_videos == True:\n",
        "  # Prepares Output Video\n",
        "  [outvid, images] = prepare_video_frames('save', 'out')\n",
        "  make_video(outvid, images, fps=fps)\n",
        "\n",
        "  # Downloads Output Video\n",
        "  name = '/content/Mask_RCNN/videos/' + str(iterator) + '.out.mp4'\n",
        "  display(name)\n",
        "  files.download(name)\n",
        "\n",
        "  if generate_representative == True:\n",
        "    #Prepares Representative Video\n",
        "    [outvid, images] = prepare_video_frames('save_white_dot', 'representative_out')\n",
        "    make_video(outvid, images, fps=fps)\n",
        "\n",
        "    # Downloads Representative Video\n",
        "    name =  '/content/Mask_RCNN/videos/' + str(iterator) + '.representative_out.mp4'\n",
        "    files.download(name)\n",
        "\n",
        "  # Prepares Overlay Video\n",
        "    create_overlay_image(len(clip_vector))\n",
        "  # [outvid, images] = prepare_video_frames('overlay', 'overlay_out')\n",
        "  # make_video(outvid, images, fps=fps)\n",
        "\n",
        "  # # Downloads Overlay Video\n",
        "  # name =  '/content/Mask_RCNN/videos/' + str(iterator) + '.overlay_out.mp4'\n",
        "  # files.download(name)\n",
        "time_stamp_dict[\"after_video_download\"] = time.time()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Mask_RCNN/videos/999.out.mp4'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0820633c-ec8b-4661-bede-93701da3ea30\", \"999.out.mp4\", 4252804)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU0dTKFvRoKv",
        "outputId": "86e424f5-eb1a-48ee-bf3d-03c5fb416b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        }
      },
      "source": [
        "# Analyzes the Video Clip\n",
        "engarde_position_buffer = 15\n",
        "# max_length = 103\n",
        "max_length = 111\n",
        "clip_vector_length = max_length - engarde_position_buffer\n",
        "\n",
        "# Checks if the ROW model is in the Fencing Clip Folder or the top level\n",
        "if exists('/content/drive/My Drive/ROW_model.h5'):\n",
        "  display(f'Loading the Right of Way Model from the top level directory.')\n",
        "  save_path = '/content/drive/My Drive/'\n",
        "elif exists('/content/drive/My Drive/projects/fencing/Fencing Clips/ROW_model.h5'):\n",
        "  display(f'Loading the Right of Way Model from Fencing Clips directory.')\n",
        "  save_path = '/content/drive/My Drive/projects/fencing/Fencing Clips/'\n",
        "else:\n",
        "  # Breaks the program execution if the ROW is not found.\n",
        "  display(f'The Right of Way Model is missing.')\n",
        "  assert False\n",
        "\n",
        "time_stamp_dict[\"before_load_model_for_analysis\"] = time.time() \n",
        "model = load_model(os.path.join(save_path, 'ROW_model.h5'))\n",
        "time_stamp_dict[\"after_load_model_for_analysis\"] = time.time() \n",
        "\n",
        "time_stamp_dict[\"before_load_clip\"] = time.time() \n",
        "x = load_clip(touch_folders[clip_call], iterator, max_length, capture_width)\n",
        "time_stamp_dict[\"after_load_clip\"] = time.time()\n",
        "\n",
        "time_stamp_dict[\"before_clip_prediction\"] = time.time()\n",
        "pred = model.predict(x)\n",
        "time_stamp_dict[\"after_clip_prediction\"] = time.time()\n",
        "\n",
        "\n",
        "# display(f'The time stamp dictionary is:')\n",
        "# display(time_stamp_dict)\n",
        "\n",
        "\n",
        "display(f'The Time Benchmarks in, h:m:s:ms, are:')\n",
        "display(f'Setting up the initial parameters: {hms_string(time_stamp_dict[\"initial_parameters\"] - time_stamp_dict[\"connect_to_google_drive\"])}')\n",
        "display(f'Loading Mask RCNN and Keypoints: {hms_string(time_stamp_dict[\"key_point_and_mask_rcnn_git_download\"] - time_stamp_dict[\"initial_parameters\"])}')\n",
        "display(f'Loading Imports: {hms_string(time_stamp_dict[\"imports\"] - time_stamp_dict[\"key_point_and_mask_rcnn_git_download\"])}')\n",
        "display(f'Loading Functions: {hms_string(time_stamp_dict[\"load_functions\"] - time_stamp_dict[\"imports\"])}')\n",
        "display(f'Loading Model: {hms_string(time_stamp_dict[\"load_model\"] - time_stamp_dict[\"load_functions\"])}')\n",
        "display(f'Loading YouTube Clip: {hms_string(time_stamp_dict[\"After_YouTube_Clip\"] - time_stamp_dict[\"Before_YouTube_Clip\"])}')\n",
        "display(f'Between Loading the Model and starting to Process the Video: {hms_string(time_stamp_dict[\"process_video\"] - time_stamp_dict[\"load_model\"])}')\n",
        "display(f'Process the Video File: {hms_string(time_stamp_dict[\"before_saving_clip_vector\"] - time_stamp_dict[\"process_video\"])}')\n",
        "display(f'Create Directories: {hms_string(time_stamp_dict[\"create_directories\"] - time_stamp_dict[\"process_video\"])}')\n",
        "display(f'Remove Duplicate Frames: {hms_string(time_stamp_dict[\"remove_duplicate_frames\"] - time_stamp_dict[\"create_directories\"])}')\n",
        "display(f'Process Video: {hms_string(time_stamp_dict[\"frame_processing_end\"] - time_stamp_dict[\"frame_processing_start\"])}')\n",
        "display(f'Read a Single Frame: {hms_string(time_stamp_dict[\"After_Reading_a_Frame\"] - time_stamp_dict[\"Before_Reading_a_Frame\"])}')\n",
        "display(f'Get Results for a Single Frame: {hms_string(time_stamp_dict[\"After_Getting_Frame_Results\"] - time_stamp_dict[\"Before_Getting_Frame_Results\"])}')\n",
        "display(f'Human Pose Results for a Single Frame: {hms_string(time_stamp_dict[\"After_Human_Pose_Results\"] - time_stamp_dict[\"Before_Human_Pose_Results\"])}')\n",
        "display(f'Combining BBox Results: {hms_string(time_stamp_dict[\"After_Combining_and_Saving_BBox_Results\"] - time_stamp_dict[\"Before_Combining_and_Saving_BBox_Results\"])}')\n",
        "display(f'Positioning an Engarde Frame: {hms_string(time_stamp_dict[\"After_Engarde_Positioning_of_a_Frame\"] - time_stamp_dict[\"Before_Engarde_Positioning_of_a_Frame\"])}')\n",
        "display(f'Analyzing at Engarde Position: {hms_string(time_stamp_dict[\"After_Engarde_Length\"] - time_stamp_dict[\"Before_Engarde_Length\"])}')\n",
        "display(f'Positioning a Post Engarde Frame: {hms_string(time_stamp_dict[\"After_Processing_of_a_Post_Engarde_Frame\"] - time_stamp_dict[\"Before_Processing_of_a_Post_Engarde_Frame\"])}')\n",
        "display(f'From Processing End to Capture Release of a Frame: {hms_string(time_stamp_dict[\"after_capture_release\"] - time_stamp_dict[\"frame_processing_end\"])}')\n",
        "display(f'Generate Simple Clip Vector: {hms_string(time_stamp_dict[\"after_generate_simple_representative\"] - time_stamp_dict[\"before_generate_simple_representative\"])}')\n",
        "display(f'Light Comparison: {hms_string(time_stamp_dict[\"after_light_comparison\"] - time_stamp_dict[\"before_light_comparison\"])}')\n",
        "display(f'Generate Clip Vector: {hms_string(time_stamp_dict[\"after_clip_vector_generation\"] - time_stamp_dict[\"before_clip_vector_generation\"])}')\n",
        "display(f'Video Download: {hms_string(time_stamp_dict[\"after_video_download\"] - time_stamp_dict[\"before_video_download\"])}')\n",
        "display(f'Load Model for Analysis: {hms_string(time_stamp_dict[\"after_load_model_for_analysis\"] - time_stamp_dict[\"before_load_model_for_analysis\"])}')\n",
        "display(f'Load and Format Clip Vector for Analysis: {hms_string(time_stamp_dict[\"after_load_clip\"] - time_stamp_dict[\"before_load_clip\"])}')\n",
        "display(f'Clip Prediction: {hms_string(time_stamp_dict[\"after_clip_prediction\"] - time_stamp_dict[\"before_clip_prediction\"])}')\n",
        "\n",
        "# display(f'The predicted touch is Left {int(pred[0][0]*100)}%, Right {int(pred[0][1]*100)}%, Simul {int(pred[0][2]*100)}%.')\n",
        "pred_total = pred[0][0] + pred[0][1] + pred[0][2]\n",
        "# display(f'The total time elapsed is: {hms_string(time.time() - t_start)}.')\n",
        "display(f'The normalized predicted touch is Left {int(pred[0][0]/pred_total*100)}%, Right {int(pred[0][1]/pred_total*100)}%, Simul {int(pred[0][2]/pred_total*100)}%.')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loading the Right of Way Model from the top level directory.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/projects/fencing/Fencing Clips/Left_Touch/Left_Touch_Vector_Clips_Acceleration/clip_vector_acceleration_np999.csv'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Time Benchmarks in, h:m:s:ms, are:'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Setting up the initial parameters: 0:00:00:000'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loading Mask RCNN and Keypoints: 0:00:05:065'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loading Imports: 0:00:01:757'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loading Functions: 0:00:13:222'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loading Model: 0:00:00:725'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loading YouTube Clip: 0:00:09:188'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Between Loading the Model and starting to Process the Video: 0:00:09:224'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Process the Video File: 0:01:42:068'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Create Directories: 0:00:00:038'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Remove Duplicate Frames: 0:00:07:080'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Process Video: 0:01:12:712'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Read a Single Frame: 0:00:00:002'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Get Results for a Single Frame: 0:00:00:450'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Human Pose Results for a Single Frame: 0:00:00:185'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Combining BBox Results: 0:00:00:031'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positioning an Engarde Frame: 0:00:00:054'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Analyzing at Engarde Position: 0:00:00:000'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positioning a Post Engarde Frame: 0:00:00:000'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'From Processing End to Capture Release of a Frame: 0:00:00:000'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Generate Simple Clip Vector: 0:00:00:000'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Light Comparison: 0:00:13:105'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Generate Clip Vector: 0:00:00:000'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Video Download: 0:00:01:807'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Load Model for Analysis: 0:00:02:733'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Load and Format Clip Vector for Analysis: 0:00:00:008'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Clip Prediction: 0:00:00:522'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The normalized predicted touch is Left 99%, Right 0%, Simul 0%.'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}